{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder example\n",
    "\n",
    "The goal here is to encode a \"toy\" beat data, represented as a shape (24, ) tensor of magnitudes, in a way that reflects the following metric generated by the following moves:\n",
    "* Shifting a single beat by 1 should be distance 0.5.\n",
    "* Removing a single beat should be distance 1.\n",
    "\n",
    "Play with these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A basic encoder/decoder with a single internal layer\n",
    "# Allows for augmentation of the encoder, i.e. if we have already extracted some information from the data and want to use it in the encoding, it can be concatenated to it.\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, in_size, out_size, mid_size, aug_size = 0):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(in_size, mid_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(mid_size, mid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_size, mid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_size, out_size))\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(out_size + aug_size, mid_size), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(mid_size, mid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_size, mid_size),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(mid_size, in_size))\n",
    "        self.aug_size = aug_size\n",
    "\n",
    "    def forward(self, x, a = None):\n",
    "        return self.decode(self.encode(x, a))\n",
    "    \n",
    "    def encode(self, x, a = None):\n",
    "        if a == None:\n",
    "            a = torch.zeros((x.shape[0], self.aug_size))\n",
    "        return torch.cat([self.enc(x), a], dim=1)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.dec(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "# The base data\n",
    "x = torch.randint(high=2, size=(N,24))\n",
    "\n",
    "# Target data\n",
    "# For ith data, look for a 1 at position i % 24 and a 0 in position i % 25; if found, transpose, otherwise, skip\n",
    "do_transpose = (x[torch.arange(N), torch.arange(N) % 24] == 1) & (x[torch.arange(N), (torch.arange(N) + 1) % 24] == 0)\n",
    "# If there is a 1 at the first position and a 1 in the next position, drop it\n",
    "do_drop = (x[torch.arange(N), torch.arange(N) % 24] == 1) & (x[torch.arange(N), (torch.arange(N) + 1) % 24] == 1)\n",
    "\n",
    "y = x.detach().clone()\n",
    "\n",
    "y[do_transpose, (torch.arange(N) % 24)[do_transpose]] = 0\n",
    "y[do_transpose, ((torch.arange(N) + 1) % 24)[do_transpose]] = 1\n",
    "\n",
    "y[do_drop, (torch.arange(N) % 24)[do_drop]] = 0\n",
    "\n",
    "\n",
    "distances = torch.zeros(N)\n",
    "distances[do_transpose] = 0.5\n",
    "distances[do_drop] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train 4000 iterations with full batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 loss 11.625568389892578\n",
      "Iteration 2 loss 11.201728820800781\n",
      "Iteration 3 loss 10.002177238464355\n",
      "Iteration 4 loss 7.765338897705078\n",
      "Iteration 5 loss 6.0670013427734375\n",
      "Iteration 6 loss 6.295544624328613\n",
      "Iteration 7 loss 5.60684061050415\n",
      "Iteration 8 loss 6.159659385681152\n",
      "Iteration 9 loss 6.222299098968506\n",
      "Iteration 10 loss 5.867192268371582\n",
      "Iteration 11 loss 5.774085521697998\n",
      "Iteration 12 loss 5.985645294189453\n",
      "Iteration 13 loss 5.747470378875732\n",
      "Iteration 14 loss 5.598006248474121\n",
      "Iteration 15 loss 5.664076805114746\n",
      "Iteration 16 loss 5.748331069946289\n",
      "Iteration 17 loss 5.694278240203857\n",
      "Iteration 18 loss 5.532658100128174\n",
      "Iteration 19 loss 5.598372936248779\n",
      "Iteration 20 loss 5.769241809844971\n",
      "Iteration 21 loss 5.563508033752441\n",
      "Iteration 22 loss 5.597104072570801\n",
      "Iteration 23 loss 5.589698791503906\n",
      "Iteration 24 loss 5.623269081115723\n",
      "Iteration 25 loss 5.655639171600342\n",
      "Iteration 26 loss 5.473281383514404\n",
      "Iteration 27 loss 5.563162803649902\n",
      "Iteration 28 loss 5.547940254211426\n",
      "Iteration 29 loss 5.5445427894592285\n",
      "Iteration 30 loss 5.503809928894043\n",
      "Iteration 31 loss 5.575505256652832\n",
      "Iteration 32 loss 5.505487442016602\n",
      "Iteration 33 loss 5.546297073364258\n",
      "Iteration 34 loss 5.517952919006348\n",
      "Iteration 35 loss 5.54123067855835\n",
      "Iteration 36 loss 5.5176849365234375\n",
      "Iteration 37 loss 5.528191566467285\n",
      "Iteration 38 loss 5.451558589935303\n",
      "Iteration 39 loss 5.5741963386535645\n",
      "Iteration 40 loss 5.479808330535889\n",
      "Iteration 41 loss 5.497875213623047\n",
      "Iteration 42 loss 5.555616855621338\n",
      "Iteration 43 loss 5.443628311157227\n",
      "Iteration 44 loss 5.456819534301758\n",
      "Iteration 45 loss 5.52628755569458\n",
      "Iteration 46 loss 5.488804817199707\n",
      "Iteration 47 loss 5.503170490264893\n",
      "Iteration 48 loss 5.457570552825928\n",
      "Iteration 49 loss 5.4784955978393555\n",
      "Iteration 50 loss 5.481149196624756\n",
      "Iteration 51 loss 5.447102069854736\n",
      "Iteration 52 loss 5.439390182495117\n",
      "Iteration 53 loss 5.466279983520508\n",
      "Iteration 54 loss 5.4787917137146\n",
      "Iteration 55 loss 5.505748271942139\n",
      "Iteration 56 loss 5.4901227951049805\n",
      "Iteration 57 loss 5.4463653564453125\n",
      "Iteration 58 loss 5.428097248077393\n",
      "Iteration 59 loss 5.435697078704834\n",
      "Iteration 60 loss 5.42063045501709\n",
      "Iteration 61 loss 5.44255256652832\n",
      "Iteration 62 loss 5.4541215896606445\n",
      "Iteration 63 loss 5.418819427490234\n",
      "Iteration 64 loss 5.4855875968933105\n",
      "Iteration 65 loss 5.461513519287109\n",
      "Iteration 66 loss 5.435966491699219\n",
      "Iteration 67 loss 5.411125659942627\n",
      "Iteration 68 loss 5.439373016357422\n",
      "Iteration 69 loss 5.448364734649658\n",
      "Iteration 70 loss 5.457433700561523\n",
      "Iteration 71 loss 5.364642143249512\n",
      "Iteration 72 loss 5.4278950691223145\n",
      "Iteration 73 loss 5.430804252624512\n",
      "Iteration 74 loss 5.443545818328857\n",
      "Iteration 75 loss 5.383033752441406\n",
      "Iteration 76 loss 5.330619812011719\n",
      "Iteration 77 loss 5.294950008392334\n",
      "Iteration 78 loss 5.367842197418213\n",
      "Iteration 79 loss 5.363536834716797\n",
      "Iteration 80 loss 5.3219146728515625\n",
      "Iteration 81 loss 5.3314032554626465\n",
      "Iteration 82 loss 5.285877227783203\n",
      "Iteration 83 loss 5.265454292297363\n",
      "Iteration 84 loss 5.188595771789551\n",
      "Iteration 85 loss 5.150418758392334\n",
      "Iteration 86 loss 5.161151885986328\n",
      "Iteration 87 loss 5.116156101226807\n",
      "Iteration 88 loss 5.03339958190918\n",
      "Iteration 89 loss 5.00720739364624\n",
      "Iteration 90 loss 4.999602317810059\n",
      "Iteration 91 loss 5.029789924621582\n",
      "Iteration 92 loss 4.899432182312012\n",
      "Iteration 93 loss 4.9267706871032715\n",
      "Iteration 94 loss 4.940001964569092\n",
      "Iteration 95 loss 4.827541828155518\n",
      "Iteration 96 loss 4.812692165374756\n",
      "Iteration 97 loss 4.717953205108643\n",
      "Iteration 98 loss 4.771824836730957\n",
      "Iteration 99 loss 4.638652801513672\n",
      "Iteration 100 loss 4.622059345245361\n",
      "Iteration 101 loss 4.546539306640625\n",
      "Iteration 102 loss 4.495046615600586\n",
      "Iteration 103 loss 4.506100654602051\n",
      "Iteration 104 loss 4.406507968902588\n",
      "Iteration 105 loss 4.4641218185424805\n",
      "Iteration 106 loss 4.565530776977539\n",
      "Iteration 107 loss 4.338208198547363\n",
      "Iteration 108 loss 4.389775276184082\n",
      "Iteration 109 loss 4.307608604431152\n",
      "Iteration 110 loss 4.380148887634277\n",
      "Iteration 111 loss 4.379685878753662\n",
      "Iteration 112 loss 4.229012966156006\n",
      "Iteration 113 loss 4.208046913146973\n",
      "Iteration 114 loss 4.193213939666748\n",
      "Iteration 115 loss 4.097887992858887\n",
      "Iteration 116 loss 4.069304943084717\n",
      "Iteration 117 loss 3.9302308559417725\n",
      "Iteration 118 loss 3.9158685207366943\n",
      "Iteration 119 loss 3.802929639816284\n",
      "Iteration 120 loss 3.9067766666412354\n",
      "Iteration 121 loss 4.002006530761719\n",
      "Iteration 122 loss 4.127799034118652\n",
      "Iteration 123 loss 3.9924144744873047\n",
      "Iteration 124 loss 3.87703800201416\n",
      "Iteration 125 loss 3.762869358062744\n",
      "Iteration 126 loss 3.7613110542297363\n",
      "Iteration 127 loss 3.66532301902771\n",
      "Iteration 128 loss 3.7004847526550293\n",
      "Iteration 129 loss 3.686582326889038\n",
      "Iteration 130 loss 3.59104323387146\n",
      "Iteration 131 loss 3.5317647457122803\n",
      "Iteration 132 loss 3.5204334259033203\n",
      "Iteration 133 loss 3.572864055633545\n",
      "Iteration 134 loss 3.3868813514709473\n",
      "Iteration 135 loss 3.495800018310547\n",
      "Iteration 136 loss 3.365924835205078\n",
      "Iteration 137 loss 3.394721031188965\n",
      "Iteration 138 loss 3.2805991172790527\n",
      "Iteration 139 loss 3.273996353149414\n",
      "Iteration 140 loss 3.242032766342163\n",
      "Iteration 141 loss 3.326033592224121\n",
      "Iteration 142 loss 3.111344337463379\n",
      "Iteration 143 loss 3.1059999465942383\n",
      "Iteration 144 loss 3.2014336585998535\n",
      "Iteration 145 loss 3.211965560913086\n",
      "Iteration 146 loss 3.097416639328003\n",
      "Iteration 147 loss 3.0669591426849365\n",
      "Iteration 148 loss 3.084611177444458\n",
      "Iteration 149 loss 3.0228161811828613\n",
      "Iteration 150 loss 2.962885618209839\n",
      "Iteration 151 loss 2.986992835998535\n",
      "Iteration 152 loss 2.996244192123413\n",
      "Iteration 153 loss 2.8291585445404053\n",
      "Iteration 154 loss 3.0300724506378174\n",
      "Iteration 155 loss 2.9430019855499268\n",
      "Iteration 156 loss 2.8422603607177734\n",
      "Iteration 157 loss 2.7349772453308105\n",
      "Iteration 158 loss 2.774129867553711\n",
      "Iteration 159 loss 2.7468371391296387\n",
      "Iteration 160 loss 2.785125970840454\n",
      "Iteration 161 loss 2.646073579788208\n",
      "Iteration 162 loss 2.709460973739624\n",
      "Iteration 163 loss 2.629917621612549\n",
      "Iteration 164 loss 2.72685170173645\n",
      "Iteration 165 loss 2.620081901550293\n",
      "Iteration 166 loss 2.6193885803222656\n",
      "Iteration 167 loss 2.5576846599578857\n",
      "Iteration 168 loss 2.5660014152526855\n",
      "Iteration 169 loss 2.5661332607269287\n",
      "Iteration 170 loss 2.645972728729248\n",
      "Iteration 171 loss 2.444298028945923\n",
      "Iteration 172 loss 2.3632795810699463\n",
      "Iteration 173 loss 2.447230339050293\n",
      "Iteration 174 loss 2.4315617084503174\n",
      "Iteration 175 loss 2.4658620357513428\n",
      "Iteration 176 loss 2.337635040283203\n",
      "Iteration 177 loss 2.301971197128296\n",
      "Iteration 178 loss 2.217694044113159\n",
      "Iteration 179 loss 2.2764880657196045\n",
      "Iteration 180 loss 2.2566375732421875\n",
      "Iteration 181 loss 2.2702043056488037\n",
      "Iteration 182 loss 2.16094708442688\n",
      "Iteration 183 loss 2.110335111618042\n",
      "Iteration 184 loss 2.176875114440918\n",
      "Iteration 185 loss 2.192087173461914\n",
      "Iteration 186 loss 2.158440113067627\n",
      "Iteration 187 loss 2.089254140853882\n",
      "Iteration 188 loss 2.0730860233306885\n",
      "Iteration 189 loss 2.0602903366088867\n",
      "Iteration 190 loss 2.0343875885009766\n",
      "Iteration 191 loss 2.019216537475586\n",
      "Iteration 192 loss 1.9402962923049927\n",
      "Iteration 193 loss 1.9428212642669678\n",
      "Iteration 194 loss 1.8957889080047607\n",
      "Iteration 195 loss 1.9526350498199463\n",
      "Iteration 196 loss 1.891036033630371\n",
      "Iteration 197 loss 1.9960169792175293\n",
      "Iteration 198 loss 1.8942043781280518\n",
      "Iteration 199 loss 1.910420536994934\n",
      "Iteration 200 loss 1.9076610803604126\n",
      "Iteration 201 loss 1.946852207183838\n",
      "Iteration 202 loss 1.837683081626892\n",
      "Iteration 203 loss 1.8609272241592407\n",
      "Iteration 204 loss 1.8801164627075195\n",
      "Iteration 205 loss 1.9452530145645142\n",
      "Iteration 206 loss 1.8279056549072266\n",
      "Iteration 207 loss 1.9228453636169434\n",
      "Iteration 208 loss 1.892147183418274\n",
      "Iteration 209 loss 1.8709181547164917\n",
      "Iteration 210 loss 1.8341528177261353\n",
      "Iteration 211 loss 1.7557966709136963\n",
      "Iteration 212 loss 1.7639894485473633\n",
      "Iteration 213 loss 1.9451541900634766\n",
      "Iteration 214 loss 1.7656575441360474\n",
      "Iteration 215 loss 1.835572361946106\n",
      "Iteration 216 loss 1.6812620162963867\n",
      "Iteration 217 loss 1.7793513536453247\n",
      "Iteration 218 loss 1.709017276763916\n",
      "Iteration 219 loss 1.7435839176177979\n",
      "Iteration 220 loss 1.8468142747879028\n",
      "Iteration 221 loss 1.7516074180603027\n",
      "Iteration 222 loss 1.7061361074447632\n",
      "Iteration 223 loss 1.712646245956421\n",
      "Iteration 224 loss 1.7374292612075806\n",
      "Iteration 225 loss 1.7271571159362793\n",
      "Iteration 226 loss 1.6781002283096313\n",
      "Iteration 227 loss 1.6354544162750244\n",
      "Iteration 228 loss 1.5695021152496338\n",
      "Iteration 229 loss 1.6311978101730347\n",
      "Iteration 230 loss 1.5743975639343262\n",
      "Iteration 231 loss 1.7714357376098633\n",
      "Iteration 232 loss 1.7313259840011597\n",
      "Iteration 233 loss 1.4939261674880981\n",
      "Iteration 234 loss 1.6858266592025757\n",
      "Iteration 235 loss 1.7364706993103027\n",
      "Iteration 236 loss 1.5076181888580322\n",
      "Iteration 237 loss 1.5461310148239136\n",
      "Iteration 238 loss 1.6404814720153809\n",
      "Iteration 239 loss 1.5598071813583374\n",
      "Iteration 240 loss 1.6522581577301025\n",
      "Iteration 241 loss 1.5644049644470215\n",
      "Iteration 242 loss 1.5754117965698242\n",
      "Iteration 243 loss 1.5229203701019287\n",
      "Iteration 244 loss 1.595225214958191\n",
      "Iteration 245 loss 1.4479670524597168\n",
      "Iteration 246 loss 1.5404127836227417\n",
      "Iteration 247 loss 1.5186101198196411\n",
      "Iteration 248 loss 1.5449037551879883\n",
      "Iteration 249 loss 1.5141085386276245\n",
      "Iteration 250 loss 1.4057642221450806\n",
      "Iteration 251 loss 1.4484946727752686\n",
      "Iteration 252 loss 1.4249345064163208\n",
      "Iteration 253 loss 1.4758808612823486\n",
      "Iteration 254 loss 1.4319502115249634\n",
      "Iteration 255 loss 1.4142547845840454\n",
      "Iteration 256 loss 1.2824028730392456\n",
      "Iteration 257 loss 1.3425790071487427\n",
      "Iteration 258 loss 1.3773329257965088\n",
      "Iteration 259 loss 1.2468457221984863\n",
      "Iteration 260 loss 1.3099113702774048\n",
      "Iteration 261 loss 1.2493895292282104\n",
      "Iteration 262 loss 1.3065677881240845\n",
      "Iteration 263 loss 1.1369984149932861\n",
      "Iteration 264 loss 1.2958674430847168\n",
      "Iteration 265 loss 1.1147160530090332\n",
      "Iteration 266 loss 1.2984706163406372\n",
      "Iteration 267 loss 1.2369524240493774\n",
      "Iteration 268 loss 1.2890024185180664\n",
      "Iteration 269 loss 1.2261110544204712\n",
      "Iteration 270 loss 1.2608113288879395\n",
      "Iteration 271 loss 1.1630932092666626\n",
      "Iteration 272 loss 1.190195918083191\n",
      "Iteration 273 loss 1.2438499927520752\n",
      "Iteration 274 loss 1.1485730409622192\n",
      "Iteration 275 loss 1.202502965927124\n",
      "Iteration 276 loss 1.1333696842193604\n",
      "Iteration 277 loss 1.206535816192627\n",
      "Iteration 278 loss 1.0792126655578613\n",
      "Iteration 279 loss 1.18340003490448\n",
      "Iteration 280 loss 1.0561623573303223\n",
      "Iteration 281 loss 1.1874388456344604\n",
      "Iteration 282 loss 1.1026488542556763\n",
      "Iteration 283 loss 1.01314115524292\n",
      "Iteration 284 loss 1.0752147436141968\n",
      "Iteration 285 loss 1.1317453384399414\n",
      "Iteration 286 loss 1.0503040552139282\n",
      "Iteration 287 loss 1.1225303411483765\n",
      "Iteration 288 loss 1.1010591983795166\n",
      "Iteration 289 loss 1.1298854351043701\n",
      "Iteration 290 loss 1.0322669744491577\n",
      "Iteration 291 loss 0.9642536044120789\n",
      "Iteration 292 loss 1.0132454633712769\n",
      "Iteration 293 loss 1.0481460094451904\n",
      "Iteration 294 loss 1.0148718357086182\n",
      "Iteration 295 loss 1.0016244649887085\n",
      "Iteration 296 loss 1.0218172073364258\n",
      "Iteration 297 loss 1.0778777599334717\n",
      "Iteration 298 loss 0.9881208539009094\n",
      "Iteration 299 loss 0.9298916459083557\n",
      "Iteration 300 loss 1.0387368202209473\n",
      "Iteration 301 loss 1.0445200204849243\n",
      "Iteration 302 loss 0.9583379030227661\n",
      "Iteration 303 loss 1.003678798675537\n",
      "Iteration 304 loss 0.9922271966934204\n",
      "Iteration 305 loss 1.0150038003921509\n",
      "Iteration 306 loss 0.9762386083602905\n",
      "Iteration 307 loss 0.9074071049690247\n",
      "Iteration 308 loss 1.0589134693145752\n",
      "Iteration 309 loss 0.980883777141571\n",
      "Iteration 310 loss 1.0013891458511353\n",
      "Iteration 311 loss 0.8907417058944702\n",
      "Iteration 312 loss 0.9992293119430542\n",
      "Iteration 313 loss 0.9133766889572144\n",
      "Iteration 314 loss 0.8335462808609009\n",
      "Iteration 315 loss 0.9165225625038147\n",
      "Iteration 316 loss 0.9587013125419617\n",
      "Iteration 317 loss 0.894917905330658\n",
      "Iteration 318 loss 0.8040730357170105\n",
      "Iteration 319 loss 0.8473631739616394\n",
      "Iteration 320 loss 0.9062696099281311\n",
      "Iteration 321 loss 0.8417444825172424\n",
      "Iteration 322 loss 0.8833656907081604\n",
      "Iteration 323 loss 0.9312554597854614\n",
      "Iteration 324 loss 0.7958977818489075\n",
      "Iteration 325 loss 0.8183006048202515\n",
      "Iteration 326 loss 0.8584887385368347\n",
      "Iteration 327 loss 0.795659065246582\n",
      "Iteration 328 loss 0.8202691078186035\n",
      "Iteration 329 loss 0.7515363693237305\n",
      "Iteration 330 loss 0.7277226448059082\n",
      "Iteration 331 loss 0.727279543876648\n",
      "Iteration 332 loss 0.8164389729499817\n",
      "Iteration 333 loss 0.8050630688667297\n",
      "Iteration 334 loss 0.811870813369751\n",
      "Iteration 335 loss 0.8496853113174438\n",
      "Iteration 336 loss 0.8332512974739075\n",
      "Iteration 337 loss 0.8073897361755371\n",
      "Iteration 338 loss 0.7099152207374573\n",
      "Iteration 339 loss 0.8495549559593201\n",
      "Iteration 340 loss 0.8432300090789795\n",
      "Iteration 341 loss 0.717074990272522\n",
      "Iteration 342 loss 0.8234810829162598\n",
      "Iteration 343 loss 0.7300654649734497\n",
      "Iteration 344 loss 0.6973851919174194\n",
      "Iteration 345 loss 0.7654531598091125\n",
      "Iteration 346 loss 0.7862164378166199\n",
      "Iteration 347 loss 0.6616012454032898\n",
      "Iteration 348 loss 0.7440796494483948\n",
      "Iteration 349 loss 0.6676893830299377\n",
      "Iteration 350 loss 0.7554149031639099\n",
      "Iteration 351 loss 0.6758747696876526\n",
      "Iteration 352 loss 0.7218217849731445\n",
      "Iteration 353 loss 0.6866316795349121\n",
      "Iteration 354 loss 0.7916705012321472\n",
      "Iteration 355 loss 0.7247396111488342\n",
      "Iteration 356 loss 0.7165584564208984\n",
      "Iteration 357 loss 0.8477013111114502\n",
      "Iteration 358 loss 0.7339922785758972\n",
      "Iteration 359 loss 0.6846051216125488\n",
      "Iteration 360 loss 0.6830690503120422\n",
      "Iteration 361 loss 0.674900233745575\n",
      "Iteration 362 loss 0.596328616142273\n",
      "Iteration 363 loss 0.6859955191612244\n",
      "Iteration 364 loss 0.5575588345527649\n",
      "Iteration 365 loss 0.683966875076294\n",
      "Iteration 366 loss 0.6207154393196106\n",
      "Iteration 367 loss 0.6327934861183167\n",
      "Iteration 368 loss 0.5797221064567566\n",
      "Iteration 369 loss 0.6370373368263245\n",
      "Iteration 370 loss 0.5905864238739014\n",
      "Iteration 371 loss 0.6489495038986206\n",
      "Iteration 372 loss 0.575589656829834\n",
      "Iteration 373 loss 0.630805492401123\n",
      "Iteration 374 loss 0.5865792632102966\n",
      "Iteration 375 loss 0.613113284111023\n",
      "Iteration 376 loss 0.5628029704093933\n",
      "Iteration 377 loss 0.6602977514266968\n",
      "Iteration 378 loss 0.5245527625083923\n",
      "Iteration 379 loss 0.7275134325027466\n",
      "Iteration 380 loss 0.638504147529602\n",
      "Iteration 381 loss 0.6399350166320801\n",
      "Iteration 382 loss 0.5772874355316162\n",
      "Iteration 383 loss 0.5999178290367126\n",
      "Iteration 384 loss 0.5526610612869263\n",
      "Iteration 385 loss 0.5683946013450623\n",
      "Iteration 386 loss 0.5879063010215759\n",
      "Iteration 387 loss 0.584253191947937\n",
      "Iteration 388 loss 0.5621575713157654\n",
      "Iteration 389 loss 0.5508072376251221\n",
      "Iteration 390 loss 0.546838641166687\n",
      "Iteration 391 loss 0.55720055103302\n",
      "Iteration 392 loss 0.5357446074485779\n",
      "Iteration 393 loss 0.6342275142669678\n",
      "Iteration 394 loss 0.5112591981887817\n",
      "Iteration 395 loss 0.49728208780288696\n",
      "Iteration 396 loss 0.4738866686820984\n",
      "Iteration 397 loss 0.5726796984672546\n",
      "Iteration 398 loss 0.5624536275863647\n",
      "Iteration 399 loss 0.6275457143783569\n",
      "Iteration 400 loss 0.533331036567688\n",
      "Iteration 401 loss 0.6085313558578491\n",
      "Iteration 402 loss 0.5165132284164429\n",
      "Iteration 403 loss 0.5403909087181091\n",
      "Iteration 404 loss 0.5819147229194641\n",
      "Iteration 405 loss 0.44843459129333496\n",
      "Iteration 406 loss 0.45322543382644653\n",
      "Iteration 407 loss 0.4799814224243164\n",
      "Iteration 408 loss 0.551252543926239\n",
      "Iteration 409 loss 0.5731794834136963\n",
      "Iteration 410 loss 0.5118834376335144\n",
      "Iteration 411 loss 0.6170864105224609\n",
      "Iteration 412 loss 0.543660581111908\n",
      "Iteration 413 loss 0.6034699082374573\n",
      "Iteration 414 loss 0.5503949522972107\n",
      "Iteration 415 loss 0.5415643453598022\n",
      "Iteration 416 loss 0.6209096312522888\n",
      "Iteration 417 loss 0.5083978176116943\n",
      "Iteration 418 loss 0.5939666628837585\n",
      "Iteration 419 loss 0.5341342687606812\n",
      "Iteration 420 loss 0.5626895427703857\n",
      "Iteration 421 loss 0.505046546459198\n",
      "Iteration 422 loss 0.5488311648368835\n",
      "Iteration 423 loss 0.5159541368484497\n",
      "Iteration 424 loss 0.5865641832351685\n",
      "Iteration 425 loss 0.5618104338645935\n",
      "Iteration 426 loss 0.5629172921180725\n",
      "Iteration 427 loss 0.4989963471889496\n",
      "Iteration 428 loss 0.6273533701896667\n",
      "Iteration 429 loss 0.5208393931388855\n",
      "Iteration 430 loss 0.6114062666893005\n",
      "Iteration 431 loss 0.5166327953338623\n",
      "Iteration 432 loss 0.5516349077224731\n",
      "Iteration 433 loss 0.48862528800964355\n",
      "Iteration 434 loss 0.5159181356430054\n",
      "Iteration 435 loss 0.5578538179397583\n",
      "Iteration 436 loss 0.5565678477287292\n",
      "Iteration 437 loss 0.5176321268081665\n",
      "Iteration 438 loss 0.4513660669326782\n",
      "Iteration 439 loss 0.5784377455711365\n",
      "Iteration 440 loss 0.6032863855361938\n",
      "Iteration 441 loss 0.5013453364372253\n",
      "Iteration 442 loss 0.5245676040649414\n",
      "Iteration 443 loss 0.5348957180976868\n",
      "Iteration 444 loss 0.4974575936794281\n",
      "Iteration 445 loss 0.5260547995567322\n",
      "Iteration 446 loss 0.4333754777908325\n",
      "Iteration 447 loss 0.595449686050415\n",
      "Iteration 448 loss 0.550451397895813\n",
      "Iteration 449 loss 0.51226407289505\n",
      "Iteration 450 loss 0.4498487412929535\n",
      "Iteration 451 loss 0.5180355906486511\n",
      "Iteration 452 loss 0.5338703989982605\n",
      "Iteration 453 loss 0.4940594732761383\n",
      "Iteration 454 loss 0.5219584107398987\n",
      "Iteration 455 loss 0.5249501466751099\n",
      "Iteration 456 loss 0.539689838886261\n",
      "Iteration 457 loss 0.5146791934967041\n",
      "Iteration 458 loss 0.4412330687046051\n",
      "Iteration 459 loss 0.5063921213150024\n",
      "Iteration 460 loss 0.5296317934989929\n",
      "Iteration 461 loss 0.5787186026573181\n",
      "Iteration 462 loss 0.4782349467277527\n",
      "Iteration 463 loss 0.5216282606124878\n",
      "Iteration 464 loss 0.47621041536331177\n",
      "Iteration 465 loss 0.5151189565658569\n",
      "Iteration 466 loss 0.4503973722457886\n",
      "Iteration 467 loss 0.5600804090499878\n",
      "Iteration 468 loss 0.4796980023384094\n",
      "Iteration 469 loss 0.523099422454834\n",
      "Iteration 470 loss 0.5460965037345886\n",
      "Iteration 471 loss 0.5127559304237366\n",
      "Iteration 472 loss 0.5134292840957642\n",
      "Iteration 473 loss 0.4662405848503113\n",
      "Iteration 474 loss 0.5576418042182922\n",
      "Iteration 475 loss 0.48052799701690674\n",
      "Iteration 476 loss 0.5389145612716675\n",
      "Iteration 477 loss 0.48789817094802856\n",
      "Iteration 478 loss 0.5162564516067505\n",
      "Iteration 479 loss 0.4949547052383423\n",
      "Iteration 480 loss 0.48165351152420044\n",
      "Iteration 481 loss 0.4917987585067749\n",
      "Iteration 482 loss 0.45059630274772644\n",
      "Iteration 483 loss 0.5092660784721375\n",
      "Iteration 484 loss 0.5607020854949951\n",
      "Iteration 485 loss 0.524242639541626\n",
      "Iteration 486 loss 0.5621110200881958\n",
      "Iteration 487 loss 0.5317622423171997\n",
      "Iteration 488 loss 0.49942973256111145\n",
      "Iteration 489 loss 0.4750756323337555\n",
      "Iteration 490 loss 0.5837064385414124\n",
      "Iteration 491 loss 0.4963483214378357\n",
      "Iteration 492 loss 0.5500973463058472\n",
      "Iteration 493 loss 0.5282612442970276\n",
      "Iteration 494 loss 0.519970715045929\n",
      "Iteration 495 loss 0.46180260181427\n",
      "Iteration 496 loss 0.5179834961891174\n",
      "Iteration 497 loss 0.6188089847564697\n",
      "Iteration 498 loss 0.47055038809776306\n",
      "Iteration 499 loss 0.4524014890193939\n",
      "Iteration 500 loss 0.5144655704498291\n",
      "Iteration 501 loss 0.4751468896865845\n",
      "Iteration 502 loss 0.4565119445323944\n",
      "Iteration 503 loss 0.5596212148666382\n",
      "Iteration 504 loss 0.5336944460868835\n",
      "Iteration 505 loss 0.435282438993454\n",
      "Iteration 506 loss 0.48815277218818665\n",
      "Iteration 507 loss 0.5318348407745361\n",
      "Iteration 508 loss 0.5491694211959839\n",
      "Iteration 509 loss 0.5343480706214905\n",
      "Iteration 510 loss 0.5387192964553833\n",
      "Iteration 511 loss 0.44233864545822144\n",
      "Iteration 512 loss 0.5143807530403137\n",
      "Iteration 513 loss 0.5040798783302307\n",
      "Iteration 514 loss 0.5041828155517578\n",
      "Iteration 515 loss 0.46953973174095154\n",
      "Iteration 516 loss 0.41722267866134644\n",
      "Iteration 517 loss 0.48281049728393555\n",
      "Iteration 518 loss 0.4762207567691803\n",
      "Iteration 519 loss 0.4911710023880005\n",
      "Iteration 520 loss 0.44405674934387207\n",
      "Iteration 521 loss 0.4952049255371094\n",
      "Iteration 522 loss 0.461363285779953\n",
      "Iteration 523 loss 0.45450541377067566\n",
      "Iteration 524 loss 0.5237831473350525\n",
      "Iteration 525 loss 0.5067129135131836\n",
      "Iteration 526 loss 0.4701138436794281\n",
      "Iteration 527 loss 0.5205737352371216\n",
      "Iteration 528 loss 0.5332646369934082\n",
      "Iteration 529 loss 0.5753318667411804\n",
      "Iteration 530 loss 0.45482516288757324\n",
      "Iteration 531 loss 0.6324169635772705\n",
      "Iteration 532 loss 0.5388630032539368\n",
      "Iteration 533 loss 0.542548418045044\n",
      "Iteration 534 loss 0.41274991631507874\n",
      "Iteration 535 loss 0.5396719574928284\n",
      "Iteration 536 loss 0.45462310314178467\n",
      "Iteration 537 loss 0.5254785418510437\n",
      "Iteration 538 loss 0.5214197635650635\n",
      "Iteration 539 loss 0.5125548243522644\n",
      "Iteration 540 loss 0.6009244918823242\n",
      "Iteration 541 loss 0.5328466892242432\n",
      "Iteration 542 loss 0.48769643902778625\n",
      "Iteration 543 loss 0.5172518491744995\n",
      "Iteration 544 loss 0.4885963201522827\n",
      "Iteration 545 loss 0.5168935656547546\n",
      "Iteration 546 loss 0.5024261474609375\n",
      "Iteration 547 loss 0.5241711735725403\n",
      "Iteration 548 loss 0.5097373723983765\n",
      "Iteration 549 loss 0.465849369764328\n",
      "Iteration 550 loss 0.5525425672531128\n",
      "Iteration 551 loss 0.5860281586647034\n",
      "Iteration 552 loss 0.4552161693572998\n",
      "Iteration 553 loss 0.5613346099853516\n",
      "Iteration 554 loss 0.5409687161445618\n",
      "Iteration 555 loss 0.5111746191978455\n",
      "Iteration 556 loss 0.49763187766075134\n",
      "Iteration 557 loss 0.5655278563499451\n",
      "Iteration 558 loss 0.5959392189979553\n",
      "Iteration 559 loss 0.5074845552444458\n",
      "Iteration 560 loss 0.4944201111793518\n",
      "Iteration 561 loss 0.5250253677368164\n",
      "Iteration 562 loss 0.4367803633213043\n",
      "Iteration 563 loss 0.49376487731933594\n",
      "Iteration 564 loss 0.4162975549697876\n",
      "Iteration 565 loss 0.4990538954734802\n",
      "Iteration 566 loss 0.49551016092300415\n",
      "Iteration 567 loss 0.43511420488357544\n",
      "Iteration 568 loss 0.5199931263923645\n",
      "Iteration 569 loss 0.48033326864242554\n",
      "Iteration 570 loss 0.49887189269065857\n",
      "Iteration 571 loss 0.5291344523429871\n",
      "Iteration 572 loss 0.44389721751213074\n",
      "Iteration 573 loss 0.5461911559104919\n",
      "Iteration 574 loss 0.5499414801597595\n",
      "Iteration 575 loss 0.45481443405151367\n",
      "Iteration 576 loss 0.5277981758117676\n",
      "Iteration 577 loss 0.5354069471359253\n",
      "Iteration 578 loss 0.5062084197998047\n",
      "Iteration 579 loss 0.4821191430091858\n",
      "Iteration 580 loss 0.4784144163131714\n",
      "Iteration 581 loss 0.5366928577423096\n",
      "Iteration 582 loss 0.43316859006881714\n",
      "Iteration 583 loss 0.5306174159049988\n",
      "Iteration 584 loss 0.5411632657051086\n",
      "Iteration 585 loss 0.49172651767730713\n",
      "Iteration 586 loss 0.5161373615264893\n",
      "Iteration 587 loss 0.5194439888000488\n",
      "Iteration 588 loss 0.5037179589271545\n",
      "Iteration 589 loss 0.5081751346588135\n",
      "Iteration 590 loss 0.474817156791687\n",
      "Iteration 591 loss 0.4735492467880249\n",
      "Iteration 592 loss 0.504130482673645\n",
      "Iteration 593 loss 0.4710838496685028\n",
      "Iteration 594 loss 0.5471436977386475\n",
      "Iteration 595 loss 0.5061880350112915\n",
      "Iteration 596 loss 0.44901758432388306\n",
      "Iteration 597 loss 0.4356650412082672\n",
      "Iteration 598 loss 0.4997870922088623\n",
      "Iteration 599 loss 0.47737908363342285\n",
      "Iteration 600 loss 0.4621336758136749\n",
      "Iteration 601 loss 0.5012169480323792\n",
      "Iteration 602 loss 0.5921192169189453\n",
      "Iteration 603 loss 0.4840030074119568\n",
      "Iteration 604 loss 0.5056444406509399\n",
      "Iteration 605 loss 0.44208642840385437\n",
      "Iteration 606 loss 0.5581411123275757\n",
      "Iteration 607 loss 0.5047376751899719\n",
      "Iteration 608 loss 0.42328080534935\n",
      "Iteration 609 loss 0.49085548520088196\n",
      "Iteration 610 loss 0.4264083206653595\n",
      "Iteration 611 loss 0.45249372720718384\n",
      "Iteration 612 loss 0.4846172332763672\n",
      "Iteration 613 loss 0.505961000919342\n",
      "Iteration 614 loss 0.5014211535453796\n",
      "Iteration 615 loss 0.4789985418319702\n",
      "Iteration 616 loss 0.46202829480171204\n",
      "Iteration 617 loss 0.4445945918560028\n",
      "Iteration 618 loss 0.4156346321105957\n",
      "Iteration 619 loss 0.5134206414222717\n",
      "Iteration 620 loss 0.48800137639045715\n",
      "Iteration 621 loss 0.4114606976509094\n",
      "Iteration 622 loss 0.44814333319664\n",
      "Iteration 623 loss 0.4867735505104065\n",
      "Iteration 624 loss 0.4800357222557068\n",
      "Iteration 625 loss 0.5166783928871155\n",
      "Iteration 626 loss 0.4684363305568695\n",
      "Iteration 627 loss 0.4803117513656616\n",
      "Iteration 628 loss 0.490139901638031\n",
      "Iteration 629 loss 0.49756062030792236\n",
      "Iteration 630 loss 0.44990241527557373\n",
      "Iteration 631 loss 0.44449687004089355\n",
      "Iteration 632 loss 0.561689555644989\n",
      "Iteration 633 loss 0.5472880601882935\n",
      "Iteration 634 loss 0.4464380741119385\n",
      "Iteration 635 loss 0.46392327547073364\n",
      "Iteration 636 loss 0.5398983359336853\n",
      "Iteration 637 loss 0.4709545373916626\n",
      "Iteration 638 loss 0.46161508560180664\n",
      "Iteration 639 loss 0.5126991868019104\n",
      "Iteration 640 loss 0.4711272716522217\n",
      "Iteration 641 loss 0.48343271017074585\n",
      "Iteration 642 loss 0.4644111394882202\n",
      "Iteration 643 loss 0.4155563712120056\n",
      "Iteration 644 loss 0.4462725818157196\n",
      "Iteration 645 loss 0.5126475095748901\n",
      "Iteration 646 loss 0.5351157784461975\n",
      "Iteration 647 loss 0.5201782584190369\n",
      "Iteration 648 loss 0.4829994738101959\n",
      "Iteration 649 loss 0.5168750286102295\n",
      "Iteration 650 loss 0.47233113646507263\n",
      "Iteration 651 loss 0.4834285080432892\n",
      "Iteration 652 loss 0.5012218356132507\n",
      "Iteration 653 loss 0.5761052370071411\n",
      "Iteration 654 loss 0.48753756284713745\n",
      "Iteration 655 loss 0.5797622203826904\n",
      "Iteration 656 loss 0.4205819368362427\n",
      "Iteration 657 loss 0.5672988891601562\n",
      "Iteration 658 loss 0.5035206079483032\n",
      "Iteration 659 loss 0.5139791965484619\n",
      "Iteration 660 loss 0.4538877010345459\n",
      "Iteration 661 loss 0.5652190446853638\n",
      "Iteration 662 loss 0.4378902018070221\n",
      "Iteration 663 loss 0.5016706585884094\n",
      "Iteration 664 loss 0.5655947923660278\n",
      "Iteration 665 loss 0.4903503358364105\n",
      "Iteration 666 loss 0.43203985691070557\n",
      "Iteration 667 loss 0.4600321352481842\n",
      "Iteration 668 loss 0.5319947600364685\n",
      "Iteration 669 loss 0.49060961604118347\n",
      "Iteration 670 loss 0.4741820693016052\n",
      "Iteration 671 loss 0.5907559394836426\n",
      "Iteration 672 loss 0.4132882058620453\n",
      "Iteration 673 loss 0.4038883447647095\n",
      "Iteration 674 loss 0.4696550965309143\n",
      "Iteration 675 loss 0.5141834020614624\n",
      "Iteration 676 loss 0.4490159749984741\n",
      "Iteration 677 loss 0.510993242263794\n",
      "Iteration 678 loss 0.43317684531211853\n",
      "Iteration 679 loss 0.4971250891685486\n",
      "Iteration 680 loss 0.40266475081443787\n",
      "Iteration 681 loss 0.47949227690696716\n",
      "Iteration 682 loss 0.41217195987701416\n",
      "Iteration 683 loss 0.5086435079574585\n",
      "Iteration 684 loss 0.42928752303123474\n",
      "Iteration 685 loss 0.45088809728622437\n",
      "Iteration 686 loss 0.47575169801712036\n",
      "Iteration 687 loss 0.45685285329818726\n",
      "Iteration 688 loss 0.40844351053237915\n",
      "Iteration 689 loss 0.3939511775970459\n",
      "Iteration 690 loss 0.42921286821365356\n",
      "Iteration 691 loss 0.4618494510650635\n",
      "Iteration 692 loss 0.5453413724899292\n",
      "Iteration 693 loss 0.4949037730693817\n",
      "Iteration 694 loss 0.42720985412597656\n",
      "Iteration 695 loss 0.4509415924549103\n",
      "Iteration 696 loss 0.45483651757240295\n",
      "Iteration 697 loss 0.44824734330177307\n",
      "Iteration 698 loss 0.48677149415016174\n",
      "Iteration 699 loss 0.4270990490913391\n",
      "Iteration 700 loss 0.45972561836242676\n",
      "Iteration 701 loss 0.4577123820781708\n",
      "Iteration 702 loss 0.48075124621391296\n",
      "Iteration 703 loss 0.4639703631401062\n",
      "Iteration 704 loss 0.4872257709503174\n",
      "Iteration 705 loss 0.4318070113658905\n",
      "Iteration 706 loss 0.521088182926178\n",
      "Iteration 707 loss 0.4732256233692169\n",
      "Iteration 708 loss 0.47363924980163574\n",
      "Iteration 709 loss 0.4688093364238739\n",
      "Iteration 710 loss 0.43118447065353394\n",
      "Iteration 711 loss 0.552824854850769\n",
      "Iteration 712 loss 0.51505047082901\n",
      "Iteration 713 loss 0.40220415592193604\n",
      "Iteration 714 loss 0.4913106858730316\n",
      "Iteration 715 loss 0.5294252038002014\n",
      "Iteration 716 loss 0.5179286599159241\n",
      "Iteration 717 loss 0.43816083669662476\n",
      "Iteration 718 loss 0.410546213388443\n",
      "Iteration 719 loss 0.4955131709575653\n",
      "Iteration 720 loss 0.4547755718231201\n",
      "Iteration 721 loss 0.5550035238265991\n",
      "Iteration 722 loss 0.4829138517379761\n",
      "Iteration 723 loss 0.5429539680480957\n",
      "Iteration 724 loss 0.49685752391815186\n",
      "Iteration 725 loss 0.46194979548454285\n",
      "Iteration 726 loss 0.43681421875953674\n",
      "Iteration 727 loss 0.5012302398681641\n",
      "Iteration 728 loss 0.4431195855140686\n",
      "Iteration 729 loss 0.4859658181667328\n",
      "Iteration 730 loss 0.42406219244003296\n",
      "Iteration 731 loss 0.46645089983940125\n",
      "Iteration 732 loss 0.4893360137939453\n",
      "Iteration 733 loss 0.5153557062149048\n",
      "Iteration 734 loss 0.4134024977684021\n",
      "Iteration 735 loss 0.40681910514831543\n",
      "Iteration 736 loss 0.45982834696769714\n",
      "Iteration 737 loss 0.5271120667457581\n",
      "Iteration 738 loss 0.446163147687912\n",
      "Iteration 739 loss 0.4641016721725464\n",
      "Iteration 740 loss 0.4262298047542572\n",
      "Iteration 741 loss 0.4280458986759186\n",
      "Iteration 742 loss 0.43266478180885315\n",
      "Iteration 743 loss 0.458967000246048\n",
      "Iteration 744 loss 0.4936532974243164\n",
      "Iteration 745 loss 0.5816135406494141\n",
      "Iteration 746 loss 0.4820462763309479\n",
      "Iteration 747 loss 0.451955109834671\n",
      "Iteration 748 loss 0.48624399304389954\n",
      "Iteration 749 loss 0.35497814416885376\n",
      "Iteration 750 loss 0.5045807361602783\n",
      "Iteration 751 loss 0.4887414574623108\n",
      "Iteration 752 loss 0.4441049098968506\n",
      "Iteration 753 loss 0.5400021076202393\n",
      "Iteration 754 loss 0.4022592604160309\n",
      "Iteration 755 loss 0.4438488185405731\n",
      "Iteration 756 loss 0.40226736664772034\n",
      "Iteration 757 loss 0.4144265055656433\n",
      "Iteration 758 loss 0.5390005111694336\n",
      "Iteration 759 loss 0.5164485573768616\n",
      "Iteration 760 loss 0.529612123966217\n",
      "Iteration 761 loss 0.5459375977516174\n",
      "Iteration 762 loss 0.5274586081504822\n",
      "Iteration 763 loss 0.5469057559967041\n",
      "Iteration 764 loss 0.4784591495990753\n",
      "Iteration 765 loss 0.4820704162120819\n",
      "Iteration 766 loss 0.44252198934555054\n",
      "Iteration 767 loss 0.4370214641094208\n",
      "Iteration 768 loss 0.4865165650844574\n",
      "Iteration 769 loss 0.4589894413948059\n",
      "Iteration 770 loss 0.4149750769138336\n",
      "Iteration 771 loss 0.48242154717445374\n",
      "Iteration 772 loss 0.4526011347770691\n",
      "Iteration 773 loss 0.46698352694511414\n",
      "Iteration 774 loss 0.4881666302680969\n",
      "Iteration 775 loss 0.4440295696258545\n",
      "Iteration 776 loss 0.485107958316803\n",
      "Iteration 777 loss 0.4644140303134918\n",
      "Iteration 778 loss 0.47487330436706543\n",
      "Iteration 779 loss 0.4837207496166229\n",
      "Iteration 780 loss 0.42672955989837646\n",
      "Iteration 781 loss 0.43442094326019287\n",
      "Iteration 782 loss 0.49459025263786316\n",
      "Iteration 783 loss 0.4263008236885071\n",
      "Iteration 784 loss 0.4004489779472351\n",
      "Iteration 785 loss 0.4262678921222687\n",
      "Iteration 786 loss 0.4989669620990753\n",
      "Iteration 787 loss 0.4435998499393463\n",
      "Iteration 788 loss 0.4350890517234802\n",
      "Iteration 789 loss 0.3715919554233551\n",
      "Iteration 790 loss 0.46890076994895935\n",
      "Iteration 791 loss 0.4641346335411072\n",
      "Iteration 792 loss 0.4323461055755615\n",
      "Iteration 793 loss 0.4403170645236969\n",
      "Iteration 794 loss 0.4438527822494507\n",
      "Iteration 795 loss 0.41481801867485046\n",
      "Iteration 796 loss 0.5045124292373657\n",
      "Iteration 797 loss 0.4631154239177704\n",
      "Iteration 798 loss 0.46813449263572693\n",
      "Iteration 799 loss 0.47358906269073486\n",
      "Iteration 800 loss 0.5219255685806274\n",
      "Iteration 801 loss 0.4343350827693939\n",
      "Iteration 802 loss 0.4471781849861145\n",
      "Iteration 803 loss 0.4128192365169525\n",
      "Iteration 804 loss 0.46848955750465393\n",
      "Iteration 805 loss 0.47753313183784485\n",
      "Iteration 806 loss 0.5557013154029846\n",
      "Iteration 807 loss 0.42483681440353394\n",
      "Iteration 808 loss 0.5096374750137329\n",
      "Iteration 809 loss 0.4287837743759155\n",
      "Iteration 810 loss 0.4974794387817383\n",
      "Iteration 811 loss 0.4084472060203552\n",
      "Iteration 812 loss 0.43877896666526794\n",
      "Iteration 813 loss 0.5031353235244751\n",
      "Iteration 814 loss 0.508734941482544\n",
      "Iteration 815 loss 0.521719753742218\n",
      "Iteration 816 loss 0.4683630168437958\n",
      "Iteration 817 loss 0.47822079062461853\n",
      "Iteration 818 loss 0.4640074372291565\n",
      "Iteration 819 loss 0.47478345036506653\n",
      "Iteration 820 loss 0.4708259403705597\n",
      "Iteration 821 loss 0.5168757438659668\n",
      "Iteration 822 loss 0.4223586320877075\n",
      "Iteration 823 loss 0.4606229364871979\n",
      "Iteration 824 loss 0.40559589862823486\n",
      "Iteration 825 loss 0.49593356251716614\n",
      "Iteration 826 loss 0.4169618487358093\n",
      "Iteration 827 loss 0.45368123054504395\n",
      "Iteration 828 loss 0.35679370164871216\n",
      "Iteration 829 loss 0.44585245847702026\n",
      "Iteration 830 loss 0.42073288559913635\n",
      "Iteration 831 loss 0.5018069744110107\n",
      "Iteration 832 loss 0.4316375255584717\n",
      "Iteration 833 loss 0.44389426708221436\n",
      "Iteration 834 loss 0.4562237560749054\n",
      "Iteration 835 loss 0.4918978810310364\n",
      "Iteration 836 loss 0.40168100595474243\n",
      "Iteration 837 loss 0.49545565247535706\n",
      "Iteration 838 loss 0.4964497983455658\n",
      "Iteration 839 loss 0.4947342276573181\n",
      "Iteration 840 loss 0.4767864942550659\n",
      "Iteration 841 loss 0.41586655378341675\n",
      "Iteration 842 loss 0.539401650428772\n",
      "Iteration 843 loss 0.44901812076568604\n",
      "Iteration 844 loss 0.4554809033870697\n",
      "Iteration 845 loss 0.4745011031627655\n",
      "Iteration 846 loss 0.4792397916316986\n",
      "Iteration 847 loss 0.4566405117511749\n",
      "Iteration 848 loss 0.4126501977443695\n",
      "Iteration 849 loss 0.44208958745002747\n",
      "Iteration 850 loss 0.4807544946670532\n",
      "Iteration 851 loss 0.49453988671302795\n",
      "Iteration 852 loss 0.43992143869400024\n",
      "Iteration 853 loss 0.47867125272750854\n",
      "Iteration 854 loss 0.4367597699165344\n",
      "Iteration 855 loss 0.4580269157886505\n",
      "Iteration 856 loss 0.5088055729866028\n",
      "Iteration 857 loss 0.47757771611213684\n",
      "Iteration 858 loss 0.4685443639755249\n",
      "Iteration 859 loss 0.41542017459869385\n",
      "Iteration 860 loss 0.48327329754829407\n",
      "Iteration 861 loss 0.40318769216537476\n",
      "Iteration 862 loss 0.4704540967941284\n",
      "Iteration 863 loss 0.4179212152957916\n",
      "Iteration 864 loss 0.50042724609375\n",
      "Iteration 865 loss 0.47712716460227966\n",
      "Iteration 866 loss 0.4952043294906616\n",
      "Iteration 867 loss 0.5140475630760193\n",
      "Iteration 868 loss 0.4240874648094177\n",
      "Iteration 869 loss 0.45246490836143494\n",
      "Iteration 870 loss 0.44936636090278625\n",
      "Iteration 871 loss 0.4778052568435669\n",
      "Iteration 872 loss 0.48764023184776306\n",
      "Iteration 873 loss 0.4497181475162506\n",
      "Iteration 874 loss 0.46355023980140686\n",
      "Iteration 875 loss 0.452640563249588\n",
      "Iteration 876 loss 0.43518730998039246\n",
      "Iteration 877 loss 0.4398688077926636\n",
      "Iteration 878 loss 0.5085954070091248\n",
      "Iteration 879 loss 0.3969244360923767\n",
      "Iteration 880 loss 0.5538023710250854\n",
      "Iteration 881 loss 0.4211835265159607\n",
      "Iteration 882 loss 0.4349702298641205\n",
      "Iteration 883 loss 0.48468899726867676\n",
      "Iteration 884 loss 0.45383286476135254\n",
      "Iteration 885 loss 0.47442370653152466\n",
      "Iteration 886 loss 0.472320556640625\n",
      "Iteration 887 loss 0.47430220246315\n",
      "Iteration 888 loss 0.4601897895336151\n",
      "Iteration 889 loss 0.4818344712257385\n",
      "Iteration 890 loss 0.3553014397621155\n",
      "Iteration 891 loss 0.45337122678756714\n",
      "Iteration 892 loss 0.4763452410697937\n",
      "Iteration 893 loss 0.500557005405426\n",
      "Iteration 894 loss 0.4396834969520569\n",
      "Iteration 895 loss 0.41047364473342896\n",
      "Iteration 896 loss 0.47969314455986023\n",
      "Iteration 897 loss 0.435646653175354\n",
      "Iteration 898 loss 0.416020005941391\n",
      "Iteration 899 loss 0.4506283402442932\n",
      "Iteration 900 loss 0.5069705247879028\n",
      "Iteration 901 loss 0.42156052589416504\n",
      "Iteration 902 loss 0.44917452335357666\n",
      "Iteration 903 loss 0.4434109330177307\n",
      "Iteration 904 loss 0.42838871479034424\n",
      "Iteration 905 loss 0.4251364469528198\n",
      "Iteration 906 loss 0.5152739882469177\n",
      "Iteration 907 loss 0.4562011957168579\n",
      "Iteration 908 loss 0.47136056423187256\n",
      "Iteration 909 loss 0.4698702394962311\n",
      "Iteration 910 loss 0.4528157711029053\n",
      "Iteration 911 loss 0.40785762667655945\n",
      "Iteration 912 loss 0.4389009475708008\n",
      "Iteration 913 loss 0.3590523600578308\n",
      "Iteration 914 loss 0.4003585875034332\n",
      "Iteration 915 loss 0.4529281556606293\n",
      "Iteration 916 loss 0.465053915977478\n",
      "Iteration 917 loss 0.4509374499320984\n",
      "Iteration 918 loss 0.45036813616752625\n",
      "Iteration 919 loss 0.39353740215301514\n",
      "Iteration 920 loss 0.47889426350593567\n",
      "Iteration 921 loss 0.4568096399307251\n",
      "Iteration 922 loss 0.5087343454360962\n",
      "Iteration 923 loss 0.44587448239326477\n",
      "Iteration 924 loss 0.47103527188301086\n",
      "Iteration 925 loss 0.4681971073150635\n",
      "Iteration 926 loss 0.4220786690711975\n",
      "Iteration 927 loss 0.48312807083129883\n",
      "Iteration 928 loss 0.4723975658416748\n",
      "Iteration 929 loss 0.40675264596939087\n",
      "Iteration 930 loss 0.408324658870697\n",
      "Iteration 931 loss 0.4003635346889496\n",
      "Iteration 932 loss 0.412056028842926\n",
      "Iteration 933 loss 0.44775691628456116\n",
      "Iteration 934 loss 0.4347217082977295\n",
      "Iteration 935 loss 0.43283843994140625\n",
      "Iteration 936 loss 0.4379899799823761\n",
      "Iteration 937 loss 0.4673433005809784\n",
      "Iteration 938 loss 0.4350178837776184\n",
      "Iteration 939 loss 0.48497676849365234\n",
      "Iteration 940 loss 0.4498128294944763\n",
      "Iteration 941 loss 0.4459397494792938\n",
      "Iteration 942 loss 0.4143652319908142\n",
      "Iteration 943 loss 0.41533535718917847\n",
      "Iteration 944 loss 0.43223631381988525\n",
      "Iteration 945 loss 0.4925166964530945\n",
      "Iteration 946 loss 0.44947195053100586\n",
      "Iteration 947 loss 0.4732145071029663\n",
      "Iteration 948 loss 0.4622102379798889\n",
      "Iteration 949 loss 0.5038952231407166\n",
      "Iteration 950 loss 0.37704145908355713\n",
      "Iteration 951 loss 0.4027829170227051\n",
      "Iteration 952 loss 0.39846232533454895\n",
      "Iteration 953 loss 0.5051143765449524\n",
      "Iteration 954 loss 0.3956279754638672\n",
      "Iteration 955 loss 0.4304601848125458\n",
      "Iteration 956 loss 0.42669665813446045\n",
      "Iteration 957 loss 0.4237818717956543\n",
      "Iteration 958 loss 0.4597622752189636\n",
      "Iteration 959 loss 0.4358002245426178\n",
      "Iteration 960 loss 0.4455954134464264\n",
      "Iteration 961 loss 0.4962964653968811\n",
      "Iteration 962 loss 0.46960943937301636\n",
      "Iteration 963 loss 0.4770631194114685\n",
      "Iteration 964 loss 0.5131419897079468\n",
      "Iteration 965 loss 0.48551082611083984\n",
      "Iteration 966 loss 0.4772217571735382\n",
      "Iteration 967 loss 0.4778459966182709\n",
      "Iteration 968 loss 0.4108331501483917\n",
      "Iteration 969 loss 0.4425427317619324\n",
      "Iteration 970 loss 0.4458564519882202\n",
      "Iteration 971 loss 0.47943398356437683\n",
      "Iteration 972 loss 0.4623734951019287\n",
      "Iteration 973 loss 0.4050743579864502\n",
      "Iteration 974 loss 0.464588463306427\n",
      "Iteration 975 loss 0.4914839267730713\n",
      "Iteration 976 loss 0.43982505798339844\n",
      "Iteration 977 loss 0.4617359936237335\n",
      "Iteration 978 loss 0.4387572407722473\n",
      "Iteration 979 loss 0.3971315920352936\n",
      "Iteration 980 loss 0.47106635570526123\n",
      "Iteration 981 loss 0.43665286898612976\n",
      "Iteration 982 loss 0.4695822596549988\n",
      "Iteration 983 loss 0.4013224244117737\n",
      "Iteration 984 loss 0.3910762369632721\n",
      "Iteration 985 loss 0.42522257566452026\n",
      "Iteration 986 loss 0.39316079020500183\n",
      "Iteration 987 loss 0.46663761138916016\n",
      "Iteration 988 loss 0.4181666374206543\n",
      "Iteration 989 loss 0.5005943775177002\n",
      "Iteration 990 loss 0.4518403112888336\n",
      "Iteration 991 loss 0.4560505747795105\n",
      "Iteration 992 loss 0.5111996531486511\n",
      "Iteration 993 loss 0.49799999594688416\n",
      "Iteration 994 loss 0.45270270109176636\n",
      "Iteration 995 loss 0.5085995197296143\n",
      "Iteration 996 loss 0.3548330068588257\n",
      "Iteration 997 loss 0.455199658870697\n",
      "Iteration 998 loss 0.4515504240989685\n",
      "Iteration 999 loss 0.42486703395843506\n",
      "Iteration 1000 loss 0.4479672312736511\n",
      "Iteration 1001 loss 0.4103148579597473\n",
      "Iteration 1002 loss 0.44134461879730225\n",
      "Iteration 1003 loss 0.46271637082099915\n",
      "Iteration 1004 loss 0.47127076983451843\n",
      "Iteration 1005 loss 0.4713580012321472\n",
      "Iteration 1006 loss 0.40700697898864746\n",
      "Iteration 1007 loss 0.3911065459251404\n",
      "Iteration 1008 loss 0.44078177213668823\n",
      "Iteration 1009 loss 0.3962351083755493\n",
      "Iteration 1010 loss 0.40045854449272156\n",
      "Iteration 1011 loss 0.4976707100868225\n",
      "Iteration 1012 loss 0.4440253674983978\n",
      "Iteration 1013 loss 0.43520599603652954\n",
      "Iteration 1014 loss 0.4340876042842865\n",
      "Iteration 1015 loss 0.41769230365753174\n",
      "Iteration 1016 loss 0.4067922830581665\n",
      "Iteration 1017 loss 0.4606072008609772\n",
      "Iteration 1018 loss 0.4236944019794464\n",
      "Iteration 1019 loss 0.4098113477230072\n",
      "Iteration 1020 loss 0.4398837089538574\n",
      "Iteration 1021 loss 0.3299931287765503\n",
      "Iteration 1022 loss 0.42153847217559814\n",
      "Iteration 1023 loss 0.46589764952659607\n",
      "Iteration 1024 loss 0.40111008286476135\n",
      "Iteration 1025 loss 0.45917564630508423\n",
      "Iteration 1026 loss 0.47217899560928345\n",
      "Iteration 1027 loss 0.48276394605636597\n",
      "Iteration 1028 loss 0.44253239035606384\n",
      "Iteration 1029 loss 0.4169257581233978\n",
      "Iteration 1030 loss 0.38897931575775146\n",
      "Iteration 1031 loss 0.4589127004146576\n",
      "Iteration 1032 loss 0.49476081132888794\n",
      "Iteration 1033 loss 0.3308734595775604\n",
      "Iteration 1034 loss 0.4132002592086792\n",
      "Iteration 1035 loss 0.44881418347358704\n",
      "Iteration 1036 loss 0.4918670654296875\n",
      "Iteration 1037 loss 0.521152675151825\n",
      "Iteration 1038 loss 0.4432690441608429\n",
      "Iteration 1039 loss 0.42046239972114563\n",
      "Iteration 1040 loss 0.3718216121196747\n",
      "Iteration 1041 loss 0.42869508266448975\n",
      "Iteration 1042 loss 0.4545135200023651\n",
      "Iteration 1043 loss 0.41718417406082153\n",
      "Iteration 1044 loss 0.43452590703964233\n",
      "Iteration 1045 loss 0.45442649722099304\n",
      "Iteration 1046 loss 0.39203202724456787\n",
      "Iteration 1047 loss 0.4213300943374634\n",
      "Iteration 1048 loss 0.43758246302604675\n",
      "Iteration 1049 loss 0.39781567454338074\n",
      "Iteration 1050 loss 0.43538475036621094\n",
      "Iteration 1051 loss 0.4070633053779602\n",
      "Iteration 1052 loss 0.36522164940834045\n",
      "Iteration 1053 loss 0.4374881982803345\n",
      "Iteration 1054 loss 0.4119968116283417\n",
      "Iteration 1055 loss 0.40747445821762085\n",
      "Iteration 1056 loss 0.384888231754303\n",
      "Iteration 1057 loss 0.44698959589004517\n",
      "Iteration 1058 loss 0.35075172781944275\n",
      "Iteration 1059 loss 0.3910035192966461\n",
      "Iteration 1060 loss 0.45247340202331543\n",
      "Iteration 1061 loss 0.43206363916397095\n",
      "Iteration 1062 loss 0.4360235333442688\n",
      "Iteration 1063 loss 0.4384797215461731\n",
      "Iteration 1064 loss 0.37364834547042847\n",
      "Iteration 1065 loss 0.4405304193496704\n",
      "Iteration 1066 loss 0.448302298784256\n",
      "Iteration 1067 loss 0.4147740602493286\n",
      "Iteration 1068 loss 0.4614781141281128\n",
      "Iteration 1069 loss 0.42435503005981445\n",
      "Iteration 1070 loss 0.47920048236846924\n",
      "Iteration 1071 loss 0.4395085871219635\n",
      "Iteration 1072 loss 0.398137629032135\n",
      "Iteration 1073 loss 0.46301305294036865\n",
      "Iteration 1074 loss 0.4158317446708679\n",
      "Iteration 1075 loss 0.4408033788204193\n",
      "Iteration 1076 loss 0.472250759601593\n",
      "Iteration 1077 loss 0.43613338470458984\n",
      "Iteration 1078 loss 0.40002402663230896\n",
      "Iteration 1079 loss 0.5034615993499756\n",
      "Iteration 1080 loss 0.43029454350471497\n",
      "Iteration 1081 loss 0.4571361243724823\n",
      "Iteration 1082 loss 0.42470604181289673\n",
      "Iteration 1083 loss 0.4396953880786896\n",
      "Iteration 1084 loss 0.4605925679206848\n",
      "Iteration 1085 loss 0.5371878743171692\n",
      "Iteration 1086 loss 0.41598057746887207\n",
      "Iteration 1087 loss 0.462485671043396\n",
      "Iteration 1088 loss 0.43549972772598267\n",
      "Iteration 1089 loss 0.48295193910598755\n",
      "Iteration 1090 loss 0.39103952050209045\n",
      "Iteration 1091 loss 0.431880921125412\n",
      "Iteration 1092 loss 0.3916023373603821\n",
      "Iteration 1093 loss 0.4357457160949707\n",
      "Iteration 1094 loss 0.5008950233459473\n",
      "Iteration 1095 loss 0.4212493896484375\n",
      "Iteration 1096 loss 0.43939170241355896\n",
      "Iteration 1097 loss 0.4630579650402069\n",
      "Iteration 1098 loss 0.4207225441932678\n",
      "Iteration 1099 loss 0.3762766718864441\n",
      "Iteration 1100 loss 0.43737250566482544\n",
      "Iteration 1101 loss 0.39633244276046753\n",
      "Iteration 1102 loss 0.4671829342842102\n",
      "Iteration 1103 loss 0.402898371219635\n",
      "Iteration 1104 loss 0.4524589776992798\n",
      "Iteration 1105 loss 0.4519968330860138\n",
      "Iteration 1106 loss 0.38341188430786133\n",
      "Iteration 1107 loss 0.5275669693946838\n",
      "Iteration 1108 loss 0.3600054383277893\n",
      "Iteration 1109 loss 0.44150859117507935\n",
      "Iteration 1110 loss 0.428214967250824\n",
      "Iteration 1111 loss 0.4188153147697449\n",
      "Iteration 1112 loss 0.3918232321739197\n",
      "Iteration 1113 loss 0.43153998255729675\n",
      "Iteration 1114 loss 0.40732866525650024\n",
      "Iteration 1115 loss 0.4334503412246704\n",
      "Iteration 1116 loss 0.4105668067932129\n",
      "Iteration 1117 loss 0.37571975588798523\n",
      "Iteration 1118 loss 0.4123029112815857\n",
      "Iteration 1119 loss 0.4890744686126709\n",
      "Iteration 1120 loss 0.415487676858902\n",
      "Iteration 1121 loss 0.4611857831478119\n",
      "Iteration 1122 loss 0.5270691514015198\n",
      "Iteration 1123 loss 0.4576055705547333\n",
      "Iteration 1124 loss 0.4857187271118164\n",
      "Iteration 1125 loss 0.4306451678276062\n",
      "Iteration 1126 loss 0.3974054455757141\n",
      "Iteration 1127 loss 0.43133288621902466\n",
      "Iteration 1128 loss 0.485465943813324\n",
      "Iteration 1129 loss 0.42603933811187744\n",
      "Iteration 1130 loss 0.46779417991638184\n",
      "Iteration 1131 loss 0.3902418911457062\n",
      "Iteration 1132 loss 0.4382217824459076\n",
      "Iteration 1133 loss 0.45432960987091064\n",
      "Iteration 1134 loss 0.44360342621803284\n",
      "Iteration 1135 loss 0.4400499761104584\n",
      "Iteration 1136 loss 0.42423930764198303\n",
      "Iteration 1137 loss 0.3507554233074188\n",
      "Iteration 1138 loss 0.47279223799705505\n",
      "Iteration 1139 loss 0.41191568970680237\n",
      "Iteration 1140 loss 0.4161524176597595\n",
      "Iteration 1141 loss 0.4904019236564636\n",
      "Iteration 1142 loss 0.45464304089546204\n",
      "Iteration 1143 loss 0.4952618479728699\n",
      "Iteration 1144 loss 0.3983548879623413\n",
      "Iteration 1145 loss 0.3926102817058563\n",
      "Iteration 1146 loss 0.4034649133682251\n",
      "Iteration 1147 loss 0.44817429780960083\n",
      "Iteration 1148 loss 0.41757795214653015\n",
      "Iteration 1149 loss 0.45412200689315796\n",
      "Iteration 1150 loss 0.41610783338546753\n",
      "Iteration 1151 loss 0.386074423789978\n",
      "Iteration 1152 loss 0.45401012897491455\n",
      "Iteration 1153 loss 0.4110206663608551\n",
      "Iteration 1154 loss 0.4413987696170807\n",
      "Iteration 1155 loss 0.5013577938079834\n",
      "Iteration 1156 loss 0.42707395553588867\n",
      "Iteration 1157 loss 0.44071662425994873\n",
      "Iteration 1158 loss 0.43922632932662964\n",
      "Iteration 1159 loss 0.4119940400123596\n",
      "Iteration 1160 loss 0.4347476661205292\n",
      "Iteration 1161 loss 0.4584306478500366\n",
      "Iteration 1162 loss 0.42906200885772705\n",
      "Iteration 1163 loss 0.3589829206466675\n",
      "Iteration 1164 loss 0.4529273211956024\n",
      "Iteration 1165 loss 0.3943071961402893\n",
      "Iteration 1166 loss 0.48164331912994385\n",
      "Iteration 1167 loss 0.4550265669822693\n",
      "Iteration 1168 loss 0.45908620953559875\n",
      "Iteration 1169 loss 0.37424343824386597\n",
      "Iteration 1170 loss 0.4332869052886963\n",
      "Iteration 1171 loss 0.45126745104789734\n",
      "Iteration 1172 loss 0.37952497601509094\n",
      "Iteration 1173 loss 0.43761831521987915\n",
      "Iteration 1174 loss 0.47508788108825684\n",
      "Iteration 1175 loss 0.4531756043434143\n",
      "Iteration 1176 loss 0.3892531394958496\n",
      "Iteration 1177 loss 0.44985419511795044\n",
      "Iteration 1178 loss 0.41674038767814636\n",
      "Iteration 1179 loss 0.47199079394340515\n",
      "Iteration 1180 loss 0.40214765071868896\n",
      "Iteration 1181 loss 0.4234107732772827\n",
      "Iteration 1182 loss 0.4220607280731201\n",
      "Iteration 1183 loss 0.41600844264030457\n",
      "Iteration 1184 loss 0.3822954297065735\n",
      "Iteration 1185 loss 0.42606526613235474\n",
      "Iteration 1186 loss 0.4386875033378601\n",
      "Iteration 1187 loss 0.4561910927295685\n",
      "Iteration 1188 loss 0.4004846513271332\n",
      "Iteration 1189 loss 0.4074220061302185\n",
      "Iteration 1190 loss 0.41154077649116516\n",
      "Iteration 1191 loss 0.44599178433418274\n",
      "Iteration 1192 loss 0.3739255368709564\n",
      "Iteration 1193 loss 0.4555598497390747\n",
      "Iteration 1194 loss 0.36808183789253235\n",
      "Iteration 1195 loss 0.4065306782722473\n",
      "Iteration 1196 loss 0.37734687328338623\n",
      "Iteration 1197 loss 0.4532202482223511\n",
      "Iteration 1198 loss 0.36998996138572693\n",
      "Iteration 1199 loss 0.34613198041915894\n",
      "Iteration 1200 loss 0.36217617988586426\n",
      "Iteration 1201 loss 0.41063180565834045\n",
      "Iteration 1202 loss 0.3663555383682251\n",
      "Iteration 1203 loss 0.41287466883659363\n",
      "Iteration 1204 loss 0.41322505474090576\n",
      "Iteration 1205 loss 0.3883480727672577\n",
      "Iteration 1206 loss 0.40091472864151\n",
      "Iteration 1207 loss 0.41624194383621216\n",
      "Iteration 1208 loss 0.37539008259773254\n",
      "Iteration 1209 loss 0.40697649121284485\n",
      "Iteration 1210 loss 0.38372284173965454\n",
      "Iteration 1211 loss 0.41249585151672363\n",
      "Iteration 1212 loss 0.3865416347980499\n",
      "Iteration 1213 loss 0.47573208808898926\n",
      "Iteration 1214 loss 0.4534321427345276\n",
      "Iteration 1215 loss 0.41195887327194214\n",
      "Iteration 1216 loss 0.39532315731048584\n",
      "Iteration 1217 loss 0.38542166352272034\n",
      "Iteration 1218 loss 0.4425192177295685\n",
      "Iteration 1219 loss 0.38630205392837524\n",
      "Iteration 1220 loss 0.4806518256664276\n",
      "Iteration 1221 loss 0.3734208643436432\n",
      "Iteration 1222 loss 0.4181438088417053\n",
      "Iteration 1223 loss 0.3880946636199951\n",
      "Iteration 1224 loss 0.41987258195877075\n",
      "Iteration 1225 loss 0.3686003088951111\n",
      "Iteration 1226 loss 0.4128689765930176\n",
      "Iteration 1227 loss 0.43555891513824463\n",
      "Iteration 1228 loss 0.40476110577583313\n",
      "Iteration 1229 loss 0.4538460075855255\n",
      "Iteration 1230 loss 0.4003406763076782\n",
      "Iteration 1231 loss 0.4665089547634125\n",
      "Iteration 1232 loss 0.4427866041660309\n",
      "Iteration 1233 loss 0.3386780619621277\n",
      "Iteration 1234 loss 0.4154922664165497\n",
      "Iteration 1235 loss 0.4365130662918091\n",
      "Iteration 1236 loss 0.4912617802619934\n",
      "Iteration 1237 loss 0.4883221983909607\n",
      "Iteration 1238 loss 0.41893306374549866\n",
      "Iteration 1239 loss 0.4075149595737457\n",
      "Iteration 1240 loss 0.4315090477466583\n",
      "Iteration 1241 loss 0.3953901529312134\n",
      "Iteration 1242 loss 0.4308263957500458\n",
      "Iteration 1243 loss 0.41662323474884033\n",
      "Iteration 1244 loss 0.3945247232913971\n",
      "Iteration 1245 loss 0.4230564832687378\n",
      "Iteration 1246 loss 0.4102727174758911\n",
      "Iteration 1247 loss 0.44789206981658936\n",
      "Iteration 1248 loss 0.4648742973804474\n",
      "Iteration 1249 loss 0.4660545885562897\n",
      "Iteration 1250 loss 0.45404207706451416\n",
      "Iteration 1251 loss 0.4414212703704834\n",
      "Iteration 1252 loss 0.4657822251319885\n",
      "Iteration 1253 loss 0.4292103052139282\n",
      "Iteration 1254 loss 0.4547673165798187\n",
      "Iteration 1255 loss 0.43648430705070496\n",
      "Iteration 1256 loss 0.3976839482784271\n",
      "Iteration 1257 loss 0.45223528146743774\n",
      "Iteration 1258 loss 0.39588189125061035\n",
      "Iteration 1259 loss 0.47325360774993896\n",
      "Iteration 1260 loss 0.39030128717422485\n",
      "Iteration 1261 loss 0.38284555077552795\n",
      "Iteration 1262 loss 0.41974103450775146\n",
      "Iteration 1263 loss 0.455772340297699\n",
      "Iteration 1264 loss 0.4251111149787903\n",
      "Iteration 1265 loss 0.37945055961608887\n",
      "Iteration 1266 loss 0.4593937397003174\n",
      "Iteration 1267 loss 0.4261251986026764\n",
      "Iteration 1268 loss 0.36649104952812195\n",
      "Iteration 1269 loss 0.39838963747024536\n",
      "Iteration 1270 loss 0.41619038581848145\n",
      "Iteration 1271 loss 0.4465372860431671\n",
      "Iteration 1272 loss 0.46066486835479736\n",
      "Iteration 1273 loss 0.3821312487125397\n",
      "Iteration 1274 loss 0.46705594658851624\n",
      "Iteration 1275 loss 0.4232789874076843\n",
      "Iteration 1276 loss 0.4345209300518036\n",
      "Iteration 1277 loss 0.47003334760665894\n",
      "Iteration 1278 loss 0.47435295581817627\n",
      "Iteration 1279 loss 0.38023054599761963\n",
      "Iteration 1280 loss 0.5091351270675659\n",
      "Iteration 1281 loss 0.4328165650367737\n",
      "Iteration 1282 loss 0.478228360414505\n",
      "Iteration 1283 loss 0.3585541248321533\n",
      "Iteration 1284 loss 0.4052490293979645\n",
      "Iteration 1285 loss 0.4450572431087494\n",
      "Iteration 1286 loss 0.45144492387771606\n",
      "Iteration 1287 loss 0.40566593408584595\n",
      "Iteration 1288 loss 0.46573227643966675\n",
      "Iteration 1289 loss 0.4348953068256378\n",
      "Iteration 1290 loss 0.4175216555595398\n",
      "Iteration 1291 loss 0.4575808644294739\n",
      "Iteration 1292 loss 0.46352654695510864\n",
      "Iteration 1293 loss 0.42570585012435913\n",
      "Iteration 1294 loss 0.4274391531944275\n",
      "Iteration 1295 loss 0.3827773928642273\n",
      "Iteration 1296 loss 0.39255163073539734\n",
      "Iteration 1297 loss 0.4209043085575104\n",
      "Iteration 1298 loss 0.36438506841659546\n",
      "Iteration 1299 loss 0.4316291809082031\n",
      "Iteration 1300 loss 0.3757595419883728\n",
      "Iteration 1301 loss 0.4773867130279541\n",
      "Iteration 1302 loss 0.444038450717926\n",
      "Iteration 1303 loss 0.45169639587402344\n",
      "Iteration 1304 loss 0.4039253890514374\n",
      "Iteration 1305 loss 0.45314180850982666\n",
      "Iteration 1306 loss 0.3994830846786499\n",
      "Iteration 1307 loss 0.41741305589675903\n",
      "Iteration 1308 loss 0.4411054253578186\n",
      "Iteration 1309 loss 0.3880212604999542\n",
      "Iteration 1310 loss 0.4504825472831726\n",
      "Iteration 1311 loss 0.3864806294441223\n",
      "Iteration 1312 loss 0.417684406042099\n",
      "Iteration 1313 loss 0.4230513274669647\n",
      "Iteration 1314 loss 0.40927961468696594\n",
      "Iteration 1315 loss 0.4096217155456543\n",
      "Iteration 1316 loss 0.38564664125442505\n",
      "Iteration 1317 loss 0.48053959012031555\n",
      "Iteration 1318 loss 0.43747973442077637\n",
      "Iteration 1319 loss 0.3990079164505005\n",
      "Iteration 1320 loss 0.4191382825374603\n",
      "Iteration 1321 loss 0.3815222978591919\n",
      "Iteration 1322 loss 0.37713584303855896\n",
      "Iteration 1323 loss 0.3586093783378601\n",
      "Iteration 1324 loss 0.42746755480766296\n",
      "Iteration 1325 loss 0.48423582315444946\n",
      "Iteration 1326 loss 0.443805068731308\n",
      "Iteration 1327 loss 0.39654362201690674\n",
      "Iteration 1328 loss 0.4397680163383484\n",
      "Iteration 1329 loss 0.43805715441703796\n",
      "Iteration 1330 loss 0.3542064428329468\n",
      "Iteration 1331 loss 0.4173302948474884\n",
      "Iteration 1332 loss 0.38417574763298035\n",
      "Iteration 1333 loss 0.4839700758457184\n",
      "Iteration 1334 loss 0.3636127710342407\n",
      "Iteration 1335 loss 0.3860401511192322\n",
      "Iteration 1336 loss 0.39404118061065674\n",
      "Iteration 1337 loss 0.42043277621269226\n",
      "Iteration 1338 loss 0.40459784865379333\n",
      "Iteration 1339 loss 0.416972815990448\n",
      "Iteration 1340 loss 0.3818811774253845\n",
      "Iteration 1341 loss 0.447677880525589\n",
      "Iteration 1342 loss 0.3713795840740204\n",
      "Iteration 1343 loss 0.4537431299686432\n",
      "Iteration 1344 loss 0.37442758679389954\n",
      "Iteration 1345 loss 0.41660720109939575\n",
      "Iteration 1346 loss 0.42031246423721313\n",
      "Iteration 1347 loss 0.4308471977710724\n",
      "Iteration 1348 loss 0.42005226016044617\n",
      "Iteration 1349 loss 0.46589502692222595\n",
      "Iteration 1350 loss 0.3674824833869934\n",
      "Iteration 1351 loss 0.48220890760421753\n",
      "Iteration 1352 loss 0.4030904471874237\n",
      "Iteration 1353 loss 0.41583219170570374\n",
      "Iteration 1354 loss 0.40706324577331543\n",
      "Iteration 1355 loss 0.3665255010128021\n",
      "Iteration 1356 loss 0.4515218138694763\n",
      "Iteration 1357 loss 0.39174947142601013\n",
      "Iteration 1358 loss 0.4291836619377136\n",
      "Iteration 1359 loss 0.398139625787735\n",
      "Iteration 1360 loss 0.3599242866039276\n",
      "Iteration 1361 loss 0.45449405908584595\n",
      "Iteration 1362 loss 0.41884559392929077\n",
      "Iteration 1363 loss 0.4083043336868286\n",
      "Iteration 1364 loss 0.4060664474964142\n",
      "Iteration 1365 loss 0.4227617383003235\n",
      "Iteration 1366 loss 0.3815491795539856\n",
      "Iteration 1367 loss 0.43459734320640564\n",
      "Iteration 1368 loss 0.4267536997795105\n",
      "Iteration 1369 loss 0.4695276618003845\n",
      "Iteration 1370 loss 0.4148275554180145\n",
      "Iteration 1371 loss 0.42934364080429077\n",
      "Iteration 1372 loss 0.37302860617637634\n",
      "Iteration 1373 loss 0.4035765528678894\n",
      "Iteration 1374 loss 0.45088469982147217\n",
      "Iteration 1375 loss 0.39790549874305725\n",
      "Iteration 1376 loss 0.38513463735580444\n",
      "Iteration 1377 loss 0.4316730499267578\n",
      "Iteration 1378 loss 0.382076233625412\n",
      "Iteration 1379 loss 0.4381445348262787\n",
      "Iteration 1380 loss 0.45808595418930054\n",
      "Iteration 1381 loss 0.40495818853378296\n",
      "Iteration 1382 loss 0.38569730520248413\n",
      "Iteration 1383 loss 0.39562472701072693\n",
      "Iteration 1384 loss 0.39981529116630554\n",
      "Iteration 1385 loss 0.44957613945007324\n",
      "Iteration 1386 loss 0.35821130871772766\n",
      "Iteration 1387 loss 0.37709373235702515\n",
      "Iteration 1388 loss 0.3486691415309906\n",
      "Iteration 1389 loss 0.4589148461818695\n",
      "Iteration 1390 loss 0.3675576448440552\n",
      "Iteration 1391 loss 0.41848137974739075\n",
      "Iteration 1392 loss 0.4239475131034851\n",
      "Iteration 1393 loss 0.4452754259109497\n",
      "Iteration 1394 loss 0.4454517960548401\n",
      "Iteration 1395 loss 0.4505036175251007\n",
      "Iteration 1396 loss 0.4522417485713959\n",
      "Iteration 1397 loss 0.37835586071014404\n",
      "Iteration 1398 loss 0.425899863243103\n",
      "Iteration 1399 loss 0.4205358922481537\n",
      "Iteration 1400 loss 0.39594966173171997\n",
      "Iteration 1401 loss 0.4796566069126129\n",
      "Iteration 1402 loss 0.4178023338317871\n",
      "Iteration 1403 loss 0.4288763999938965\n",
      "Iteration 1404 loss 0.40706706047058105\n",
      "Iteration 1405 loss 0.368490993976593\n",
      "Iteration 1406 loss 0.37278813123703003\n",
      "Iteration 1407 loss 0.36543506383895874\n",
      "Iteration 1408 loss 0.40735459327697754\n",
      "Iteration 1409 loss 0.4292925000190735\n",
      "Iteration 1410 loss 0.3954699635505676\n",
      "Iteration 1411 loss 0.4086906909942627\n",
      "Iteration 1412 loss 0.38988131284713745\n",
      "Iteration 1413 loss 0.4358288049697876\n",
      "Iteration 1414 loss 0.4430496394634247\n",
      "Iteration 1415 loss 0.48492708802223206\n",
      "Iteration 1416 loss 0.3815780580043793\n",
      "Iteration 1417 loss 0.4307158291339874\n",
      "Iteration 1418 loss 0.337495356798172\n",
      "Iteration 1419 loss 0.5112032294273376\n",
      "Iteration 1420 loss 0.4088743329048157\n",
      "Iteration 1421 loss 0.38007164001464844\n",
      "Iteration 1422 loss 0.4029909372329712\n",
      "Iteration 1423 loss 0.49219584465026855\n",
      "Iteration 1424 loss 0.4064391851425171\n",
      "Iteration 1425 loss 0.4211779534816742\n",
      "Iteration 1426 loss 0.40988343954086304\n",
      "Iteration 1427 loss 0.38584208488464355\n",
      "Iteration 1428 loss 0.3752424120903015\n",
      "Iteration 1429 loss 0.40461021661758423\n",
      "Iteration 1430 loss 0.4343007504940033\n",
      "Iteration 1431 loss 0.40599486231803894\n",
      "Iteration 1432 loss 0.3716775178909302\n",
      "Iteration 1433 loss 0.41928255558013916\n",
      "Iteration 1434 loss 0.3854653835296631\n",
      "Iteration 1435 loss 0.38715606927871704\n",
      "Iteration 1436 loss 0.34506118297576904\n",
      "Iteration 1437 loss 0.4351770579814911\n",
      "Iteration 1438 loss 0.44050607085227966\n",
      "Iteration 1439 loss 0.4399658441543579\n",
      "Iteration 1440 loss 0.37033170461654663\n",
      "Iteration 1441 loss 0.39735502004623413\n",
      "Iteration 1442 loss 0.33555060625076294\n",
      "Iteration 1443 loss 0.3879412114620209\n",
      "Iteration 1444 loss 0.4081690311431885\n",
      "Iteration 1445 loss 0.4233468472957611\n",
      "Iteration 1446 loss 0.42969799041748047\n",
      "Iteration 1447 loss 0.41846710443496704\n",
      "Iteration 1448 loss 0.375794917345047\n",
      "Iteration 1449 loss 0.34891068935394287\n",
      "Iteration 1450 loss 0.4504252076148987\n",
      "Iteration 1451 loss 0.3761935234069824\n",
      "Iteration 1452 loss 0.3943646252155304\n",
      "Iteration 1453 loss 0.3780101537704468\n",
      "Iteration 1454 loss 0.3922082781791687\n",
      "Iteration 1455 loss 0.3641055226325989\n",
      "Iteration 1456 loss 0.35753071308135986\n",
      "Iteration 1457 loss 0.43062207102775574\n",
      "Iteration 1458 loss 0.4096525311470032\n",
      "Iteration 1459 loss 0.410963237285614\n",
      "Iteration 1460 loss 0.4330844283103943\n",
      "Iteration 1461 loss 0.3890196979045868\n",
      "Iteration 1462 loss 0.4219329059123993\n",
      "Iteration 1463 loss 0.43474429845809937\n",
      "Iteration 1464 loss 0.41996270418167114\n",
      "Iteration 1465 loss 0.4693423807621002\n",
      "Iteration 1466 loss 0.40302687883377075\n",
      "Iteration 1467 loss 0.4288700520992279\n",
      "Iteration 1468 loss 0.45240744948387146\n",
      "Iteration 1469 loss 0.45667600631713867\n",
      "Iteration 1470 loss 0.447446346282959\n",
      "Iteration 1471 loss 0.3760700523853302\n",
      "Iteration 1472 loss 0.4100903272628784\n",
      "Iteration 1473 loss 0.3910937011241913\n",
      "Iteration 1474 loss 0.440569669008255\n",
      "Iteration 1475 loss 0.4325368404388428\n",
      "Iteration 1476 loss 0.4384831488132477\n",
      "Iteration 1477 loss 0.3917371928691864\n",
      "Iteration 1478 loss 0.4617849588394165\n",
      "Iteration 1479 loss 0.42747074365615845\n",
      "Iteration 1480 loss 0.4147874116897583\n",
      "Iteration 1481 loss 0.411865234375\n",
      "Iteration 1482 loss 0.45275846123695374\n",
      "Iteration 1483 loss 0.39421796798706055\n",
      "Iteration 1484 loss 0.4302016794681549\n",
      "Iteration 1485 loss 0.3900126814842224\n",
      "Iteration 1486 loss 0.4175521731376648\n",
      "Iteration 1487 loss 0.4370992183685303\n",
      "Iteration 1488 loss 0.38091936707496643\n",
      "Iteration 1489 loss 0.4995526373386383\n",
      "Iteration 1490 loss 0.4074130058288574\n",
      "Iteration 1491 loss 0.4291805624961853\n",
      "Iteration 1492 loss 0.444426029920578\n",
      "Iteration 1493 loss 0.3746378719806671\n",
      "Iteration 1494 loss 0.3884776830673218\n",
      "Iteration 1495 loss 0.4348761737346649\n",
      "Iteration 1496 loss 0.45468616485595703\n",
      "Iteration 1497 loss 0.39589422941207886\n",
      "Iteration 1498 loss 0.3892621099948883\n",
      "Iteration 1499 loss 0.39404499530792236\n",
      "Iteration 1500 loss 0.39269325137138367\n",
      "Iteration 1501 loss 0.4008680582046509\n",
      "Iteration 1502 loss 0.4144195318222046\n",
      "Iteration 1503 loss 0.3365240693092346\n",
      "Iteration 1504 loss 0.39640918374061584\n",
      "Iteration 1505 loss 0.4053274691104889\n",
      "Iteration 1506 loss 0.4348592460155487\n",
      "Iteration 1507 loss 0.4442126750946045\n",
      "Iteration 1508 loss 0.42496272921562195\n",
      "Iteration 1509 loss 0.43724706768989563\n",
      "Iteration 1510 loss 0.46178245544433594\n",
      "Iteration 1511 loss 0.4033850431442261\n",
      "Iteration 1512 loss 0.44959431886672974\n",
      "Iteration 1513 loss 0.3932338356971741\n",
      "Iteration 1514 loss 0.3975295126438141\n",
      "Iteration 1515 loss 0.4103687107563019\n",
      "Iteration 1516 loss 0.45360076427459717\n",
      "Iteration 1517 loss 0.35185977816581726\n",
      "Iteration 1518 loss 0.3731851577758789\n",
      "Iteration 1519 loss 0.36380136013031006\n",
      "Iteration 1520 loss 0.375301331281662\n",
      "Iteration 1521 loss 0.2831050753593445\n",
      "Iteration 1522 loss 0.42187222838401794\n",
      "Iteration 1523 loss 0.40266862511634827\n",
      "Iteration 1524 loss 0.4051932990550995\n",
      "Iteration 1525 loss 0.39476290345191956\n",
      "Iteration 1526 loss 0.4458758234977722\n",
      "Iteration 1527 loss 0.38986751437187195\n",
      "Iteration 1528 loss 0.35817158222198486\n",
      "Iteration 1529 loss 0.40092140436172485\n",
      "Iteration 1530 loss 0.42820167541503906\n",
      "Iteration 1531 loss 0.35917314887046814\n",
      "Iteration 1532 loss 0.44203981757164\n",
      "Iteration 1533 loss 0.401644229888916\n",
      "Iteration 1534 loss 0.37914273142814636\n",
      "Iteration 1535 loss 0.37853655219078064\n",
      "Iteration 1536 loss 0.3567376136779785\n",
      "Iteration 1537 loss 0.43730849027633667\n",
      "Iteration 1538 loss 0.3604273796081543\n",
      "Iteration 1539 loss 0.3768768310546875\n",
      "Iteration 1540 loss 0.3716691732406616\n",
      "Iteration 1541 loss 0.38739174604415894\n",
      "Iteration 1542 loss 0.3940771222114563\n",
      "Iteration 1543 loss 0.3419877588748932\n",
      "Iteration 1544 loss 0.4027743339538574\n",
      "Iteration 1545 loss 0.4334431290626526\n",
      "Iteration 1546 loss 0.4048053026199341\n",
      "Iteration 1547 loss 0.3972282111644745\n",
      "Iteration 1548 loss 0.4139808118343353\n",
      "Iteration 1549 loss 0.3975125849246979\n",
      "Iteration 1550 loss 0.4029991030693054\n",
      "Iteration 1551 loss 0.4289032518863678\n",
      "Iteration 1552 loss 0.43433380126953125\n",
      "Iteration 1553 loss 0.38625386357307434\n",
      "Iteration 1554 loss 0.41502442955970764\n",
      "Iteration 1555 loss 0.38316428661346436\n",
      "Iteration 1556 loss 0.3521561622619629\n",
      "Iteration 1557 loss 0.4900912940502167\n",
      "Iteration 1558 loss 0.39693278074264526\n",
      "Iteration 1559 loss 0.4258952736854553\n",
      "Iteration 1560 loss 0.39142662286758423\n",
      "Iteration 1561 loss 0.424902081489563\n",
      "Iteration 1562 loss 0.36910104751586914\n",
      "Iteration 1563 loss 0.3771161139011383\n",
      "Iteration 1564 loss 0.400655597448349\n",
      "Iteration 1565 loss 0.3713752329349518\n",
      "Iteration 1566 loss 0.3937307298183441\n",
      "Iteration 1567 loss 0.3279898464679718\n",
      "Iteration 1568 loss 0.3911310136318207\n",
      "Iteration 1569 loss 0.4717000722885132\n",
      "Iteration 1570 loss 0.32807594537734985\n",
      "Iteration 1571 loss 0.397775262594223\n",
      "Iteration 1572 loss 0.3693227767944336\n",
      "Iteration 1573 loss 0.39064669609069824\n",
      "Iteration 1574 loss 0.37371063232421875\n",
      "Iteration 1575 loss 0.3764766454696655\n",
      "Iteration 1576 loss 0.3805960714817047\n",
      "Iteration 1577 loss 0.40215960144996643\n",
      "Iteration 1578 loss 0.3928571045398712\n",
      "Iteration 1579 loss 0.4389383792877197\n",
      "Iteration 1580 loss 0.40592414140701294\n",
      "Iteration 1581 loss 0.38600313663482666\n",
      "Iteration 1582 loss 0.3439972400665283\n",
      "Iteration 1583 loss 0.4306129813194275\n",
      "Iteration 1584 loss 0.3678077459335327\n",
      "Iteration 1585 loss 0.36907440423965454\n",
      "Iteration 1586 loss 0.42848461866378784\n",
      "Iteration 1587 loss 0.3463730216026306\n",
      "Iteration 1588 loss 0.36108553409576416\n",
      "Iteration 1589 loss 0.3637092113494873\n",
      "Iteration 1590 loss 0.36984819173812866\n",
      "Iteration 1591 loss 0.4226076900959015\n",
      "Iteration 1592 loss 0.36273646354675293\n",
      "Iteration 1593 loss 0.44002872705459595\n",
      "Iteration 1594 loss 0.34392690658569336\n",
      "Iteration 1595 loss 0.33457624912261963\n",
      "Iteration 1596 loss 0.37341246008872986\n",
      "Iteration 1597 loss 0.3732587397098541\n",
      "Iteration 1598 loss 0.39369866251945496\n",
      "Iteration 1599 loss 0.4192735254764557\n",
      "Iteration 1600 loss 0.383836567401886\n",
      "Iteration 1601 loss 0.36949867010116577\n",
      "Iteration 1602 loss 0.3178524672985077\n",
      "Iteration 1603 loss 0.3617016077041626\n",
      "Iteration 1604 loss 0.32089364528656006\n",
      "Iteration 1605 loss 0.33646851778030396\n",
      "Iteration 1606 loss 0.37763142585754395\n",
      "Iteration 1607 loss 0.4175148010253906\n",
      "Iteration 1608 loss 0.37999382615089417\n",
      "Iteration 1609 loss 0.363033652305603\n",
      "Iteration 1610 loss 0.34669116139411926\n",
      "Iteration 1611 loss 0.4183352589607239\n",
      "Iteration 1612 loss 0.3685774505138397\n",
      "Iteration 1613 loss 0.40865811705589294\n",
      "Iteration 1614 loss 0.355716735124588\n",
      "Iteration 1615 loss 0.38348114490509033\n",
      "Iteration 1616 loss 0.40085962414741516\n",
      "Iteration 1617 loss 0.31490570306777954\n",
      "Iteration 1618 loss 0.36983421444892883\n",
      "Iteration 1619 loss 0.42444542050361633\n",
      "Iteration 1620 loss 0.3722497224807739\n",
      "Iteration 1621 loss 0.35660114884376526\n",
      "Iteration 1622 loss 0.3641080856323242\n",
      "Iteration 1623 loss 0.3972257077693939\n",
      "Iteration 1624 loss 0.41719362139701843\n",
      "Iteration 1625 loss 0.4218370318412781\n",
      "Iteration 1626 loss 0.4328143894672394\n",
      "Iteration 1627 loss 0.38450658321380615\n",
      "Iteration 1628 loss 0.3821966350078583\n",
      "Iteration 1629 loss 0.3893778622150421\n",
      "Iteration 1630 loss 0.3730382025241852\n",
      "Iteration 1631 loss 0.3800865709781647\n",
      "Iteration 1632 loss 0.42568954825401306\n",
      "Iteration 1633 loss 0.36491745710372925\n",
      "Iteration 1634 loss 0.3992428779602051\n",
      "Iteration 1635 loss 0.42686617374420166\n",
      "Iteration 1636 loss 0.41858619451522827\n",
      "Iteration 1637 loss 0.38193565607070923\n",
      "Iteration 1638 loss 0.3497244715690613\n",
      "Iteration 1639 loss 0.4208686947822571\n",
      "Iteration 1640 loss 0.4298368990421295\n",
      "Iteration 1641 loss 0.39594751596450806\n",
      "Iteration 1642 loss 0.4141339659690857\n",
      "Iteration 1643 loss 0.393527090549469\n",
      "Iteration 1644 loss 0.4070581793785095\n",
      "Iteration 1645 loss 0.3910366892814636\n",
      "Iteration 1646 loss 0.3506930470466614\n",
      "Iteration 1647 loss 0.3653174936771393\n",
      "Iteration 1648 loss 0.41619861125946045\n",
      "Iteration 1649 loss 0.35230469703674316\n",
      "Iteration 1650 loss 0.40571990609169006\n",
      "Iteration 1651 loss 0.36831122636795044\n",
      "Iteration 1652 loss 0.42885929346084595\n",
      "Iteration 1653 loss 0.4269639849662781\n",
      "Iteration 1654 loss 0.45522937178611755\n",
      "Iteration 1655 loss 0.4186071753501892\n",
      "Iteration 1656 loss 0.36960142850875854\n",
      "Iteration 1657 loss 0.39826977252960205\n",
      "Iteration 1658 loss 0.3956603705883026\n",
      "Iteration 1659 loss 0.3792933225631714\n",
      "Iteration 1660 loss 0.38918736577033997\n",
      "Iteration 1661 loss 0.383301317691803\n",
      "Iteration 1662 loss 0.3941343128681183\n",
      "Iteration 1663 loss 0.40966254472732544\n",
      "Iteration 1664 loss 0.4261162281036377\n",
      "Iteration 1665 loss 0.38293468952178955\n",
      "Iteration 1666 loss 0.4119718372821808\n",
      "Iteration 1667 loss 0.3625831604003906\n",
      "Iteration 1668 loss 0.37397274374961853\n",
      "Iteration 1669 loss 0.35515058040618896\n",
      "Iteration 1670 loss 0.3914220333099365\n",
      "Iteration 1671 loss 0.3492472767829895\n",
      "Iteration 1672 loss 0.37869328260421753\n",
      "Iteration 1673 loss 0.39063864946365356\n",
      "Iteration 1674 loss 0.3623576760292053\n",
      "Iteration 1675 loss 0.41013795137405396\n",
      "Iteration 1676 loss 0.3858186900615692\n",
      "Iteration 1677 loss 0.3856302499771118\n",
      "Iteration 1678 loss 0.41071978211402893\n",
      "Iteration 1679 loss 0.3874043822288513\n",
      "Iteration 1680 loss 0.39915114641189575\n",
      "Iteration 1681 loss 0.3907333016395569\n",
      "Iteration 1682 loss 0.37420645356178284\n",
      "Iteration 1683 loss 0.38223469257354736\n",
      "Iteration 1684 loss 0.3611915707588196\n",
      "Iteration 1685 loss 0.3506495952606201\n",
      "Iteration 1686 loss 0.3887389600276947\n",
      "Iteration 1687 loss 0.37670832872390747\n",
      "Iteration 1688 loss 0.3989756405353546\n",
      "Iteration 1689 loss 0.41499578952789307\n",
      "Iteration 1690 loss 0.3659372329711914\n",
      "Iteration 1691 loss 0.3880007863044739\n",
      "Iteration 1692 loss 0.34317612648010254\n",
      "Iteration 1693 loss 0.32633689045906067\n",
      "Iteration 1694 loss 0.3787859380245209\n",
      "Iteration 1695 loss 0.37725910544395447\n",
      "Iteration 1696 loss 0.4408174157142639\n",
      "Iteration 1697 loss 0.37296533584594727\n",
      "Iteration 1698 loss 0.34772923588752747\n",
      "Iteration 1699 loss 0.39753085374832153\n",
      "Iteration 1700 loss 0.376575231552124\n",
      "Iteration 1701 loss 0.3113464117050171\n",
      "Iteration 1702 loss 0.3797879219055176\n",
      "Iteration 1703 loss 0.4545041024684906\n",
      "Iteration 1704 loss 0.3669876456260681\n",
      "Iteration 1705 loss 0.361910343170166\n",
      "Iteration 1706 loss 0.41018542647361755\n",
      "Iteration 1707 loss 0.39690911769866943\n",
      "Iteration 1708 loss 0.3426845073699951\n",
      "Iteration 1709 loss 0.3977798819541931\n",
      "Iteration 1710 loss 0.4275991916656494\n",
      "Iteration 1711 loss 0.43700841069221497\n",
      "Iteration 1712 loss 0.38388901948928833\n",
      "Iteration 1713 loss 0.3797018826007843\n",
      "Iteration 1714 loss 0.40235477685928345\n",
      "Iteration 1715 loss 0.3813357353210449\n",
      "Iteration 1716 loss 0.44896841049194336\n",
      "Iteration 1717 loss 0.4175060987472534\n",
      "Iteration 1718 loss 0.3377060890197754\n",
      "Iteration 1719 loss 0.3619527816772461\n",
      "Iteration 1720 loss 0.3944154381752014\n",
      "Iteration 1721 loss 0.41723403334617615\n",
      "Iteration 1722 loss 0.2935844659805298\n",
      "Iteration 1723 loss 0.36535507440567017\n",
      "Iteration 1724 loss 0.3493784964084625\n",
      "Iteration 1725 loss 0.4088889956474304\n",
      "Iteration 1726 loss 0.3757098913192749\n",
      "Iteration 1727 loss 0.38556721806526184\n",
      "Iteration 1728 loss 0.3946017026901245\n",
      "Iteration 1729 loss 0.38686636090278625\n",
      "Iteration 1730 loss 0.3697223365306854\n",
      "Iteration 1731 loss 0.35999003052711487\n",
      "Iteration 1732 loss 0.3820051848888397\n",
      "Iteration 1733 loss 0.36343610286712646\n",
      "Iteration 1734 loss 0.42879804968833923\n",
      "Iteration 1735 loss 0.412908673286438\n",
      "Iteration 1736 loss 0.37446460127830505\n",
      "Iteration 1737 loss 0.3823612630367279\n",
      "Iteration 1738 loss 0.38481685519218445\n",
      "Iteration 1739 loss 0.3441750407218933\n",
      "Iteration 1740 loss 0.37327155470848083\n",
      "Iteration 1741 loss 0.39923375844955444\n",
      "Iteration 1742 loss 0.3609938621520996\n",
      "Iteration 1743 loss 0.3827895522117615\n",
      "Iteration 1744 loss 0.3536701202392578\n",
      "Iteration 1745 loss 0.3644046187400818\n",
      "Iteration 1746 loss 0.32589486241340637\n",
      "Iteration 1747 loss 0.38619959354400635\n",
      "Iteration 1748 loss 0.41954007744789124\n",
      "Iteration 1749 loss 0.372107595205307\n",
      "Iteration 1750 loss 0.42222774028778076\n",
      "Iteration 1751 loss 0.37357309460639954\n",
      "Iteration 1752 loss 0.3838406205177307\n",
      "Iteration 1753 loss 0.39208537340164185\n",
      "Iteration 1754 loss 0.4330567717552185\n",
      "Iteration 1755 loss 0.3711017370223999\n",
      "Iteration 1756 loss 0.3370964527130127\n",
      "Iteration 1757 loss 0.36496540904045105\n",
      "Iteration 1758 loss 0.3879335820674896\n",
      "Iteration 1759 loss 0.38006699085235596\n",
      "Iteration 1760 loss 0.3757994771003723\n",
      "Iteration 1761 loss 0.3481424152851105\n",
      "Iteration 1762 loss 0.3806203603744507\n",
      "Iteration 1763 loss 0.42528897523880005\n",
      "Iteration 1764 loss 0.3780324459075928\n",
      "Iteration 1765 loss 0.3744482398033142\n",
      "Iteration 1766 loss 0.388901948928833\n",
      "Iteration 1767 loss 0.4023911654949188\n",
      "Iteration 1768 loss 0.3756769597530365\n",
      "Iteration 1769 loss 0.3734384775161743\n",
      "Iteration 1770 loss 0.3500880002975464\n",
      "Iteration 1771 loss 0.4058034420013428\n",
      "Iteration 1772 loss 0.33492472767829895\n",
      "Iteration 1773 loss 0.39631202816963196\n",
      "Iteration 1774 loss 0.39511045813560486\n",
      "Iteration 1775 loss 0.4172745645046234\n",
      "Iteration 1776 loss 0.3365230858325958\n",
      "Iteration 1777 loss 0.3695935606956482\n",
      "Iteration 1778 loss 0.3981364667415619\n",
      "Iteration 1779 loss 0.3758503794670105\n",
      "Iteration 1780 loss 0.37505239248275757\n",
      "Iteration 1781 loss 0.3115050494670868\n",
      "Iteration 1782 loss 0.34657570719718933\n",
      "Iteration 1783 loss 0.37058907747268677\n",
      "Iteration 1784 loss 0.3662792146205902\n",
      "Iteration 1785 loss 0.39629170298576355\n",
      "Iteration 1786 loss 0.3439488410949707\n",
      "Iteration 1787 loss 0.33439144492149353\n",
      "Iteration 1788 loss 0.3690745234489441\n",
      "Iteration 1789 loss 0.3938787579536438\n",
      "Iteration 1790 loss 0.3877808451652527\n",
      "Iteration 1791 loss 0.35411855578422546\n",
      "Iteration 1792 loss 0.4009172320365906\n",
      "Iteration 1793 loss 0.40671780705451965\n",
      "Iteration 1794 loss 0.31544849276542664\n",
      "Iteration 1795 loss 0.3272859752178192\n",
      "Iteration 1796 loss 0.40218496322631836\n",
      "Iteration 1797 loss 0.399305522441864\n",
      "Iteration 1798 loss 0.35177090764045715\n",
      "Iteration 1799 loss 0.4053555130958557\n",
      "Iteration 1800 loss 0.3619624972343445\n",
      "Iteration 1801 loss 0.3813434839248657\n",
      "Iteration 1802 loss 0.3492394685745239\n",
      "Iteration 1803 loss 0.38192129135131836\n",
      "Iteration 1804 loss 0.35834261775016785\n",
      "Iteration 1805 loss 0.35721713304519653\n",
      "Iteration 1806 loss 0.35876891016960144\n",
      "Iteration 1807 loss 0.3310889005661011\n",
      "Iteration 1808 loss 0.3715836703777313\n",
      "Iteration 1809 loss 0.35353976488113403\n",
      "Iteration 1810 loss 0.39136844873428345\n",
      "Iteration 1811 loss 0.3392167091369629\n",
      "Iteration 1812 loss 0.4185170531272888\n",
      "Iteration 1813 loss 0.38155674934387207\n",
      "Iteration 1814 loss 0.33851921558380127\n",
      "Iteration 1815 loss 0.3809165358543396\n",
      "Iteration 1816 loss 0.37219181656837463\n",
      "Iteration 1817 loss 0.350620836019516\n",
      "Iteration 1818 loss 0.348944753408432\n",
      "Iteration 1819 loss 0.3904465138912201\n",
      "Iteration 1820 loss 0.3774551451206207\n",
      "Iteration 1821 loss 0.33524227142333984\n",
      "Iteration 1822 loss 0.3899378478527069\n",
      "Iteration 1823 loss 0.3628411591053009\n",
      "Iteration 1824 loss 0.3757593333721161\n",
      "Iteration 1825 loss 0.42858371138572693\n",
      "Iteration 1826 loss 0.34116336703300476\n",
      "Iteration 1827 loss 0.3801947236061096\n",
      "Iteration 1828 loss 0.36865970492362976\n",
      "Iteration 1829 loss 0.35686951875686646\n",
      "Iteration 1830 loss 0.35979801416397095\n",
      "Iteration 1831 loss 0.38973352313041687\n",
      "Iteration 1832 loss 0.31870391964912415\n",
      "Iteration 1833 loss 0.33479636907577515\n",
      "Iteration 1834 loss 0.4124075174331665\n",
      "Iteration 1835 loss 0.35265475511550903\n",
      "Iteration 1836 loss 0.40111392736434937\n",
      "Iteration 1837 loss 0.35605788230895996\n",
      "Iteration 1838 loss 0.3641352355480194\n",
      "Iteration 1839 loss 0.33535414934158325\n",
      "Iteration 1840 loss 0.39860886335372925\n",
      "Iteration 1841 loss 0.3880007266998291\n",
      "Iteration 1842 loss 0.3780537545681\n",
      "Iteration 1843 loss 0.3373832106590271\n",
      "Iteration 1844 loss 0.36466652154922485\n",
      "Iteration 1845 loss 0.4151481091976166\n",
      "Iteration 1846 loss 0.3789222836494446\n",
      "Iteration 1847 loss 0.4268324375152588\n",
      "Iteration 1848 loss 0.40003734827041626\n",
      "Iteration 1849 loss 0.3727108836174011\n",
      "Iteration 1850 loss 0.42143189907073975\n",
      "Iteration 1851 loss 0.4205446243286133\n",
      "Iteration 1852 loss 0.37467846274375916\n",
      "Iteration 1853 loss 0.3211180865764618\n",
      "Iteration 1854 loss 0.3773276209831238\n",
      "Iteration 1855 loss 0.36464837193489075\n",
      "Iteration 1856 loss 0.3447464108467102\n",
      "Iteration 1857 loss 0.3190414011478424\n",
      "Iteration 1858 loss 0.3200947344303131\n",
      "Iteration 1859 loss 0.3640452027320862\n",
      "Iteration 1860 loss 0.36616194248199463\n",
      "Iteration 1861 loss 0.3794308006763458\n",
      "Iteration 1862 loss 0.3659879267215729\n",
      "Iteration 1863 loss 0.30364537239074707\n",
      "Iteration 1864 loss 0.37717366218566895\n",
      "Iteration 1865 loss 0.340749591588974\n",
      "Iteration 1866 loss 0.3719451129436493\n",
      "Iteration 1867 loss 0.331559419631958\n",
      "Iteration 1868 loss 0.38690003752708435\n",
      "Iteration 1869 loss 0.3962695598602295\n",
      "Iteration 1870 loss 0.40172716975212097\n",
      "Iteration 1871 loss 0.41154980659484863\n",
      "Iteration 1872 loss 0.447324275970459\n",
      "Iteration 1873 loss 0.3971659541130066\n",
      "Iteration 1874 loss 0.39232560992240906\n",
      "Iteration 1875 loss 0.32197216153144836\n",
      "Iteration 1876 loss 0.3908352255821228\n",
      "Iteration 1877 loss 0.40055495500564575\n",
      "Iteration 1878 loss 0.3944568336009979\n",
      "Iteration 1879 loss 0.3503614068031311\n",
      "Iteration 1880 loss 0.3610891103744507\n",
      "Iteration 1881 loss 0.3588441014289856\n",
      "Iteration 1882 loss 0.35169532895088196\n",
      "Iteration 1883 loss 0.3299676179885864\n",
      "Iteration 1884 loss 0.3861618936061859\n",
      "Iteration 1885 loss 0.355888694524765\n",
      "Iteration 1886 loss 0.3625025451183319\n",
      "Iteration 1887 loss 0.3591984510421753\n",
      "Iteration 1888 loss 0.33204081654548645\n",
      "Iteration 1889 loss 0.3925327956676483\n",
      "Iteration 1890 loss 0.4230270981788635\n",
      "Iteration 1891 loss 0.35913923382759094\n",
      "Iteration 1892 loss 0.3361251950263977\n",
      "Iteration 1893 loss 0.3376918435096741\n",
      "Iteration 1894 loss 0.339549720287323\n",
      "Iteration 1895 loss 0.45318007469177246\n",
      "Iteration 1896 loss 0.4087122082710266\n",
      "Iteration 1897 loss 0.37785792350769043\n",
      "Iteration 1898 loss 0.33870577812194824\n",
      "Iteration 1899 loss 0.41962501406669617\n",
      "Iteration 1900 loss 0.3685725927352905\n",
      "Iteration 1901 loss 0.3754339814186096\n",
      "Iteration 1902 loss 0.38470837473869324\n",
      "Iteration 1903 loss 0.3292171359062195\n",
      "Iteration 1904 loss 0.4093046486377716\n",
      "Iteration 1905 loss 0.3596259355545044\n",
      "Iteration 1906 loss 0.37071463465690613\n",
      "Iteration 1907 loss 0.37636229395866394\n",
      "Iteration 1908 loss 0.35985255241394043\n",
      "Iteration 1909 loss 0.36887550354003906\n",
      "Iteration 1910 loss 0.397599995136261\n",
      "Iteration 1911 loss 0.38466793298721313\n",
      "Iteration 1912 loss 0.3429768681526184\n",
      "Iteration 1913 loss 0.3794501721858978\n",
      "Iteration 1914 loss 0.3327518403530121\n",
      "Iteration 1915 loss 0.3514845669269562\n",
      "Iteration 1916 loss 0.36117303371429443\n",
      "Iteration 1917 loss 0.33153751492500305\n",
      "Iteration 1918 loss 0.3935736119747162\n",
      "Iteration 1919 loss 0.3450482487678528\n",
      "Iteration 1920 loss 0.3851528763771057\n",
      "Iteration 1921 loss 0.4003136157989502\n",
      "Iteration 1922 loss 0.40244996547698975\n",
      "Iteration 1923 loss 0.4108680784702301\n",
      "Iteration 1924 loss 0.3847583830356598\n",
      "Iteration 1925 loss 0.348559707403183\n",
      "Iteration 1926 loss 0.35661399364471436\n",
      "Iteration 1927 loss 0.3740670084953308\n",
      "Iteration 1928 loss 0.3734448254108429\n",
      "Iteration 1929 loss 0.3869304656982422\n",
      "Iteration 1930 loss 0.4018324017524719\n",
      "Iteration 1931 loss 0.4121778607368469\n",
      "Iteration 1932 loss 0.35146552324295044\n",
      "Iteration 1933 loss 0.36710456013679504\n",
      "Iteration 1934 loss 0.3608741760253906\n",
      "Iteration 1935 loss 0.3102041482925415\n",
      "Iteration 1936 loss 0.3749307692050934\n",
      "Iteration 1937 loss 0.36803799867630005\n",
      "Iteration 1938 loss 0.3833296000957489\n",
      "Iteration 1939 loss 0.3723718225955963\n",
      "Iteration 1940 loss 0.36239731311798096\n",
      "Iteration 1941 loss 0.3518281579017639\n",
      "Iteration 1942 loss 0.3204953372478485\n",
      "Iteration 1943 loss 0.35887980461120605\n",
      "Iteration 1944 loss 0.350500226020813\n",
      "Iteration 1945 loss 0.38969728350639343\n",
      "Iteration 1946 loss 0.38592010736465454\n",
      "Iteration 1947 loss 0.3479928970336914\n",
      "Iteration 1948 loss 0.3800428509712219\n",
      "Iteration 1949 loss 0.360081285238266\n",
      "Iteration 1950 loss 0.3385366201400757\n",
      "Iteration 1951 loss 0.3281264305114746\n",
      "Iteration 1952 loss 0.3699539005756378\n",
      "Iteration 1953 loss 0.4131868779659271\n",
      "Iteration 1954 loss 0.3771790862083435\n",
      "Iteration 1955 loss 0.37168705463409424\n",
      "Iteration 1956 loss 0.3450593650341034\n",
      "Iteration 1957 loss 0.3415924608707428\n",
      "Iteration 1958 loss 0.37842974066734314\n",
      "Iteration 1959 loss 0.3586091101169586\n",
      "Iteration 1960 loss 0.34693270921707153\n",
      "Iteration 1961 loss 0.3657378852367401\n",
      "Iteration 1962 loss 0.39727020263671875\n",
      "Iteration 1963 loss 0.36784085631370544\n",
      "Iteration 1964 loss 0.3967759609222412\n",
      "Iteration 1965 loss 0.36680999398231506\n",
      "Iteration 1966 loss 0.34314849972724915\n",
      "Iteration 1967 loss 0.33743682503700256\n",
      "Iteration 1968 loss 0.3653268814086914\n",
      "Iteration 1969 loss 0.3718527853488922\n",
      "Iteration 1970 loss 0.36917755007743835\n",
      "Iteration 1971 loss 0.3723301291465759\n",
      "Iteration 1972 loss 0.34385594725608826\n",
      "Iteration 1973 loss 0.3315116763114929\n",
      "Iteration 1974 loss 0.3705188035964966\n",
      "Iteration 1975 loss 0.32030072808265686\n",
      "Iteration 1976 loss 0.32130783796310425\n",
      "Iteration 1977 loss 0.33791565895080566\n",
      "Iteration 1978 loss 0.34916359186172485\n",
      "Iteration 1979 loss 0.36949214339256287\n",
      "Iteration 1980 loss 0.3473047614097595\n",
      "Iteration 1981 loss 0.37287330627441406\n",
      "Iteration 1982 loss 0.31187373399734497\n",
      "Iteration 1983 loss 0.3856441378593445\n",
      "Iteration 1984 loss 0.3164796233177185\n",
      "Iteration 1985 loss 0.30821359157562256\n",
      "Iteration 1986 loss 0.3617650270462036\n",
      "Iteration 1987 loss 0.33634164929389954\n",
      "Iteration 1988 loss 0.36652112007141113\n",
      "Iteration 1989 loss 0.34523600339889526\n",
      "Iteration 1990 loss 0.28794077038764954\n",
      "Iteration 1991 loss 0.3334074020385742\n",
      "Iteration 1992 loss 0.35238155722618103\n",
      "Iteration 1993 loss 0.38986077904701233\n",
      "Iteration 1994 loss 0.38480767607688904\n",
      "Iteration 1995 loss 0.3512239456176758\n",
      "Iteration 1996 loss 0.2957119941711426\n",
      "Iteration 1997 loss 0.3330899775028229\n",
      "Iteration 1998 loss 0.3456721603870392\n",
      "Iteration 1999 loss 0.3624476194381714\n",
      "Iteration 2000 loss 0.3359937369823456\n",
      "Iteration 2001 loss 0.3489719033241272\n",
      "Iteration 2002 loss 0.36265885829925537\n",
      "Iteration 2003 loss 0.3192025423049927\n",
      "Iteration 2004 loss 0.34496572613716125\n",
      "Iteration 2005 loss 0.33133748173713684\n",
      "Iteration 2006 loss 0.3675228953361511\n",
      "Iteration 2007 loss 0.34549686312675476\n",
      "Iteration 2008 loss 0.33124983310699463\n",
      "Iteration 2009 loss 0.3274855613708496\n",
      "Iteration 2010 loss 0.3371187746524811\n",
      "Iteration 2011 loss 0.3408506214618683\n",
      "Iteration 2012 loss 0.3836396336555481\n",
      "Iteration 2013 loss 0.36544734239578247\n",
      "Iteration 2014 loss 0.34753352403640747\n",
      "Iteration 2015 loss 0.38546043634414673\n",
      "Iteration 2016 loss 0.34991925954818726\n",
      "Iteration 2017 loss 0.38621288537979126\n",
      "Iteration 2018 loss 0.3599775731563568\n",
      "Iteration 2019 loss 0.3748878836631775\n",
      "Iteration 2020 loss 0.3289763629436493\n",
      "Iteration 2021 loss 0.35477742552757263\n",
      "Iteration 2022 loss 0.33422359824180603\n",
      "Iteration 2023 loss 0.37061449885368347\n",
      "Iteration 2024 loss 0.3531513810157776\n",
      "Iteration 2025 loss 0.3913384675979614\n",
      "Iteration 2026 loss 0.32299086451530457\n",
      "Iteration 2027 loss 0.3448061943054199\n",
      "Iteration 2028 loss 0.35373878479003906\n",
      "Iteration 2029 loss 0.3569389283657074\n",
      "Iteration 2030 loss 0.3656887412071228\n",
      "Iteration 2031 loss 0.33796390891075134\n",
      "Iteration 2032 loss 0.38149702548980713\n",
      "Iteration 2033 loss 0.34129777550697327\n",
      "Iteration 2034 loss 0.403866708278656\n",
      "Iteration 2035 loss 0.3285018801689148\n",
      "Iteration 2036 loss 0.34655413031578064\n",
      "Iteration 2037 loss 0.3085517883300781\n",
      "Iteration 2038 loss 0.36512279510498047\n",
      "Iteration 2039 loss 0.3722617030143738\n",
      "Iteration 2040 loss 0.3747198283672333\n",
      "Iteration 2041 loss 0.3858441710472107\n",
      "Iteration 2042 loss 0.3219491243362427\n",
      "Iteration 2043 loss 0.3773791193962097\n",
      "Iteration 2044 loss 0.3340022563934326\n",
      "Iteration 2045 loss 0.3566642701625824\n",
      "Iteration 2046 loss 0.3757372200489044\n",
      "Iteration 2047 loss 0.3831316828727722\n",
      "Iteration 2048 loss 0.3462710380554199\n",
      "Iteration 2049 loss 0.36592909693717957\n",
      "Iteration 2050 loss 0.3767298758029938\n",
      "Iteration 2051 loss 0.364398330450058\n",
      "Iteration 2052 loss 0.4049178957939148\n",
      "Iteration 2053 loss 0.34198251366615295\n",
      "Iteration 2054 loss 0.3146441578865051\n",
      "Iteration 2055 loss 0.3905404210090637\n",
      "Iteration 2056 loss 0.35095876455307007\n",
      "Iteration 2057 loss 0.3585314154624939\n",
      "Iteration 2058 loss 0.3407028615474701\n",
      "Iteration 2059 loss 0.321021169424057\n",
      "Iteration 2060 loss 0.3236209452152252\n",
      "Iteration 2061 loss 0.3305049538612366\n",
      "Iteration 2062 loss 0.3300992548465729\n",
      "Iteration 2063 loss 0.3761959969997406\n",
      "Iteration 2064 loss 0.3457622230052948\n",
      "Iteration 2065 loss 0.32906582951545715\n",
      "Iteration 2066 loss 0.3066277503967285\n",
      "Iteration 2067 loss 0.3706456124782562\n",
      "Iteration 2068 loss 0.35609936714172363\n",
      "Iteration 2069 loss 0.3577565550804138\n",
      "Iteration 2070 loss 0.3289693593978882\n",
      "Iteration 2071 loss 0.4100530445575714\n",
      "Iteration 2072 loss 0.3774728775024414\n",
      "Iteration 2073 loss 0.35101670026779175\n",
      "Iteration 2074 loss 0.43236008286476135\n",
      "Iteration 2075 loss 0.3595384955406189\n",
      "Iteration 2076 loss 0.38582438230514526\n",
      "Iteration 2077 loss 0.34955382347106934\n",
      "Iteration 2078 loss 0.35461702942848206\n",
      "Iteration 2079 loss 0.3782048225402832\n",
      "Iteration 2080 loss 0.33730652928352356\n",
      "Iteration 2081 loss 0.3139371871948242\n",
      "Iteration 2082 loss 0.3342136740684509\n",
      "Iteration 2083 loss 0.33638083934783936\n",
      "Iteration 2084 loss 0.35897788405418396\n",
      "Iteration 2085 loss 0.3951534628868103\n",
      "Iteration 2086 loss 0.3488437533378601\n",
      "Iteration 2087 loss 0.38130471110343933\n",
      "Iteration 2088 loss 0.3260522484779358\n",
      "Iteration 2089 loss 0.4208502471446991\n",
      "Iteration 2090 loss 0.3400205969810486\n",
      "Iteration 2091 loss 0.37341225147247314\n",
      "Iteration 2092 loss 0.37246039509773254\n",
      "Iteration 2093 loss 0.37758079171180725\n",
      "Iteration 2094 loss 0.343497633934021\n",
      "Iteration 2095 loss 0.3514941930770874\n",
      "Iteration 2096 loss 0.3388177156448364\n",
      "Iteration 2097 loss 0.30547696352005005\n",
      "Iteration 2098 loss 0.39406487345695496\n",
      "Iteration 2099 loss 0.35820314288139343\n",
      "Iteration 2100 loss 0.32902631163597107\n",
      "Iteration 2101 loss 0.38585934042930603\n",
      "Iteration 2102 loss 0.3571986258029938\n",
      "Iteration 2103 loss 0.33811259269714355\n",
      "Iteration 2104 loss 0.3941343128681183\n",
      "Iteration 2105 loss 0.38782331347465515\n",
      "Iteration 2106 loss 0.3849208950996399\n",
      "Iteration 2107 loss 0.35061588883399963\n",
      "Iteration 2108 loss 0.3997285068035126\n",
      "Iteration 2109 loss 0.408805251121521\n",
      "Iteration 2110 loss 0.3262944519519806\n",
      "Iteration 2111 loss 0.3530927002429962\n",
      "Iteration 2112 loss 0.3731497526168823\n",
      "Iteration 2113 loss 0.29808101058006287\n",
      "Iteration 2114 loss 0.3609887957572937\n",
      "Iteration 2115 loss 0.35581400990486145\n",
      "Iteration 2116 loss 0.3676201403141022\n",
      "Iteration 2117 loss 0.3770354688167572\n",
      "Iteration 2118 loss 0.35503917932510376\n",
      "Iteration 2119 loss 0.3828248679637909\n",
      "Iteration 2120 loss 0.3016214668750763\n",
      "Iteration 2121 loss 0.35786202549934387\n",
      "Iteration 2122 loss 0.35149431228637695\n",
      "Iteration 2123 loss 0.3784330487251282\n",
      "Iteration 2124 loss 0.3767563998699188\n",
      "Iteration 2125 loss 0.34168314933776855\n",
      "Iteration 2126 loss 0.3242483139038086\n",
      "Iteration 2127 loss 0.3203043043613434\n",
      "Iteration 2128 loss 0.33858221769332886\n",
      "Iteration 2129 loss 0.3188755214214325\n",
      "Iteration 2130 loss 0.32541313767433167\n",
      "Iteration 2131 loss 0.35666748881340027\n",
      "Iteration 2132 loss 0.37805071473121643\n",
      "Iteration 2133 loss 0.34620949625968933\n",
      "Iteration 2134 loss 0.3697403073310852\n",
      "Iteration 2135 loss 0.3516347408294678\n",
      "Iteration 2136 loss 0.3502724766731262\n",
      "Iteration 2137 loss 0.3250792622566223\n",
      "Iteration 2138 loss 0.37154871225357056\n",
      "Iteration 2139 loss 0.3791654109954834\n",
      "Iteration 2140 loss 0.352807879447937\n",
      "Iteration 2141 loss 0.336739718914032\n",
      "Iteration 2142 loss 0.3747670352458954\n",
      "Iteration 2143 loss 0.36555352807044983\n",
      "Iteration 2144 loss 0.3506582975387573\n",
      "Iteration 2145 loss 0.3477151393890381\n",
      "Iteration 2146 loss 0.33255720138549805\n",
      "Iteration 2147 loss 0.3678438067436218\n",
      "Iteration 2148 loss 0.3459058701992035\n",
      "Iteration 2149 loss 0.35047340393066406\n",
      "Iteration 2150 loss 0.3591465353965759\n",
      "Iteration 2151 loss 0.37258124351501465\n",
      "Iteration 2152 loss 0.3603062927722931\n",
      "Iteration 2153 loss 0.38229501247406006\n",
      "Iteration 2154 loss 0.3425971567630768\n",
      "Iteration 2155 loss 0.32889384031295776\n",
      "Iteration 2156 loss 0.3382890820503235\n",
      "Iteration 2157 loss 0.3293288052082062\n",
      "Iteration 2158 loss 0.35726213455200195\n",
      "Iteration 2159 loss 0.3335169553756714\n",
      "Iteration 2160 loss 0.3707030713558197\n",
      "Iteration 2161 loss 0.3051378130912781\n",
      "Iteration 2162 loss 0.3860395848751068\n",
      "Iteration 2163 loss 0.3488004803657532\n",
      "Iteration 2164 loss 0.3574945032596588\n",
      "Iteration 2165 loss 0.3348720073699951\n",
      "Iteration 2166 loss 0.36909183859825134\n",
      "Iteration 2167 loss 0.32680821418762207\n",
      "Iteration 2168 loss 0.3084707260131836\n",
      "Iteration 2169 loss 0.3051947355270386\n",
      "Iteration 2170 loss 0.3855469226837158\n",
      "Iteration 2171 loss 0.31486865878105164\n",
      "Iteration 2172 loss 0.32422754168510437\n",
      "Iteration 2173 loss 0.3336660861968994\n",
      "Iteration 2174 loss 0.31493106484413147\n",
      "Iteration 2175 loss 0.3594122529029846\n",
      "Iteration 2176 loss 0.3798889219760895\n",
      "Iteration 2177 loss 0.3217551112174988\n",
      "Iteration 2178 loss 0.38247615098953247\n",
      "Iteration 2179 loss 0.33065852522850037\n",
      "Iteration 2180 loss 0.3247964084148407\n",
      "Iteration 2181 loss 0.3562493622303009\n",
      "Iteration 2182 loss 0.3399779796600342\n",
      "Iteration 2183 loss 0.32555413246154785\n",
      "Iteration 2184 loss 0.35722145438194275\n",
      "Iteration 2185 loss 0.35789912939071655\n",
      "Iteration 2186 loss 0.2858377993106842\n",
      "Iteration 2187 loss 0.34853076934814453\n",
      "Iteration 2188 loss 0.3288227319717407\n",
      "Iteration 2189 loss 0.37987813353538513\n",
      "Iteration 2190 loss 0.390928715467453\n",
      "Iteration 2191 loss 0.34346532821655273\n",
      "Iteration 2192 loss 0.33999449014663696\n",
      "Iteration 2193 loss 0.34163525700569153\n",
      "Iteration 2194 loss 0.3424304127693176\n",
      "Iteration 2195 loss 0.3317534029483795\n",
      "Iteration 2196 loss 0.3098742365837097\n",
      "Iteration 2197 loss 0.3427358865737915\n",
      "Iteration 2198 loss 0.35225290060043335\n",
      "Iteration 2199 loss 0.3604653477668762\n",
      "Iteration 2200 loss 0.3906175196170807\n",
      "Iteration 2201 loss 0.30801963806152344\n",
      "Iteration 2202 loss 0.4004916250705719\n",
      "Iteration 2203 loss 0.3284483551979065\n",
      "Iteration 2204 loss 0.34273889660835266\n",
      "Iteration 2205 loss 0.34909212589263916\n",
      "Iteration 2206 loss 0.2953997254371643\n",
      "Iteration 2207 loss 0.33068379759788513\n",
      "Iteration 2208 loss 0.35349035263061523\n",
      "Iteration 2209 loss 0.3409654200077057\n",
      "Iteration 2210 loss 0.2906738519668579\n",
      "Iteration 2211 loss 0.3941723704338074\n",
      "Iteration 2212 loss 0.35012200474739075\n",
      "Iteration 2213 loss 0.3802715837955475\n",
      "Iteration 2214 loss 0.3412451148033142\n",
      "Iteration 2215 loss 0.3203793168067932\n",
      "Iteration 2216 loss 0.3586055636405945\n",
      "Iteration 2217 loss 0.3296642005443573\n",
      "Iteration 2218 loss 0.34611696004867554\n",
      "Iteration 2219 loss 0.35146772861480713\n",
      "Iteration 2220 loss 0.3821523189544678\n",
      "Iteration 2221 loss 0.34898579120635986\n",
      "Iteration 2222 loss 0.3483060896396637\n",
      "Iteration 2223 loss 0.3616550862789154\n",
      "Iteration 2224 loss 0.38280028104782104\n",
      "Iteration 2225 loss 0.3392229676246643\n",
      "Iteration 2226 loss 0.3765517771244049\n",
      "Iteration 2227 loss 0.3827373683452606\n",
      "Iteration 2228 loss 0.3514645993709564\n",
      "Iteration 2229 loss 0.34975266456604004\n",
      "Iteration 2230 loss 0.3704488277435303\n",
      "Iteration 2231 loss 0.35826703906059265\n",
      "Iteration 2232 loss 0.33321380615234375\n",
      "Iteration 2233 loss 0.38754576444625854\n",
      "Iteration 2234 loss 0.3228718042373657\n",
      "Iteration 2235 loss 0.3674948811531067\n",
      "Iteration 2236 loss 0.30584412813186646\n",
      "Iteration 2237 loss 0.3078944683074951\n",
      "Iteration 2238 loss 0.32427382469177246\n",
      "Iteration 2239 loss 0.3362535238265991\n",
      "Iteration 2240 loss 0.36947280168533325\n",
      "Iteration 2241 loss 0.34297868609428406\n",
      "Iteration 2242 loss 0.38487353920936584\n",
      "Iteration 2243 loss 0.3384743630886078\n",
      "Iteration 2244 loss 0.352450966835022\n",
      "Iteration 2245 loss 0.3623397648334503\n",
      "Iteration 2246 loss 0.3850507438182831\n",
      "Iteration 2247 loss 0.3926085829734802\n",
      "Iteration 2248 loss 0.39526674151420593\n",
      "Iteration 2249 loss 0.3598838150501251\n",
      "Iteration 2250 loss 0.3114033043384552\n",
      "Iteration 2251 loss 0.364635169506073\n",
      "Iteration 2252 loss 0.35961806774139404\n",
      "Iteration 2253 loss 0.32308295369148254\n",
      "Iteration 2254 loss 0.33598631620407104\n",
      "Iteration 2255 loss 0.31468266248703003\n",
      "Iteration 2256 loss 0.3613618314266205\n",
      "Iteration 2257 loss 0.28649890422821045\n",
      "Iteration 2258 loss 0.36348414421081543\n",
      "Iteration 2259 loss 0.3399302363395691\n",
      "Iteration 2260 loss 0.3435571789741516\n",
      "Iteration 2261 loss 0.374754935503006\n",
      "Iteration 2262 loss 0.3053722679615021\n",
      "Iteration 2263 loss 0.3627990484237671\n",
      "Iteration 2264 loss 0.35743576288223267\n",
      "Iteration 2265 loss 0.3801216781139374\n",
      "Iteration 2266 loss 0.36326730251312256\n",
      "Iteration 2267 loss 0.34972232580184937\n",
      "Iteration 2268 loss 0.3124837279319763\n",
      "Iteration 2269 loss 0.33295026421546936\n",
      "Iteration 2270 loss 0.37805265188217163\n",
      "Iteration 2271 loss 0.3278600573539734\n",
      "Iteration 2272 loss 0.36396539211273193\n",
      "Iteration 2273 loss 0.3547106981277466\n",
      "Iteration 2274 loss 0.36871275305747986\n",
      "Iteration 2275 loss 0.32777512073516846\n",
      "Iteration 2276 loss 0.3258760869503021\n",
      "Iteration 2277 loss 0.3024280071258545\n",
      "Iteration 2278 loss 0.34086960554122925\n",
      "Iteration 2279 loss 0.3537083566188812\n",
      "Iteration 2280 loss 0.32873091101646423\n",
      "Iteration 2281 loss 0.31463226675987244\n",
      "Iteration 2282 loss 0.3552468717098236\n",
      "Iteration 2283 loss 0.3168325424194336\n",
      "Iteration 2284 loss 0.3145236372947693\n",
      "Iteration 2285 loss 0.32699066400527954\n",
      "Iteration 2286 loss 0.34600335359573364\n",
      "Iteration 2287 loss 0.3090379238128662\n",
      "Iteration 2288 loss 0.33769893646240234\n",
      "Iteration 2289 loss 0.30943435430526733\n",
      "Iteration 2290 loss 0.29515770077705383\n",
      "Iteration 2291 loss 0.3629099726676941\n",
      "Iteration 2292 loss 0.33765989542007446\n",
      "Iteration 2293 loss 0.3689005672931671\n",
      "Iteration 2294 loss 0.29728877544403076\n",
      "Iteration 2295 loss 0.3444487154483795\n",
      "Iteration 2296 loss 0.34480440616607666\n",
      "Iteration 2297 loss 0.3827492296695709\n",
      "Iteration 2298 loss 0.35725730657577515\n",
      "Iteration 2299 loss 0.3507862687110901\n",
      "Iteration 2300 loss 0.29096701741218567\n",
      "Iteration 2301 loss 0.36397457122802734\n",
      "Iteration 2302 loss 0.34779006242752075\n",
      "Iteration 2303 loss 0.3618551790714264\n",
      "Iteration 2304 loss 0.3244968354701996\n",
      "Iteration 2305 loss 0.3455095887184143\n",
      "Iteration 2306 loss 0.3638642132282257\n",
      "Iteration 2307 loss 0.37707898020744324\n",
      "Iteration 2308 loss 0.33744537830352783\n",
      "Iteration 2309 loss 0.3592326045036316\n",
      "Iteration 2310 loss 0.36706340312957764\n",
      "Iteration 2311 loss 0.38308602571487427\n",
      "Iteration 2312 loss 0.3362111449241638\n",
      "Iteration 2313 loss 0.36647266149520874\n",
      "Iteration 2314 loss 0.34520766139030457\n",
      "Iteration 2315 loss 0.33593177795410156\n",
      "Iteration 2316 loss 0.33587273955345154\n",
      "Iteration 2317 loss 0.3440181314945221\n",
      "Iteration 2318 loss 0.338182270526886\n",
      "Iteration 2319 loss 0.338145911693573\n",
      "Iteration 2320 loss 0.3677425682544708\n",
      "Iteration 2321 loss 0.3172416090965271\n",
      "Iteration 2322 loss 0.3367959260940552\n",
      "Iteration 2323 loss 0.3513430655002594\n",
      "Iteration 2324 loss 0.3101693093776703\n",
      "Iteration 2325 loss 0.37656596302986145\n",
      "Iteration 2326 loss 0.34184837341308594\n",
      "Iteration 2327 loss 0.30802416801452637\n",
      "Iteration 2328 loss 0.3281956613063812\n",
      "Iteration 2329 loss 0.32045257091522217\n",
      "Iteration 2330 loss 0.3307214677333832\n",
      "Iteration 2331 loss 0.3181506395339966\n",
      "Iteration 2332 loss 0.3579861521720886\n",
      "Iteration 2333 loss 0.32235726714134216\n",
      "Iteration 2334 loss 0.34247004985809326\n",
      "Iteration 2335 loss 0.34467676281929016\n",
      "Iteration 2336 loss 0.3342825472354889\n",
      "Iteration 2337 loss 0.35099688172340393\n",
      "Iteration 2338 loss 0.36095723509788513\n",
      "Iteration 2339 loss 0.35957908630371094\n",
      "Iteration 2340 loss 0.3072987198829651\n",
      "Iteration 2341 loss 0.3445058763027191\n",
      "Iteration 2342 loss 0.33966150879859924\n",
      "Iteration 2343 loss 0.33493927121162415\n",
      "Iteration 2344 loss 0.33023014664649963\n",
      "Iteration 2345 loss 0.3109440803527832\n",
      "Iteration 2346 loss 0.30818188190460205\n",
      "Iteration 2347 loss 0.33551454544067383\n",
      "Iteration 2348 loss 0.3181937634944916\n",
      "Iteration 2349 loss 0.3539879620075226\n",
      "Iteration 2350 loss 0.3106016516685486\n",
      "Iteration 2351 loss 0.2914554476737976\n",
      "Iteration 2352 loss 0.39146196842193604\n",
      "Iteration 2353 loss 0.3118894100189209\n",
      "Iteration 2354 loss 0.30039358139038086\n",
      "Iteration 2355 loss 0.34297534823417664\n",
      "Iteration 2356 loss 0.3385617136955261\n",
      "Iteration 2357 loss 0.3591049015522003\n",
      "Iteration 2358 loss 0.3514901101589203\n",
      "Iteration 2359 loss 0.303898423910141\n",
      "Iteration 2360 loss 0.37591445446014404\n",
      "Iteration 2361 loss 0.288783460855484\n",
      "Iteration 2362 loss 0.38717973232269287\n",
      "Iteration 2363 loss 0.33227819204330444\n",
      "Iteration 2364 loss 0.3082341253757477\n",
      "Iteration 2365 loss 0.35601797699928284\n",
      "Iteration 2366 loss 0.34153151512145996\n",
      "Iteration 2367 loss 0.3366248607635498\n",
      "Iteration 2368 loss 0.33446595072746277\n",
      "Iteration 2369 loss 0.3483854830265045\n",
      "Iteration 2370 loss 0.3076808750629425\n",
      "Iteration 2371 loss 0.3187882900238037\n",
      "Iteration 2372 loss 0.3357539176940918\n",
      "Iteration 2373 loss 0.335696816444397\n",
      "Iteration 2374 loss 0.34622687101364136\n",
      "Iteration 2375 loss 0.3329012989997864\n",
      "Iteration 2376 loss 0.3392225205898285\n",
      "Iteration 2377 loss 0.33806708455085754\n",
      "Iteration 2378 loss 0.3428235352039337\n",
      "Iteration 2379 loss 0.33936837315559387\n",
      "Iteration 2380 loss 0.32570260763168335\n",
      "Iteration 2381 loss 0.3227926790714264\n",
      "Iteration 2382 loss 0.35163700580596924\n",
      "Iteration 2383 loss 0.2991153597831726\n",
      "Iteration 2384 loss 0.36494481563568115\n",
      "Iteration 2385 loss 0.3310861587524414\n",
      "Iteration 2386 loss 0.35391536355018616\n",
      "Iteration 2387 loss 0.37037456035614014\n",
      "Iteration 2388 loss 0.35319221019744873\n",
      "Iteration 2389 loss 0.31936943531036377\n",
      "Iteration 2390 loss 0.3382047414779663\n",
      "Iteration 2391 loss 0.36249908804893494\n",
      "Iteration 2392 loss 0.35951974987983704\n",
      "Iteration 2393 loss 0.34266796708106995\n",
      "Iteration 2394 loss 0.32187119126319885\n",
      "Iteration 2395 loss 0.35992297530174255\n",
      "Iteration 2396 loss 0.3306697607040405\n",
      "Iteration 2397 loss 0.3700377941131592\n",
      "Iteration 2398 loss 0.3132469356060028\n",
      "Iteration 2399 loss 0.28357023000717163\n",
      "Iteration 2400 loss 0.31575194001197815\n",
      "Iteration 2401 loss 0.30637213587760925\n",
      "Iteration 2402 loss 0.3342207670211792\n",
      "Iteration 2403 loss 0.37339478731155396\n",
      "Iteration 2404 loss 0.3767792582511902\n",
      "Iteration 2405 loss 0.3228450119495392\n",
      "Iteration 2406 loss 0.307177871465683\n",
      "Iteration 2407 loss 0.2873333692550659\n",
      "Iteration 2408 loss 0.32461509108543396\n",
      "Iteration 2409 loss 0.32811182737350464\n",
      "Iteration 2410 loss 0.3220471739768982\n",
      "Iteration 2411 loss 0.3188878893852234\n",
      "Iteration 2412 loss 0.2819003164768219\n",
      "Iteration 2413 loss 0.32933568954467773\n",
      "Iteration 2414 loss 0.2932210862636566\n",
      "Iteration 2415 loss 0.3198314309120178\n",
      "Iteration 2416 loss 0.3165886402130127\n",
      "Iteration 2417 loss 0.3170180320739746\n",
      "Iteration 2418 loss 0.2933696508407593\n",
      "Iteration 2419 loss 0.3291281461715698\n",
      "Iteration 2420 loss 0.2847414016723633\n",
      "Iteration 2421 loss 0.3568383455276489\n",
      "Iteration 2422 loss 0.3578140437602997\n",
      "Iteration 2423 loss 0.393416166305542\n",
      "Iteration 2424 loss 0.29569441080093384\n",
      "Iteration 2425 loss 0.3063501715660095\n",
      "Iteration 2426 loss 0.32557711005210876\n",
      "Iteration 2427 loss 0.2996794879436493\n",
      "Iteration 2428 loss 0.3125617504119873\n",
      "Iteration 2429 loss 0.35141098499298096\n",
      "Iteration 2430 loss 0.3322449326515198\n",
      "Iteration 2431 loss 0.3437027633190155\n",
      "Iteration 2432 loss 0.29681622982025146\n",
      "Iteration 2433 loss 0.32396993041038513\n",
      "Iteration 2434 loss 0.31218889355659485\n",
      "Iteration 2435 loss 0.33630481362342834\n",
      "Iteration 2436 loss 0.31110215187072754\n",
      "Iteration 2437 loss 0.2985479533672333\n",
      "Iteration 2438 loss 0.32321280241012573\n",
      "Iteration 2439 loss 0.3494472801685333\n",
      "Iteration 2440 loss 0.31969118118286133\n",
      "Iteration 2441 loss 0.308829128742218\n",
      "Iteration 2442 loss 0.2563707232475281\n",
      "Iteration 2443 loss 0.2727770507335663\n",
      "Iteration 2444 loss 0.3218414783477783\n",
      "Iteration 2445 loss 0.3076191842556\n",
      "Iteration 2446 loss 0.31803563237190247\n",
      "Iteration 2447 loss 0.3479417860507965\n",
      "Iteration 2448 loss 0.32513561844825745\n",
      "Iteration 2449 loss 0.297935426235199\n",
      "Iteration 2450 loss 0.2726535201072693\n",
      "Iteration 2451 loss 0.31689733266830444\n",
      "Iteration 2452 loss 0.3321966528892517\n",
      "Iteration 2453 loss 0.29860907793045044\n",
      "Iteration 2454 loss 0.2763199806213379\n",
      "Iteration 2455 loss 0.2819311022758484\n",
      "Iteration 2456 loss 0.32417893409729004\n",
      "Iteration 2457 loss 0.33679261803627014\n",
      "Iteration 2458 loss 0.3596682548522949\n",
      "Iteration 2459 loss 0.3047559857368469\n",
      "Iteration 2460 loss 0.35100069642066956\n",
      "Iteration 2461 loss 0.31220805644989014\n",
      "Iteration 2462 loss 0.37305644154548645\n",
      "Iteration 2463 loss 0.29890546202659607\n",
      "Iteration 2464 loss 0.3240465521812439\n",
      "Iteration 2465 loss 0.2874351441860199\n",
      "Iteration 2466 loss 0.3173505365848541\n",
      "Iteration 2467 loss 0.2979962229728699\n",
      "Iteration 2468 loss 0.32645541429519653\n",
      "Iteration 2469 loss 0.3145076036453247\n",
      "Iteration 2470 loss 0.3268119990825653\n",
      "Iteration 2471 loss 0.346951425075531\n",
      "Iteration 2472 loss 0.3102989196777344\n",
      "Iteration 2473 loss 0.37383952736854553\n",
      "Iteration 2474 loss 0.27475064992904663\n",
      "Iteration 2475 loss 0.30969399213790894\n",
      "Iteration 2476 loss 0.3530190587043762\n",
      "Iteration 2477 loss 0.35791730880737305\n",
      "Iteration 2478 loss 0.3312583267688751\n",
      "Iteration 2479 loss 0.314853310585022\n",
      "Iteration 2480 loss 0.31523001194000244\n",
      "Iteration 2481 loss 0.3218579888343811\n",
      "Iteration 2482 loss 0.3277103304862976\n",
      "Iteration 2483 loss 0.3355526030063629\n",
      "Iteration 2484 loss 0.3142864406108856\n",
      "Iteration 2485 loss 0.344538152217865\n",
      "Iteration 2486 loss 0.37250903248786926\n",
      "Iteration 2487 loss 0.29986631870269775\n",
      "Iteration 2488 loss 0.3280854821205139\n",
      "Iteration 2489 loss 0.3275853991508484\n",
      "Iteration 2490 loss 0.3351937532424927\n",
      "Iteration 2491 loss 0.35551536083221436\n",
      "Iteration 2492 loss 0.3390367031097412\n",
      "Iteration 2493 loss 0.3796067237854004\n",
      "Iteration 2494 loss 0.34282386302948\n",
      "Iteration 2495 loss 0.2962905168533325\n",
      "Iteration 2496 loss 0.3208811283111572\n",
      "Iteration 2497 loss 0.3369247615337372\n",
      "Iteration 2498 loss 0.3842576742172241\n",
      "Iteration 2499 loss 0.3302804231643677\n",
      "Iteration 2500 loss 0.37915676832199097\n",
      "Iteration 2501 loss 0.3651424050331116\n",
      "Iteration 2502 loss 0.3009398579597473\n",
      "Iteration 2503 loss 0.33588576316833496\n",
      "Iteration 2504 loss 0.333197683095932\n",
      "Iteration 2505 loss 0.358010470867157\n",
      "Iteration 2506 loss 0.34287208318710327\n",
      "Iteration 2507 loss 0.31235432624816895\n",
      "Iteration 2508 loss 0.36527639627456665\n",
      "Iteration 2509 loss 0.29368317127227783\n",
      "Iteration 2510 loss 0.32033342123031616\n",
      "Iteration 2511 loss 0.32425063848495483\n",
      "Iteration 2512 loss 0.3395102322101593\n",
      "Iteration 2513 loss 0.3193976581096649\n",
      "Iteration 2514 loss 0.27972033619880676\n",
      "Iteration 2515 loss 0.3097524046897888\n",
      "Iteration 2516 loss 0.313098281621933\n",
      "Iteration 2517 loss 0.303890585899353\n",
      "Iteration 2518 loss 0.35325154662132263\n",
      "Iteration 2519 loss 0.3237890601158142\n",
      "Iteration 2520 loss 0.31460368633270264\n",
      "Iteration 2521 loss 0.34245187044143677\n",
      "Iteration 2522 loss 0.3130788207054138\n",
      "Iteration 2523 loss 0.28406330943107605\n",
      "Iteration 2524 loss 0.2970811128616333\n",
      "Iteration 2525 loss 0.3302350342273712\n",
      "Iteration 2526 loss 0.3123966455459595\n",
      "Iteration 2527 loss 0.30349576473236084\n",
      "Iteration 2528 loss 0.28837597370147705\n",
      "Iteration 2529 loss 0.31528574228286743\n",
      "Iteration 2530 loss 0.32058951258659363\n",
      "Iteration 2531 loss 0.28644707798957825\n",
      "Iteration 2532 loss 0.2829779088497162\n",
      "Iteration 2533 loss 0.30587154626846313\n",
      "Iteration 2534 loss 0.3122422993183136\n",
      "Iteration 2535 loss 0.37445512413978577\n",
      "Iteration 2536 loss 0.24676989018917084\n",
      "Iteration 2537 loss 0.3062167465686798\n",
      "Iteration 2538 loss 0.2903107702732086\n",
      "Iteration 2539 loss 0.33385053277015686\n",
      "Iteration 2540 loss 0.3118164539337158\n",
      "Iteration 2541 loss 0.30441814661026\n",
      "Iteration 2542 loss 0.32097044587135315\n",
      "Iteration 2543 loss 0.33280202746391296\n",
      "Iteration 2544 loss 0.31012481451034546\n",
      "Iteration 2545 loss 0.33777111768722534\n",
      "Iteration 2546 loss 0.2886037826538086\n",
      "Iteration 2547 loss 0.36290237307548523\n",
      "Iteration 2548 loss 0.32792752981185913\n",
      "Iteration 2549 loss 0.32550373673439026\n",
      "Iteration 2550 loss 0.29382407665252686\n",
      "Iteration 2551 loss 0.3133484721183777\n",
      "Iteration 2552 loss 0.30513641238212585\n",
      "Iteration 2553 loss 0.3213197886943817\n",
      "Iteration 2554 loss 0.31235432624816895\n",
      "Iteration 2555 loss 0.31234171986579895\n",
      "Iteration 2556 loss 0.3300991952419281\n",
      "Iteration 2557 loss 0.34005630016326904\n",
      "Iteration 2558 loss 0.3577128052711487\n",
      "Iteration 2559 loss 0.3792857229709625\n",
      "Iteration 2560 loss 0.2972877025604248\n",
      "Iteration 2561 loss 0.35854554176330566\n",
      "Iteration 2562 loss 0.30853888392448425\n",
      "Iteration 2563 loss 0.3297070264816284\n",
      "Iteration 2564 loss 0.312206506729126\n",
      "Iteration 2565 loss 0.3181670904159546\n",
      "Iteration 2566 loss 0.349281907081604\n",
      "Iteration 2567 loss 0.3021546006202698\n",
      "Iteration 2568 loss 0.3340838849544525\n",
      "Iteration 2569 loss 0.3222891390323639\n",
      "Iteration 2570 loss 0.3191380798816681\n",
      "Iteration 2571 loss 0.34775298833847046\n",
      "Iteration 2572 loss 0.2971283793449402\n",
      "Iteration 2573 loss 0.35155773162841797\n",
      "Iteration 2574 loss 0.3246362507343292\n",
      "Iteration 2575 loss 0.31003183126449585\n",
      "Iteration 2576 loss 0.36489957571029663\n",
      "Iteration 2577 loss 0.289263516664505\n",
      "Iteration 2578 loss 0.30228060483932495\n",
      "Iteration 2579 loss 0.33763387799263\n",
      "Iteration 2580 loss 0.32017087936401367\n",
      "Iteration 2581 loss 0.3275754451751709\n",
      "Iteration 2582 loss 0.3278152644634247\n",
      "Iteration 2583 loss 0.33659330010414124\n",
      "Iteration 2584 loss 0.32147443294525146\n",
      "Iteration 2585 loss 0.32068777084350586\n",
      "Iteration 2586 loss 0.3109583854675293\n",
      "Iteration 2587 loss 0.32460901141166687\n",
      "Iteration 2588 loss 0.32933858036994934\n",
      "Iteration 2589 loss 0.2711467742919922\n",
      "Iteration 2590 loss 0.31055450439453125\n",
      "Iteration 2591 loss 0.2890862226486206\n",
      "Iteration 2592 loss 0.3013495206832886\n",
      "Iteration 2593 loss 0.31461137533187866\n",
      "Iteration 2594 loss 0.2589493691921234\n",
      "Iteration 2595 loss 0.27352210879325867\n",
      "Iteration 2596 loss 0.2741336226463318\n",
      "Iteration 2597 loss 0.2720769941806793\n",
      "Iteration 2598 loss 0.352976530790329\n",
      "Iteration 2599 loss 0.31125593185424805\n",
      "Iteration 2600 loss 0.3281209170818329\n",
      "Iteration 2601 loss 0.34485697746276855\n",
      "Iteration 2602 loss 0.313598096370697\n",
      "Iteration 2603 loss 0.3296383321285248\n",
      "Iteration 2604 loss 0.320821613073349\n",
      "Iteration 2605 loss 0.28341782093048096\n",
      "Iteration 2606 loss 0.33391016721725464\n",
      "Iteration 2607 loss 0.2967013716697693\n",
      "Iteration 2608 loss 0.34446224570274353\n",
      "Iteration 2609 loss 0.29864394664764404\n",
      "Iteration 2610 loss 0.3492506444454193\n",
      "Iteration 2611 loss 0.33022138476371765\n",
      "Iteration 2612 loss 0.3226760923862457\n",
      "Iteration 2613 loss 0.29795578122138977\n",
      "Iteration 2614 loss 0.3191728889942169\n",
      "Iteration 2615 loss 0.36174696683883667\n",
      "Iteration 2616 loss 0.32014477252960205\n",
      "Iteration 2617 loss 0.36374324560165405\n",
      "Iteration 2618 loss 0.29762303829193115\n",
      "Iteration 2619 loss 0.297109991312027\n",
      "Iteration 2620 loss 0.3344353437423706\n",
      "Iteration 2621 loss 0.3226673901081085\n",
      "Iteration 2622 loss 0.31221097707748413\n",
      "Iteration 2623 loss 0.32770588994026184\n",
      "Iteration 2624 loss 0.315955251455307\n",
      "Iteration 2625 loss 0.3394351899623871\n",
      "Iteration 2626 loss 0.323496550321579\n",
      "Iteration 2627 loss 0.28947317600250244\n",
      "Iteration 2628 loss 0.31905242800712585\n",
      "Iteration 2629 loss 0.31786248087882996\n",
      "Iteration 2630 loss 0.32492300868034363\n",
      "Iteration 2631 loss 0.3253364861011505\n",
      "Iteration 2632 loss 0.32170310616493225\n",
      "Iteration 2633 loss 0.27923157811164856\n",
      "Iteration 2634 loss 0.3000035285949707\n",
      "Iteration 2635 loss 0.33066433668136597\n",
      "Iteration 2636 loss 0.2692338824272156\n",
      "Iteration 2637 loss 0.34032395482063293\n",
      "Iteration 2638 loss 0.35268449783325195\n",
      "Iteration 2639 loss 0.29012200236320496\n",
      "Iteration 2640 loss 0.33706533908843994\n",
      "Iteration 2641 loss 0.33795976638793945\n",
      "Iteration 2642 loss 0.3061157763004303\n",
      "Iteration 2643 loss 0.35026270151138306\n",
      "Iteration 2644 loss 0.34467190504074097\n",
      "Iteration 2645 loss 0.3160730302333832\n",
      "Iteration 2646 loss 0.30621376633644104\n",
      "Iteration 2647 loss 0.33846405148506165\n",
      "Iteration 2648 loss 0.350494921207428\n",
      "Iteration 2649 loss 0.3530503809452057\n",
      "Iteration 2650 loss 0.3506125807762146\n",
      "Iteration 2651 loss 0.3513408601284027\n",
      "Iteration 2652 loss 0.33913084864616394\n",
      "Iteration 2653 loss 0.2753508985042572\n",
      "Iteration 2654 loss 0.3434804677963257\n",
      "Iteration 2655 loss 0.3118520975112915\n",
      "Iteration 2656 loss 0.31739503145217896\n",
      "Iteration 2657 loss 0.29831984639167786\n",
      "Iteration 2658 loss 0.30001258850097656\n",
      "Iteration 2659 loss 0.34893471002578735\n",
      "Iteration 2660 loss 0.34536173939704895\n",
      "Iteration 2661 loss 0.3497428894042969\n",
      "Iteration 2662 loss 0.2973429560661316\n",
      "Iteration 2663 loss 0.3350304067134857\n",
      "Iteration 2664 loss 0.3387719392776489\n",
      "Iteration 2665 loss 0.33845245838165283\n",
      "Iteration 2666 loss 0.32052361965179443\n",
      "Iteration 2667 loss 0.32346588373184204\n",
      "Iteration 2668 loss 0.3345482051372528\n",
      "Iteration 2669 loss 0.34618401527404785\n",
      "Iteration 2670 loss 0.31245315074920654\n",
      "Iteration 2671 loss 0.3494250774383545\n",
      "Iteration 2672 loss 0.32654592394828796\n",
      "Iteration 2673 loss 0.2885949909687042\n",
      "Iteration 2674 loss 0.2981385588645935\n",
      "Iteration 2675 loss 0.28293198347091675\n",
      "Iteration 2676 loss 0.33651718497276306\n",
      "Iteration 2677 loss 0.294065922498703\n",
      "Iteration 2678 loss 0.3202221691608429\n",
      "Iteration 2679 loss 0.3047875761985779\n",
      "Iteration 2680 loss 0.329444020986557\n",
      "Iteration 2681 loss 0.2973535358905792\n",
      "Iteration 2682 loss 0.25764933228492737\n",
      "Iteration 2683 loss 0.29310792684555054\n",
      "Iteration 2684 loss 0.2715107202529907\n",
      "Iteration 2685 loss 0.3118656277656555\n",
      "Iteration 2686 loss 0.28477370738983154\n",
      "Iteration 2687 loss 0.27705565094947815\n",
      "Iteration 2688 loss 0.2893492579460144\n",
      "Iteration 2689 loss 0.28803160786628723\n",
      "Iteration 2690 loss 0.315666526556015\n",
      "Iteration 2691 loss 0.2719905376434326\n",
      "Iteration 2692 loss 0.28821539878845215\n",
      "Iteration 2693 loss 0.2825040817260742\n",
      "Iteration 2694 loss 0.2683221399784088\n",
      "Iteration 2695 loss 0.3110024631023407\n",
      "Iteration 2696 loss 0.3202611207962036\n",
      "Iteration 2697 loss 0.30119121074676514\n",
      "Iteration 2698 loss 0.2739010751247406\n",
      "Iteration 2699 loss 0.29895439743995667\n",
      "Iteration 2700 loss 0.28466615080833435\n",
      "Iteration 2701 loss 0.29033583402633667\n",
      "Iteration 2702 loss 0.3114699125289917\n",
      "Iteration 2703 loss 0.30560222268104553\n",
      "Iteration 2704 loss 0.2787007689476013\n",
      "Iteration 2705 loss 0.29647842049598694\n",
      "Iteration 2706 loss 0.3128771185874939\n",
      "Iteration 2707 loss 0.302351176738739\n",
      "Iteration 2708 loss 0.2968827486038208\n",
      "Iteration 2709 loss 0.34923824667930603\n",
      "Iteration 2710 loss 0.3132578730583191\n",
      "Iteration 2711 loss 0.2807602286338806\n",
      "Iteration 2712 loss 0.30947449803352356\n",
      "Iteration 2713 loss 0.3164084851741791\n",
      "Iteration 2714 loss 0.31852906942367554\n",
      "Iteration 2715 loss 0.32081252336502075\n",
      "Iteration 2716 loss 0.3753378987312317\n",
      "Iteration 2717 loss 0.32457658648490906\n",
      "Iteration 2718 loss 0.33370012044906616\n",
      "Iteration 2719 loss 0.3285001218318939\n",
      "Iteration 2720 loss 0.3168138265609741\n",
      "Iteration 2721 loss 0.34896910190582275\n",
      "Iteration 2722 loss 0.3077264130115509\n",
      "Iteration 2723 loss 0.3159160614013672\n",
      "Iteration 2724 loss 0.3028869926929474\n",
      "Iteration 2725 loss 0.30661511421203613\n",
      "Iteration 2726 loss 0.28783345222473145\n",
      "Iteration 2727 loss 0.30387425422668457\n",
      "Iteration 2728 loss 0.31900477409362793\n",
      "Iteration 2729 loss 0.2906419634819031\n",
      "Iteration 2730 loss 0.2768145799636841\n",
      "Iteration 2731 loss 0.33007752895355225\n",
      "Iteration 2732 loss 0.2845563590526581\n",
      "Iteration 2733 loss 0.3108755946159363\n",
      "Iteration 2734 loss 0.3029208183288574\n",
      "Iteration 2735 loss 0.3074202835559845\n",
      "Iteration 2736 loss 0.2913915514945984\n",
      "Iteration 2737 loss 0.2800365686416626\n",
      "Iteration 2738 loss 0.27495840191841125\n",
      "Iteration 2739 loss 0.2824770510196686\n",
      "Iteration 2740 loss 0.2920379638671875\n",
      "Iteration 2741 loss 0.31379127502441406\n",
      "Iteration 2742 loss 0.3264584541320801\n",
      "Iteration 2743 loss 0.32880744338035583\n",
      "Iteration 2744 loss 0.298993319272995\n",
      "Iteration 2745 loss 0.2969222068786621\n",
      "Iteration 2746 loss 0.32169109582901\n",
      "Iteration 2747 loss 0.2822120189666748\n",
      "Iteration 2748 loss 0.3397189974784851\n",
      "Iteration 2749 loss 0.3155795633792877\n",
      "Iteration 2750 loss 0.313899964094162\n",
      "Iteration 2751 loss 0.3036741018295288\n",
      "Iteration 2752 loss 0.3056820034980774\n",
      "Iteration 2753 loss 0.33171042799949646\n",
      "Iteration 2754 loss 0.31009575724601746\n",
      "Iteration 2755 loss 0.28074657917022705\n",
      "Iteration 2756 loss 0.3110857903957367\n",
      "Iteration 2757 loss 0.3242385685443878\n",
      "Iteration 2758 loss 0.28714436292648315\n",
      "Iteration 2759 loss 0.3085783123970032\n",
      "Iteration 2760 loss 0.2655966579914093\n",
      "Iteration 2761 loss 0.35969796776771545\n",
      "Iteration 2762 loss 0.2840527296066284\n",
      "Iteration 2763 loss 0.30943548679351807\n",
      "Iteration 2764 loss 0.28929099440574646\n",
      "Iteration 2765 loss 0.26346728205680847\n",
      "Iteration 2766 loss 0.300033837556839\n",
      "Iteration 2767 loss 0.3284417986869812\n",
      "Iteration 2768 loss 0.3150985836982727\n",
      "Iteration 2769 loss 0.30825790762901306\n",
      "Iteration 2770 loss 0.3116800785064697\n",
      "Iteration 2771 loss 0.3257831633090973\n",
      "Iteration 2772 loss 0.29875418543815613\n",
      "Iteration 2773 loss 0.32923775911331177\n",
      "Iteration 2774 loss 0.32268375158309937\n",
      "Iteration 2775 loss 0.2984617352485657\n",
      "Iteration 2776 loss 0.3093908429145813\n",
      "Iteration 2777 loss 0.275913804769516\n",
      "Iteration 2778 loss 0.2964428663253784\n",
      "Iteration 2779 loss 0.34010642766952515\n",
      "Iteration 2780 loss 0.302010178565979\n",
      "Iteration 2781 loss 0.31245988607406616\n",
      "Iteration 2782 loss 0.2965686321258545\n",
      "Iteration 2783 loss 0.3021717369556427\n",
      "Iteration 2784 loss 0.2942010462284088\n",
      "Iteration 2785 loss 0.2965494394302368\n",
      "Iteration 2786 loss 0.277923583984375\n",
      "Iteration 2787 loss 0.29296326637268066\n",
      "Iteration 2788 loss 0.31439000368118286\n",
      "Iteration 2789 loss 0.31368109583854675\n",
      "Iteration 2790 loss 0.2971433997154236\n",
      "Iteration 2791 loss 0.2818368971347809\n",
      "Iteration 2792 loss 0.2983473241329193\n",
      "Iteration 2793 loss 0.32895293831825256\n",
      "Iteration 2794 loss 0.30813276767730713\n",
      "Iteration 2795 loss 0.29394590854644775\n",
      "Iteration 2796 loss 0.31229397654533386\n",
      "Iteration 2797 loss 0.3437846004962921\n",
      "Iteration 2798 loss 0.3082011342048645\n",
      "Iteration 2799 loss 0.30657505989074707\n",
      "Iteration 2800 loss 0.2991861402988434\n",
      "Iteration 2801 loss 0.3165830075740814\n",
      "Iteration 2802 loss 0.3392379581928253\n",
      "Iteration 2803 loss 0.31356632709503174\n",
      "Iteration 2804 loss 0.3122137486934662\n",
      "Iteration 2805 loss 0.28083336353302\n",
      "Iteration 2806 loss 0.3034482002258301\n",
      "Iteration 2807 loss 0.3023224174976349\n",
      "Iteration 2808 loss 0.30698320269584656\n",
      "Iteration 2809 loss 0.322700560092926\n",
      "Iteration 2810 loss 0.322359174489975\n",
      "Iteration 2811 loss 0.3271375298500061\n",
      "Iteration 2812 loss 0.2571408748626709\n",
      "Iteration 2813 loss 0.3220512866973877\n",
      "Iteration 2814 loss 0.27506816387176514\n",
      "Iteration 2815 loss 0.26951444149017334\n",
      "Iteration 2816 loss 0.27644336223602295\n",
      "Iteration 2817 loss 0.31646811962127686\n",
      "Iteration 2818 loss 0.32575511932373047\n",
      "Iteration 2819 loss 0.31922411918640137\n",
      "Iteration 2820 loss 0.3194509744644165\n",
      "Iteration 2821 loss 0.29392683506011963\n",
      "Iteration 2822 loss 0.2917422652244568\n",
      "Iteration 2823 loss 0.310899019241333\n",
      "Iteration 2824 loss 0.3117583394050598\n",
      "Iteration 2825 loss 0.30960485339164734\n",
      "Iteration 2826 loss 0.3409302234649658\n",
      "Iteration 2827 loss 0.31546252965927124\n",
      "Iteration 2828 loss 0.3261203169822693\n",
      "Iteration 2829 loss 0.3002154529094696\n",
      "Iteration 2830 loss 0.3186572194099426\n",
      "Iteration 2831 loss 0.3078140914440155\n",
      "Iteration 2832 loss 0.34359419345855713\n",
      "Iteration 2833 loss 0.2901344895362854\n",
      "Iteration 2834 loss 0.345588743686676\n",
      "Iteration 2835 loss 0.29085594415664673\n",
      "Iteration 2836 loss 0.3175208568572998\n",
      "Iteration 2837 loss 0.3018713593482971\n",
      "Iteration 2838 loss 0.3086510896682739\n",
      "Iteration 2839 loss 0.2609104812145233\n",
      "Iteration 2840 loss 0.3717593848705292\n",
      "Iteration 2841 loss 0.33841365575790405\n",
      "Iteration 2842 loss 0.2768009305000305\n",
      "Iteration 2843 loss 0.30128929018974304\n",
      "Iteration 2844 loss 0.32759448885917664\n",
      "Iteration 2845 loss 0.3146834671497345\n",
      "Iteration 2846 loss 0.285360723733902\n",
      "Iteration 2847 loss 0.28977587819099426\n",
      "Iteration 2848 loss 0.31688326597213745\n",
      "Iteration 2849 loss 0.313015341758728\n",
      "Iteration 2850 loss 0.346432089805603\n",
      "Iteration 2851 loss 0.32128241658210754\n",
      "Iteration 2852 loss 0.3104233741760254\n",
      "Iteration 2853 loss 0.25387439131736755\n",
      "Iteration 2854 loss 0.28582191467285156\n",
      "Iteration 2855 loss 0.2738083600997925\n",
      "Iteration 2856 loss 0.2985886037349701\n",
      "Iteration 2857 loss 0.3111084997653961\n",
      "Iteration 2858 loss 0.2809184491634369\n",
      "Iteration 2859 loss 0.27782732248306274\n",
      "Iteration 2860 loss 0.3149149417877197\n",
      "Iteration 2861 loss 0.29824098944664\n",
      "Iteration 2862 loss 0.2688891291618347\n",
      "Iteration 2863 loss 0.31382453441619873\n",
      "Iteration 2864 loss 0.2786965072154999\n",
      "Iteration 2865 loss 0.2963675856590271\n",
      "Iteration 2866 loss 0.2837711572647095\n",
      "Iteration 2867 loss 0.28956228494644165\n",
      "Iteration 2868 loss 0.2787359952926636\n",
      "Iteration 2869 loss 0.27340805530548096\n",
      "Iteration 2870 loss 0.31103479862213135\n",
      "Iteration 2871 loss 0.2763827443122864\n",
      "Iteration 2872 loss 0.31803813576698303\n",
      "Iteration 2873 loss 0.28888237476348877\n",
      "Iteration 2874 loss 0.29946666955947876\n",
      "Iteration 2875 loss 0.2505820393562317\n",
      "Iteration 2876 loss 0.2859262228012085\n",
      "Iteration 2877 loss 0.307892382144928\n",
      "Iteration 2878 loss 0.3164409399032593\n",
      "Iteration 2879 loss 0.28652697801589966\n",
      "Iteration 2880 loss 0.26228755712509155\n",
      "Iteration 2881 loss 0.3032402992248535\n",
      "Iteration 2882 loss 0.27885380387306213\n",
      "Iteration 2883 loss 0.2842614948749542\n",
      "Iteration 2884 loss 0.289969265460968\n",
      "Iteration 2885 loss 0.2831194996833801\n",
      "Iteration 2886 loss 0.28101587295532227\n",
      "Iteration 2887 loss 0.2327878475189209\n",
      "Iteration 2888 loss 0.30053070187568665\n",
      "Iteration 2889 loss 0.252999871969223\n",
      "Iteration 2890 loss 0.29826226830482483\n",
      "Iteration 2891 loss 0.3143712282180786\n",
      "Iteration 2892 loss 0.28561627864837646\n",
      "Iteration 2893 loss 0.30949151515960693\n",
      "Iteration 2894 loss 0.28835010528564453\n",
      "Iteration 2895 loss 0.2893851101398468\n",
      "Iteration 2896 loss 0.29101240634918213\n",
      "Iteration 2897 loss 0.30182433128356934\n",
      "Iteration 2898 loss 0.31661257147789\n",
      "Iteration 2899 loss 0.30775731801986694\n",
      "Iteration 2900 loss 0.29730454087257385\n",
      "Iteration 2901 loss 0.32053351402282715\n",
      "Iteration 2902 loss 0.3274194002151489\n",
      "Iteration 2903 loss 0.30517804622650146\n",
      "Iteration 2904 loss 0.3169325888156891\n",
      "Iteration 2905 loss 0.28956013917922974\n",
      "Iteration 2906 loss 0.3162862956523895\n",
      "Iteration 2907 loss 0.30988603830337524\n",
      "Iteration 2908 loss 0.2683139443397522\n",
      "Iteration 2909 loss 0.2812647223472595\n",
      "Iteration 2910 loss 0.32625633478164673\n",
      "Iteration 2911 loss 0.27910253405570984\n",
      "Iteration 2912 loss 0.3307640552520752\n",
      "Iteration 2913 loss 0.3035227954387665\n",
      "Iteration 2914 loss 0.3261792063713074\n",
      "Iteration 2915 loss 0.2731335759162903\n",
      "Iteration 2916 loss 0.3191271424293518\n",
      "Iteration 2917 loss 0.3363149166107178\n",
      "Iteration 2918 loss 0.31543466448783875\n",
      "Iteration 2919 loss 0.30763524770736694\n",
      "Iteration 2920 loss 0.3463923931121826\n",
      "Iteration 2921 loss 0.3188682198524475\n",
      "Iteration 2922 loss 0.3485221564769745\n",
      "Iteration 2923 loss 0.31244078278541565\n",
      "Iteration 2924 loss 0.3010377883911133\n",
      "Iteration 2925 loss 0.3178398609161377\n",
      "Iteration 2926 loss 0.3054201304912567\n",
      "Iteration 2927 loss 0.2748844623565674\n",
      "Iteration 2928 loss 0.2991383671760559\n",
      "Iteration 2929 loss 0.30030718445777893\n",
      "Iteration 2930 loss 0.3375420868396759\n",
      "Iteration 2931 loss 0.28940728306770325\n",
      "Iteration 2932 loss 0.3109068274497986\n",
      "Iteration 2933 loss 0.3128579556941986\n",
      "Iteration 2934 loss 0.310920387506485\n",
      "Iteration 2935 loss 0.2852945923805237\n",
      "Iteration 2936 loss 0.2981446385383606\n",
      "Iteration 2937 loss 0.2779109477996826\n",
      "Iteration 2938 loss 0.2990373373031616\n",
      "Iteration 2939 loss 0.2704034745693207\n",
      "Iteration 2940 loss 0.3249429166316986\n",
      "Iteration 2941 loss 0.2758173942565918\n",
      "Iteration 2942 loss 0.284490704536438\n",
      "Iteration 2943 loss 0.2885607182979584\n",
      "Iteration 2944 loss 0.27387678623199463\n",
      "Iteration 2945 loss 0.27905943989753723\n",
      "Iteration 2946 loss 0.31423723697662354\n",
      "Iteration 2947 loss 0.3195759654045105\n",
      "Iteration 2948 loss 0.33562707901000977\n",
      "Iteration 2949 loss 0.2858627438545227\n",
      "Iteration 2950 loss 0.25025367736816406\n",
      "Iteration 2951 loss 0.27422359585762024\n",
      "Iteration 2952 loss 0.3011084198951721\n",
      "Iteration 2953 loss 0.2986903786659241\n",
      "Iteration 2954 loss 0.26414597034454346\n",
      "Iteration 2955 loss 0.3232061266899109\n",
      "Iteration 2956 loss 0.32062992453575134\n",
      "Iteration 2957 loss 0.33129316568374634\n",
      "Iteration 2958 loss 0.3189908266067505\n",
      "Iteration 2959 loss 0.2766187787055969\n",
      "Iteration 2960 loss 0.29442736506462097\n",
      "Iteration 2961 loss 0.27857130765914917\n",
      "Iteration 2962 loss 0.2769624590873718\n",
      "Iteration 2963 loss 0.2917337715625763\n",
      "Iteration 2964 loss 0.30321741104125977\n",
      "Iteration 2965 loss 0.33222341537475586\n",
      "Iteration 2966 loss 0.29815924167633057\n",
      "Iteration 2967 loss 0.30582985281944275\n",
      "Iteration 2968 loss 0.32688993215560913\n",
      "Iteration 2969 loss 0.27964556217193604\n",
      "Iteration 2970 loss 0.28063544631004333\n",
      "Iteration 2971 loss 0.25652211904525757\n",
      "Iteration 2972 loss 0.33046606183052063\n",
      "Iteration 2973 loss 0.2656422257423401\n",
      "Iteration 2974 loss 0.29993367195129395\n",
      "Iteration 2975 loss 0.2969334125518799\n",
      "Iteration 2976 loss 0.30121898651123047\n",
      "Iteration 2977 loss 0.28915709257125854\n",
      "Iteration 2978 loss 0.2886459231376648\n",
      "Iteration 2979 loss 0.2806866765022278\n",
      "Iteration 2980 loss 0.26861199736595154\n",
      "Iteration 2981 loss 0.28002214431762695\n",
      "Iteration 2982 loss 0.2820785343647003\n",
      "Iteration 2983 loss 0.2696075439453125\n",
      "Iteration 2984 loss 0.3009072542190552\n",
      "Iteration 2985 loss 0.2975086569786072\n",
      "Iteration 2986 loss 0.2828827500343323\n",
      "Iteration 2987 loss 0.26387810707092285\n",
      "Iteration 2988 loss 0.33130326867103577\n",
      "Iteration 2989 loss 0.30189138650894165\n",
      "Iteration 2990 loss 0.2777639329433441\n",
      "Iteration 2991 loss 0.2943577170372009\n",
      "Iteration 2992 loss 0.27701428532600403\n",
      "Iteration 2993 loss 0.2669369876384735\n",
      "Iteration 2994 loss 0.2922258675098419\n",
      "Iteration 2995 loss 0.30270636081695557\n",
      "Iteration 2996 loss 0.2976835370063782\n",
      "Iteration 2997 loss 0.29871317744255066\n",
      "Iteration 2998 loss 0.259091317653656\n",
      "Iteration 2999 loss 0.3120726943016052\n",
      "Iteration 3000 loss 0.32957878708839417\n",
      "Iteration 3001 loss 0.2953062057495117\n",
      "Iteration 3002 loss 0.27603793144226074\n",
      "Iteration 3003 loss 0.25936195254325867\n",
      "Iteration 3004 loss 0.2866552770137787\n",
      "Iteration 3005 loss 0.30246397852897644\n",
      "Iteration 3006 loss 0.294528067111969\n",
      "Iteration 3007 loss 0.32412147521972656\n",
      "Iteration 3008 loss 0.31095799803733826\n",
      "Iteration 3009 loss 0.31107643246650696\n",
      "Iteration 3010 loss 0.3214088976383209\n",
      "Iteration 3011 loss 0.3052113950252533\n",
      "Iteration 3012 loss 0.3158189058303833\n",
      "Iteration 3013 loss 0.26459693908691406\n",
      "Iteration 3014 loss 0.2671394944190979\n",
      "Iteration 3015 loss 0.28027257323265076\n",
      "Iteration 3016 loss 0.274901419878006\n",
      "Iteration 3017 loss 0.3151012659072876\n",
      "Iteration 3018 loss 0.25741779804229736\n",
      "Iteration 3019 loss 0.22878117859363556\n",
      "Iteration 3020 loss 0.23811356723308563\n",
      "Iteration 3021 loss 0.27537328004837036\n",
      "Iteration 3022 loss 0.2569824755191803\n",
      "Iteration 3023 loss 0.28632354736328125\n",
      "Iteration 3024 loss 0.29334113001823425\n",
      "Iteration 3025 loss 0.2664661407470703\n",
      "Iteration 3026 loss 0.2595389783382416\n",
      "Iteration 3027 loss 0.283269464969635\n",
      "Iteration 3028 loss 0.2703303098678589\n",
      "Iteration 3029 loss 0.2897518575191498\n",
      "Iteration 3030 loss 0.27724528312683105\n",
      "Iteration 3031 loss 0.2651677429676056\n",
      "Iteration 3032 loss 0.276355117559433\n",
      "Iteration 3033 loss 0.24849466979503632\n",
      "Iteration 3034 loss 0.27735045552253723\n",
      "Iteration 3035 loss 0.28319522738456726\n",
      "Iteration 3036 loss 0.28472790122032166\n",
      "Iteration 3037 loss 0.29243093729019165\n",
      "Iteration 3038 loss 0.2766242027282715\n",
      "Iteration 3039 loss 0.33523091673851013\n",
      "Iteration 3040 loss 0.3024907112121582\n",
      "Iteration 3041 loss 0.31151339411735535\n",
      "Iteration 3042 loss 0.2610718309879303\n",
      "Iteration 3043 loss 0.298847496509552\n",
      "Iteration 3044 loss 0.2813953757286072\n",
      "Iteration 3045 loss 0.29261964559555054\n",
      "Iteration 3046 loss 0.26311761140823364\n",
      "Iteration 3047 loss 0.30599290132522583\n",
      "Iteration 3048 loss 0.28554514050483704\n",
      "Iteration 3049 loss 0.32051944732666016\n",
      "Iteration 3050 loss 0.2551526725292206\n",
      "Iteration 3051 loss 0.24360427260398865\n",
      "Iteration 3052 loss 0.3055335283279419\n",
      "Iteration 3053 loss 0.3172498345375061\n",
      "Iteration 3054 loss 0.27769070863723755\n",
      "Iteration 3055 loss 0.29120534658432007\n",
      "Iteration 3056 loss 0.27158403396606445\n",
      "Iteration 3057 loss 0.2851530909538269\n",
      "Iteration 3058 loss 0.3131014108657837\n",
      "Iteration 3059 loss 0.31953972578048706\n",
      "Iteration 3060 loss 0.26004135608673096\n",
      "Iteration 3061 loss 0.27236470580101013\n",
      "Iteration 3062 loss 0.2813376188278198\n",
      "Iteration 3063 loss 0.29258283972740173\n",
      "Iteration 3064 loss 0.26667752861976624\n",
      "Iteration 3065 loss 0.29150980710983276\n",
      "Iteration 3066 loss 0.3035333752632141\n",
      "Iteration 3067 loss 0.3208026885986328\n",
      "Iteration 3068 loss 0.2910740077495575\n",
      "Iteration 3069 loss 0.27073997259140015\n",
      "Iteration 3070 loss 0.307168573141098\n",
      "Iteration 3071 loss 0.24808090925216675\n",
      "Iteration 3072 loss 0.29052335023880005\n",
      "Iteration 3073 loss 0.30015259981155396\n",
      "Iteration 3074 loss 0.23076540231704712\n",
      "Iteration 3075 loss 0.33203643560409546\n",
      "Iteration 3076 loss 0.27877962589263916\n",
      "Iteration 3077 loss 0.307260662317276\n",
      "Iteration 3078 loss 0.32951706647872925\n",
      "Iteration 3079 loss 0.31316155195236206\n",
      "Iteration 3080 loss 0.32493895292282104\n",
      "Iteration 3081 loss 0.3156481385231018\n",
      "Iteration 3082 loss 0.24528972804546356\n",
      "Iteration 3083 loss 0.27763012051582336\n",
      "Iteration 3084 loss 0.3070673942565918\n",
      "Iteration 3085 loss 0.2927210032939911\n",
      "Iteration 3086 loss 0.30761271715164185\n",
      "Iteration 3087 loss 0.31889137625694275\n",
      "Iteration 3088 loss 0.3036818504333496\n",
      "Iteration 3089 loss 0.35249224305152893\n",
      "Iteration 3090 loss 0.276452898979187\n",
      "Iteration 3091 loss 0.3270876109600067\n",
      "Iteration 3092 loss 0.25927528738975525\n",
      "Iteration 3093 loss 0.32745760679244995\n",
      "Iteration 3094 loss 0.29930245876312256\n",
      "Iteration 3095 loss 0.32281428575515747\n",
      "Iteration 3096 loss 0.3217014670372009\n",
      "Iteration 3097 loss 0.31110939383506775\n",
      "Iteration 3098 loss 0.3180914521217346\n",
      "Iteration 3099 loss 0.3154226839542389\n",
      "Iteration 3100 loss 0.2837168276309967\n",
      "Iteration 3101 loss 0.2961242198944092\n",
      "Iteration 3102 loss 0.29948708415031433\n",
      "Iteration 3103 loss 0.3160454332828522\n",
      "Iteration 3104 loss 0.28268828988075256\n",
      "Iteration 3105 loss 0.2901742458343506\n",
      "Iteration 3106 loss 0.2631447911262512\n",
      "Iteration 3107 loss 0.3213633596897125\n",
      "Iteration 3108 loss 0.28212466835975647\n",
      "Iteration 3109 loss 0.27834153175354004\n",
      "Iteration 3110 loss 0.2740028202533722\n",
      "Iteration 3111 loss 0.318966269493103\n",
      "Iteration 3112 loss 0.2810790538787842\n",
      "Iteration 3113 loss 0.2854974865913391\n",
      "Iteration 3114 loss 0.2730669677257538\n",
      "Iteration 3115 loss 0.27464452385902405\n",
      "Iteration 3116 loss 0.2859225869178772\n",
      "Iteration 3117 loss 0.27476102113723755\n",
      "Iteration 3118 loss 0.2938440144062042\n",
      "Iteration 3119 loss 0.27271199226379395\n",
      "Iteration 3120 loss 0.2901081144809723\n",
      "Iteration 3121 loss 0.26883143186569214\n",
      "Iteration 3122 loss 0.2969452738761902\n",
      "Iteration 3123 loss 0.26326683163642883\n",
      "Iteration 3124 loss 0.3042360544204712\n",
      "Iteration 3125 loss 0.26493531465530396\n",
      "Iteration 3126 loss 0.29456666111946106\n",
      "Iteration 3127 loss 0.30226993560791016\n",
      "Iteration 3128 loss 0.26771485805511475\n",
      "Iteration 3129 loss 0.2873249650001526\n",
      "Iteration 3130 loss 0.25922757387161255\n",
      "Iteration 3131 loss 0.29949691891670227\n",
      "Iteration 3132 loss 0.26182591915130615\n",
      "Iteration 3133 loss 0.28096312284469604\n",
      "Iteration 3134 loss 0.28452861309051514\n",
      "Iteration 3135 loss 0.28056004643440247\n",
      "Iteration 3136 loss 0.3169846832752228\n",
      "Iteration 3137 loss 0.269516259431839\n",
      "Iteration 3138 loss 0.2720468044281006\n",
      "Iteration 3139 loss 0.2825680375099182\n",
      "Iteration 3140 loss 0.3043822646141052\n",
      "Iteration 3141 loss 0.30775028467178345\n",
      "Iteration 3142 loss 0.2764461636543274\n",
      "Iteration 3143 loss 0.28887253999710083\n",
      "Iteration 3144 loss 0.3015715181827545\n",
      "Iteration 3145 loss 0.2691819965839386\n",
      "Iteration 3146 loss 0.2802293598651886\n",
      "Iteration 3147 loss 0.2853354811668396\n",
      "Iteration 3148 loss 0.29335761070251465\n",
      "Iteration 3149 loss 0.2925707995891571\n",
      "Iteration 3150 loss 0.2919328212738037\n",
      "Iteration 3151 loss 0.30351126194000244\n",
      "Iteration 3152 loss 0.27651306986808777\n",
      "Iteration 3153 loss 0.30699530243873596\n",
      "Iteration 3154 loss 0.2802731990814209\n",
      "Iteration 3155 loss 0.30397462844848633\n",
      "Iteration 3156 loss 0.30771365761756897\n",
      "Iteration 3157 loss 0.25205081701278687\n",
      "Iteration 3158 loss 0.30866122245788574\n",
      "Iteration 3159 loss 0.30310308933258057\n",
      "Iteration 3160 loss 0.2870057225227356\n",
      "Iteration 3161 loss 0.24279356002807617\n",
      "Iteration 3162 loss 0.3038910925388336\n",
      "Iteration 3163 loss 0.2796940505504608\n",
      "Iteration 3164 loss 0.31541794538497925\n",
      "Iteration 3165 loss 0.27141857147216797\n",
      "Iteration 3166 loss 0.2702207863330841\n",
      "Iteration 3167 loss 0.30894771218299866\n",
      "Iteration 3168 loss 0.26579150557518005\n",
      "Iteration 3169 loss 0.3114958703517914\n",
      "Iteration 3170 loss 0.2693183422088623\n",
      "Iteration 3171 loss 0.2669690251350403\n",
      "Iteration 3172 loss 0.2686938941478729\n",
      "Iteration 3173 loss 0.28788888454437256\n",
      "Iteration 3174 loss 0.29763662815093994\n",
      "Iteration 3175 loss 0.2775084674358368\n",
      "Iteration 3176 loss 0.2609268128871918\n",
      "Iteration 3177 loss 0.277570903301239\n",
      "Iteration 3178 loss 0.29603877663612366\n",
      "Iteration 3179 loss 0.2698197364807129\n",
      "Iteration 3180 loss 0.28775161504745483\n",
      "Iteration 3181 loss 0.28202441334724426\n",
      "Iteration 3182 loss 0.28729045391082764\n",
      "Iteration 3183 loss 0.28469395637512207\n",
      "Iteration 3184 loss 0.28262919187545776\n",
      "Iteration 3185 loss 0.2893117666244507\n",
      "Iteration 3186 loss 0.28353196382522583\n",
      "Iteration 3187 loss 0.2972424030303955\n",
      "Iteration 3188 loss 0.27953508496284485\n",
      "Iteration 3189 loss 0.28490710258483887\n",
      "Iteration 3190 loss 0.27247878909111023\n",
      "Iteration 3191 loss 0.30792853236198425\n",
      "Iteration 3192 loss 0.27344629168510437\n",
      "Iteration 3193 loss 0.2925240099430084\n",
      "Iteration 3194 loss 0.3097481429576874\n",
      "Iteration 3195 loss 0.27913981676101685\n",
      "Iteration 3196 loss 0.27687615156173706\n",
      "Iteration 3197 loss 0.268372118473053\n",
      "Iteration 3198 loss 0.2980508804321289\n",
      "Iteration 3199 loss 0.3107350170612335\n",
      "Iteration 3200 loss 0.2659120559692383\n",
      "Iteration 3201 loss 0.28335973620414734\n",
      "Iteration 3202 loss 0.2705063819885254\n",
      "Iteration 3203 loss 0.2900008261203766\n",
      "Iteration 3204 loss 0.29797202348709106\n",
      "Iteration 3205 loss 0.281144380569458\n",
      "Iteration 3206 loss 0.29977989196777344\n",
      "Iteration 3207 loss 0.24539294838905334\n",
      "Iteration 3208 loss 0.2794453501701355\n",
      "Iteration 3209 loss 0.26979687809944153\n",
      "Iteration 3210 loss 0.2899329960346222\n",
      "Iteration 3211 loss 0.26177841424942017\n",
      "Iteration 3212 loss 0.28790730237960815\n",
      "Iteration 3213 loss 0.259946346282959\n",
      "Iteration 3214 loss 0.25778093934059143\n",
      "Iteration 3215 loss 0.28440412878990173\n",
      "Iteration 3216 loss 0.28484076261520386\n",
      "Iteration 3217 loss 0.2776701748371124\n",
      "Iteration 3218 loss 0.2852814197540283\n",
      "Iteration 3219 loss 0.23228834569454193\n",
      "Iteration 3220 loss 0.2599737346172333\n",
      "Iteration 3221 loss 0.26342612504959106\n",
      "Iteration 3222 loss 0.27870386838912964\n",
      "Iteration 3223 loss 0.2817291021347046\n",
      "Iteration 3224 loss 0.2841016352176666\n",
      "Iteration 3225 loss 0.27928751707077026\n",
      "Iteration 3226 loss 0.2848459482192993\n",
      "Iteration 3227 loss 0.27633368968963623\n",
      "Iteration 3228 loss 0.24814705550670624\n",
      "Iteration 3229 loss 0.2595584988594055\n",
      "Iteration 3230 loss 0.30387431383132935\n",
      "Iteration 3231 loss 0.2805691361427307\n",
      "Iteration 3232 loss 0.27917030453681946\n",
      "Iteration 3233 loss 0.2855321168899536\n",
      "Iteration 3234 loss 0.24023815989494324\n",
      "Iteration 3235 loss 0.27412527799606323\n",
      "Iteration 3236 loss 0.27384239435195923\n",
      "Iteration 3237 loss 0.3033265173435211\n",
      "Iteration 3238 loss 0.26907190680503845\n",
      "Iteration 3239 loss 0.26432734727859497\n",
      "Iteration 3240 loss 0.2961427569389343\n",
      "Iteration 3241 loss 0.2950512170791626\n",
      "Iteration 3242 loss 0.2634478807449341\n",
      "Iteration 3243 loss 0.2712891697883606\n",
      "Iteration 3244 loss 0.2701371908187866\n",
      "Iteration 3245 loss 0.2584986090660095\n",
      "Iteration 3246 loss 0.26814165711402893\n",
      "Iteration 3247 loss 0.2610333263874054\n",
      "Iteration 3248 loss 0.2828436493873596\n",
      "Iteration 3249 loss 0.2569209933280945\n",
      "Iteration 3250 loss 0.23603367805480957\n",
      "Iteration 3251 loss 0.25162824988365173\n",
      "Iteration 3252 loss 0.25794684886932373\n",
      "Iteration 3253 loss 0.27488505840301514\n",
      "Iteration 3254 loss 0.26428428292274475\n",
      "Iteration 3255 loss 0.27257269620895386\n",
      "Iteration 3256 loss 0.2657715678215027\n",
      "Iteration 3257 loss 0.2626206576824188\n",
      "Iteration 3258 loss 0.24588461220264435\n",
      "Iteration 3259 loss 0.27145904302597046\n",
      "Iteration 3260 loss 0.24475383758544922\n",
      "Iteration 3261 loss 0.27086445689201355\n",
      "Iteration 3262 loss 0.2562752962112427\n",
      "Iteration 3263 loss 0.27796101570129395\n",
      "Iteration 3264 loss 0.2552715539932251\n",
      "Iteration 3265 loss 0.26669904589653015\n",
      "Iteration 3266 loss 0.23660331964492798\n",
      "Iteration 3267 loss 0.2639480233192444\n",
      "Iteration 3268 loss 0.26491519808769226\n",
      "Iteration 3269 loss 0.23530131578445435\n",
      "Iteration 3270 loss 0.2910325527191162\n",
      "Iteration 3271 loss 0.24770712852478027\n",
      "Iteration 3272 loss 0.2702817916870117\n",
      "Iteration 3273 loss 0.24591806530952454\n",
      "Iteration 3274 loss 0.24821989238262177\n",
      "Iteration 3275 loss 0.2698129713535309\n",
      "Iteration 3276 loss 0.24639740586280823\n",
      "Iteration 3277 loss 0.2607508599758148\n",
      "Iteration 3278 loss 0.23316437005996704\n",
      "Iteration 3279 loss 0.2856466472148895\n",
      "Iteration 3280 loss 0.2474917769432068\n",
      "Iteration 3281 loss 0.26279833912849426\n",
      "Iteration 3282 loss 0.25708165764808655\n",
      "Iteration 3283 loss 0.2510707378387451\n",
      "Iteration 3284 loss 0.24029508233070374\n",
      "Iteration 3285 loss 0.27212288975715637\n",
      "Iteration 3286 loss 0.25869256258010864\n",
      "Iteration 3287 loss 0.2548583745956421\n",
      "Iteration 3288 loss 0.25895360112190247\n",
      "Iteration 3289 loss 0.25703325867652893\n",
      "Iteration 3290 loss 0.28430837392807007\n",
      "Iteration 3291 loss 0.23633235692977905\n",
      "Iteration 3292 loss 0.24831336736679077\n",
      "Iteration 3293 loss 0.2219715118408203\n",
      "Iteration 3294 loss 0.31072354316711426\n",
      "Iteration 3295 loss 0.26371458172798157\n",
      "Iteration 3296 loss 0.25316321849823\n",
      "Iteration 3297 loss 0.25443869829177856\n",
      "Iteration 3298 loss 0.2474389374256134\n",
      "Iteration 3299 loss 0.2389109879732132\n",
      "Iteration 3300 loss 0.27866607904434204\n",
      "Iteration 3301 loss 0.2637748122215271\n",
      "Iteration 3302 loss 0.28267839550971985\n",
      "Iteration 3303 loss 0.22622936964035034\n",
      "Iteration 3304 loss 0.26215147972106934\n",
      "Iteration 3305 loss 0.2847784161567688\n",
      "Iteration 3306 loss 0.25757625699043274\n",
      "Iteration 3307 loss 0.26730862259864807\n",
      "Iteration 3308 loss 0.2220764458179474\n",
      "Iteration 3309 loss 0.24106357991695404\n",
      "Iteration 3310 loss 0.2714960277080536\n",
      "Iteration 3311 loss 0.2655709683895111\n",
      "Iteration 3312 loss 0.30431312322616577\n",
      "Iteration 3313 loss 0.27717846632003784\n",
      "Iteration 3314 loss 0.2653336822986603\n",
      "Iteration 3315 loss 0.25515711307525635\n",
      "Iteration 3316 loss 0.26328814029693604\n",
      "Iteration 3317 loss 0.2596284747123718\n",
      "Iteration 3318 loss 0.31152188777923584\n",
      "Iteration 3319 loss 0.2584450840950012\n",
      "Iteration 3320 loss 0.24256403744220734\n",
      "Iteration 3321 loss 0.24545010924339294\n",
      "Iteration 3322 loss 0.2608974874019623\n",
      "Iteration 3323 loss 0.23565688729286194\n",
      "Iteration 3324 loss 0.2600581645965576\n",
      "Iteration 3325 loss 0.2816258370876312\n",
      "Iteration 3326 loss 0.2370503693819046\n",
      "Iteration 3327 loss 0.2961169481277466\n",
      "Iteration 3328 loss 0.2494577318429947\n",
      "Iteration 3329 loss 0.2719983458518982\n",
      "Iteration 3330 loss 0.262572705745697\n",
      "Iteration 3331 loss 0.26292142271995544\n",
      "Iteration 3332 loss 0.2737310230731964\n",
      "Iteration 3333 loss 0.2820562720298767\n",
      "Iteration 3334 loss 0.28707706928253174\n",
      "Iteration 3335 loss 0.2592519223690033\n",
      "Iteration 3336 loss 0.2683662176132202\n",
      "Iteration 3337 loss 0.27692925930023193\n",
      "Iteration 3338 loss 0.28990042209625244\n",
      "Iteration 3339 loss 0.2626572549343109\n",
      "Iteration 3340 loss 0.24959583580493927\n",
      "Iteration 3341 loss 0.23501352965831757\n",
      "Iteration 3342 loss 0.2983338534832001\n",
      "Iteration 3343 loss 0.27412718534469604\n",
      "Iteration 3344 loss 0.286736398935318\n",
      "Iteration 3345 loss 0.2955285310745239\n",
      "Iteration 3346 loss 0.25028374791145325\n",
      "Iteration 3347 loss 0.272330641746521\n",
      "Iteration 3348 loss 0.2589747905731201\n",
      "Iteration 3349 loss 0.3142291307449341\n",
      "Iteration 3350 loss 0.26888322830200195\n",
      "Iteration 3351 loss 0.2799976170063019\n",
      "Iteration 3352 loss 0.29137247800827026\n",
      "Iteration 3353 loss 0.2651211619377136\n",
      "Iteration 3354 loss 0.27227315306663513\n",
      "Iteration 3355 loss 0.26210835576057434\n",
      "Iteration 3356 loss 0.2536403238773346\n",
      "Iteration 3357 loss 0.2453528344631195\n",
      "Iteration 3358 loss 0.25516843795776367\n",
      "Iteration 3359 loss 0.26252758502960205\n",
      "Iteration 3360 loss 0.29021069407463074\n",
      "Iteration 3361 loss 0.23922322690486908\n",
      "Iteration 3362 loss 0.2900279760360718\n",
      "Iteration 3363 loss 0.2631568908691406\n",
      "Iteration 3364 loss 0.26271551847457886\n",
      "Iteration 3365 loss 0.2505702078342438\n",
      "Iteration 3366 loss 0.2874515950679779\n",
      "Iteration 3367 loss 0.27884441614151\n",
      "Iteration 3368 loss 0.27891916036605835\n",
      "Iteration 3369 loss 0.2809877395629883\n",
      "Iteration 3370 loss 0.2645842134952545\n",
      "Iteration 3371 loss 0.2872583568096161\n",
      "Iteration 3372 loss 0.2736842632293701\n",
      "Iteration 3373 loss 0.28650423884391785\n",
      "Iteration 3374 loss 0.30044397711753845\n",
      "Iteration 3375 loss 0.26364198327064514\n",
      "Iteration 3376 loss 0.268172025680542\n",
      "Iteration 3377 loss 0.2734363377094269\n",
      "Iteration 3378 loss 0.28792983293533325\n",
      "Iteration 3379 loss 0.2585531771183014\n",
      "Iteration 3380 loss 0.2875833511352539\n",
      "Iteration 3381 loss 0.27908167243003845\n",
      "Iteration 3382 loss 0.29506799578666687\n",
      "Iteration 3383 loss 0.286592572927475\n",
      "Iteration 3384 loss 0.25104254484176636\n",
      "Iteration 3385 loss 0.3172028362751007\n",
      "Iteration 3386 loss 0.263449102640152\n",
      "Iteration 3387 loss 0.3111867904663086\n",
      "Iteration 3388 loss 0.2879599630832672\n",
      "Iteration 3389 loss 0.3125051259994507\n",
      "Iteration 3390 loss 0.2960769534111023\n",
      "Iteration 3391 loss 0.29595184326171875\n",
      "Iteration 3392 loss 0.2997733950614929\n",
      "Iteration 3393 loss 0.2890954315662384\n",
      "Iteration 3394 loss 0.285349577665329\n",
      "Iteration 3395 loss 0.2583354115486145\n",
      "Iteration 3396 loss 0.3014940321445465\n",
      "Iteration 3397 loss 0.25786879658699036\n",
      "Iteration 3398 loss 0.26880162954330444\n",
      "Iteration 3399 loss 0.26199430227279663\n",
      "Iteration 3400 loss 0.26315897703170776\n",
      "Iteration 3401 loss 0.2674219012260437\n",
      "Iteration 3402 loss 0.2516849935054779\n",
      "Iteration 3403 loss 0.2781760096549988\n",
      "Iteration 3404 loss 0.27061569690704346\n",
      "Iteration 3405 loss 0.25857824087142944\n",
      "Iteration 3406 loss 0.2582634687423706\n",
      "Iteration 3407 loss 0.27449098229408264\n",
      "Iteration 3408 loss 0.2537073493003845\n",
      "Iteration 3409 loss 0.281978964805603\n",
      "Iteration 3410 loss 0.25154197216033936\n",
      "Iteration 3411 loss 0.2320939600467682\n",
      "Iteration 3412 loss 0.30567625164985657\n",
      "Iteration 3413 loss 0.23182564973831177\n",
      "Iteration 3414 loss 0.2787119746208191\n",
      "Iteration 3415 loss 0.22851631045341492\n",
      "Iteration 3416 loss 0.2714448869228363\n",
      "Iteration 3417 loss 0.22774909436702728\n",
      "Iteration 3418 loss 0.24416886270046234\n",
      "Iteration 3419 loss 0.2174108326435089\n",
      "Iteration 3420 loss 0.2793835699558258\n",
      "Iteration 3421 loss 0.2565748393535614\n",
      "Iteration 3422 loss 0.2401268631219864\n",
      "Iteration 3423 loss 0.2272709161043167\n",
      "Iteration 3424 loss 0.28422001004219055\n",
      "Iteration 3425 loss 0.2253282070159912\n",
      "Iteration 3426 loss 0.26112666726112366\n",
      "Iteration 3427 loss 0.25826093554496765\n",
      "Iteration 3428 loss 0.2577750086784363\n",
      "Iteration 3429 loss 0.27034667134284973\n",
      "Iteration 3430 loss 0.2613428831100464\n",
      "Iteration 3431 loss 0.24700532853603363\n",
      "Iteration 3432 loss 0.26848623156547546\n",
      "Iteration 3433 loss 0.24576710164546967\n",
      "Iteration 3434 loss 0.23969759047031403\n",
      "Iteration 3435 loss 0.23771734535694122\n",
      "Iteration 3436 loss 0.28539860248565674\n",
      "Iteration 3437 loss 0.25110310316085815\n",
      "Iteration 3438 loss 0.23825564980506897\n",
      "Iteration 3439 loss 0.28845566511154175\n",
      "Iteration 3440 loss 0.2466830611228943\n",
      "Iteration 3441 loss 0.31007397174835205\n",
      "Iteration 3442 loss 0.24634632468223572\n",
      "Iteration 3443 loss 0.24883365631103516\n",
      "Iteration 3444 loss 0.2718825936317444\n",
      "Iteration 3445 loss 0.2721216678619385\n",
      "Iteration 3446 loss 0.2804957628250122\n",
      "Iteration 3447 loss 0.3003997802734375\n",
      "Iteration 3448 loss 0.28715866804122925\n",
      "Iteration 3449 loss 0.2994232177734375\n",
      "Iteration 3450 loss 0.26298198103904724\n",
      "Iteration 3451 loss 0.2490597516298294\n",
      "Iteration 3452 loss 0.24825137853622437\n",
      "Iteration 3453 loss 0.265374094247818\n",
      "Iteration 3454 loss 0.2776457369327545\n",
      "Iteration 3455 loss 0.28141704201698303\n",
      "Iteration 3456 loss 0.26852622628211975\n",
      "Iteration 3457 loss 0.258525013923645\n",
      "Iteration 3458 loss 0.26819631457328796\n",
      "Iteration 3459 loss 0.2838510572910309\n",
      "Iteration 3460 loss 0.27455636858940125\n",
      "Iteration 3461 loss 0.27002498507499695\n",
      "Iteration 3462 loss 0.2633783221244812\n",
      "Iteration 3463 loss 0.25057411193847656\n",
      "Iteration 3464 loss 0.26383545994758606\n",
      "Iteration 3465 loss 0.2751849591732025\n",
      "Iteration 3466 loss 0.25762632489204407\n",
      "Iteration 3467 loss 0.3041287660598755\n",
      "Iteration 3468 loss 0.2856420576572418\n",
      "Iteration 3469 loss 0.25168856978416443\n",
      "Iteration 3470 loss 0.2693164348602295\n",
      "Iteration 3471 loss 0.25032350420951843\n",
      "Iteration 3472 loss 0.28163838386535645\n",
      "Iteration 3473 loss 0.2606498599052429\n",
      "Iteration 3474 loss 0.27478188276290894\n",
      "Iteration 3475 loss 0.26368504762649536\n",
      "Iteration 3476 loss 0.27599677443504333\n",
      "Iteration 3477 loss 0.30936551094055176\n",
      "Iteration 3478 loss 0.2734004557132721\n",
      "Iteration 3479 loss 0.27753910422325134\n",
      "Iteration 3480 loss 0.2906387150287628\n",
      "Iteration 3481 loss 0.28294140100479126\n",
      "Iteration 3482 loss 0.2928617000579834\n",
      "Iteration 3483 loss 0.2980633080005646\n",
      "Iteration 3484 loss 0.2975667119026184\n",
      "Iteration 3485 loss 0.2627723813056946\n",
      "Iteration 3486 loss 0.2617117762565613\n",
      "Iteration 3487 loss 0.23545855283737183\n",
      "Iteration 3488 loss 0.2596716284751892\n",
      "Iteration 3489 loss 0.25645560026168823\n",
      "Iteration 3490 loss 0.2562202215194702\n",
      "Iteration 3491 loss 0.2588341236114502\n",
      "Iteration 3492 loss 0.23553548753261566\n",
      "Iteration 3493 loss 0.24776266515254974\n",
      "Iteration 3494 loss 0.30576884746551514\n",
      "Iteration 3495 loss 0.27457141876220703\n",
      "Iteration 3496 loss 0.24053719639778137\n",
      "Iteration 3497 loss 0.2585328221321106\n",
      "Iteration 3498 loss 0.23766256868839264\n",
      "Iteration 3499 loss 0.2942487597465515\n",
      "Iteration 3500 loss 0.24875368177890778\n",
      "Iteration 3501 loss 0.26950401067733765\n",
      "Iteration 3502 loss 0.24792855978012085\n",
      "Iteration 3503 loss 0.23843705654144287\n",
      "Iteration 3504 loss 0.2882992625236511\n",
      "Iteration 3505 loss 0.23926612734794617\n",
      "Iteration 3506 loss 0.2830049991607666\n",
      "Iteration 3507 loss 0.25137513875961304\n",
      "Iteration 3508 loss 0.28332075476646423\n",
      "Iteration 3509 loss 0.282489538192749\n",
      "Iteration 3510 loss 0.27281638979911804\n",
      "Iteration 3511 loss 0.24242305755615234\n",
      "Iteration 3512 loss 0.25195735692977905\n",
      "Iteration 3513 loss 0.23253193497657776\n",
      "Iteration 3514 loss 0.2593700587749481\n",
      "Iteration 3515 loss 0.2522886097431183\n",
      "Iteration 3516 loss 0.24223430454730988\n",
      "Iteration 3517 loss 0.3044407367706299\n",
      "Iteration 3518 loss 0.257298082113266\n",
      "Iteration 3519 loss 0.25636398792266846\n",
      "Iteration 3520 loss 0.26627543568611145\n",
      "Iteration 3521 loss 0.28580960631370544\n",
      "Iteration 3522 loss 0.24899834394454956\n",
      "Iteration 3523 loss 0.2818585932254791\n",
      "Iteration 3524 loss 0.24762974679470062\n",
      "Iteration 3525 loss 0.2740374803543091\n",
      "Iteration 3526 loss 0.23640313744544983\n",
      "Iteration 3527 loss 0.26262760162353516\n",
      "Iteration 3528 loss 0.2612762153148651\n",
      "Iteration 3529 loss 0.25743451714515686\n",
      "Iteration 3530 loss 0.25802358984947205\n",
      "Iteration 3531 loss 0.2931438386440277\n",
      "Iteration 3532 loss 0.2528994381427765\n",
      "Iteration 3533 loss 0.294760137796402\n",
      "Iteration 3534 loss 0.26166701316833496\n",
      "Iteration 3535 loss 0.26248249411582947\n",
      "Iteration 3536 loss 0.26154977083206177\n",
      "Iteration 3537 loss 0.2750515639781952\n",
      "Iteration 3538 loss 0.27958253026008606\n",
      "Iteration 3539 loss 0.27560317516326904\n",
      "Iteration 3540 loss 0.24724972248077393\n",
      "Iteration 3541 loss 0.25187256932258606\n",
      "Iteration 3542 loss 0.30552709102630615\n",
      "Iteration 3543 loss 0.2107296586036682\n",
      "Iteration 3544 loss 0.27087685465812683\n",
      "Iteration 3545 loss 0.24603545665740967\n",
      "Iteration 3546 loss 0.2729461193084717\n",
      "Iteration 3547 loss 0.26079314947128296\n",
      "Iteration 3548 loss 0.2993987500667572\n",
      "Iteration 3549 loss 0.2487940937280655\n",
      "Iteration 3550 loss 0.26419246196746826\n",
      "Iteration 3551 loss 0.26288700103759766\n",
      "Iteration 3552 loss 0.2703890800476074\n",
      "Iteration 3553 loss 0.2603871524333954\n",
      "Iteration 3554 loss 0.2656630277633667\n",
      "Iteration 3555 loss 0.24869567155838013\n",
      "Iteration 3556 loss 0.27950119972229004\n",
      "Iteration 3557 loss 0.23389509320259094\n",
      "Iteration 3558 loss 0.28109869360923767\n",
      "Iteration 3559 loss 0.25269538164138794\n",
      "Iteration 3560 loss 0.2788236141204834\n",
      "Iteration 3561 loss 0.24344342947006226\n",
      "Iteration 3562 loss 0.2891997694969177\n",
      "Iteration 3563 loss 0.25909966230392456\n",
      "Iteration 3564 loss 0.25059670209884644\n",
      "Iteration 3565 loss 0.24187493324279785\n",
      "Iteration 3566 loss 0.23368941247463226\n",
      "Iteration 3567 loss 0.27463579177856445\n",
      "Iteration 3568 loss 0.2579123079776764\n",
      "Iteration 3569 loss 0.21220117807388306\n",
      "Iteration 3570 loss 0.2428320348262787\n",
      "Iteration 3571 loss 0.2742622494697571\n",
      "Iteration 3572 loss 0.2390972077846527\n",
      "Iteration 3573 loss 0.2698858082294464\n",
      "Iteration 3574 loss 0.22036685049533844\n",
      "Iteration 3575 loss 0.23927484452724457\n",
      "Iteration 3576 loss 0.25441107153892517\n",
      "Iteration 3577 loss 0.24258652329444885\n",
      "Iteration 3578 loss 0.2594878077507019\n",
      "Iteration 3579 loss 0.2382684350013733\n",
      "Iteration 3580 loss 0.26317983865737915\n",
      "Iteration 3581 loss 0.2786557674407959\n",
      "Iteration 3582 loss 0.2502884268760681\n",
      "Iteration 3583 loss 0.2530403733253479\n",
      "Iteration 3584 loss 0.24548687040805817\n",
      "Iteration 3585 loss 0.2643306851387024\n",
      "Iteration 3586 loss 0.2462746500968933\n",
      "Iteration 3587 loss 0.25963759422302246\n",
      "Iteration 3588 loss 0.23903419077396393\n",
      "Iteration 3589 loss 0.27203723788261414\n",
      "Iteration 3590 loss 0.23951315879821777\n",
      "Iteration 3591 loss 0.28524845838546753\n",
      "Iteration 3592 loss 0.2434069812297821\n",
      "Iteration 3593 loss 0.275709867477417\n",
      "Iteration 3594 loss 0.2302960306406021\n",
      "Iteration 3595 loss 0.2512657940387726\n",
      "Iteration 3596 loss 0.23414346575737\n",
      "Iteration 3597 loss 0.2559543550014496\n",
      "Iteration 3598 loss 0.23964214324951172\n",
      "Iteration 3599 loss 0.25136205554008484\n",
      "Iteration 3600 loss 0.23964744806289673\n",
      "Iteration 3601 loss 0.24277840554714203\n",
      "Iteration 3602 loss 0.2414187490940094\n",
      "Iteration 3603 loss 0.2445838451385498\n",
      "Iteration 3604 loss 0.24170289933681488\n",
      "Iteration 3605 loss 0.258730411529541\n",
      "Iteration 3606 loss 0.23691323399543762\n",
      "Iteration 3607 loss 0.28064751625061035\n",
      "Iteration 3608 loss 0.2443113774061203\n",
      "Iteration 3609 loss 0.23572102189064026\n",
      "Iteration 3610 loss 0.26932162046432495\n",
      "Iteration 3611 loss 0.23705606162548065\n",
      "Iteration 3612 loss 0.26097574830055237\n",
      "Iteration 3613 loss 0.2365371435880661\n",
      "Iteration 3614 loss 0.22714832425117493\n",
      "Iteration 3615 loss 0.22297616302967072\n",
      "Iteration 3616 loss 0.23959876596927643\n",
      "Iteration 3617 loss 0.2577914595603943\n",
      "Iteration 3618 loss 0.23311075568199158\n",
      "Iteration 3619 loss 0.2388441115617752\n",
      "Iteration 3620 loss 0.23113059997558594\n",
      "Iteration 3621 loss 0.22217296063899994\n",
      "Iteration 3622 loss 0.2650160491466522\n",
      "Iteration 3623 loss 0.2519504129886627\n",
      "Iteration 3624 loss 0.2672569155693054\n",
      "Iteration 3625 loss 0.26214033365249634\n",
      "Iteration 3626 loss 0.252462774515152\n",
      "Iteration 3627 loss 0.29374754428863525\n",
      "Iteration 3628 loss 0.27427810430526733\n",
      "Iteration 3629 loss 0.2459176927804947\n",
      "Iteration 3630 loss 0.2707274854183197\n",
      "Iteration 3631 loss 0.2659338712692261\n",
      "Iteration 3632 loss 0.2389204502105713\n",
      "Iteration 3633 loss 0.2902984619140625\n",
      "Iteration 3634 loss 0.24015335738658905\n",
      "Iteration 3635 loss 0.23434869945049286\n",
      "Iteration 3636 loss 0.27395933866500854\n",
      "Iteration 3637 loss 0.23170627653598785\n",
      "Iteration 3638 loss 0.2879613935947418\n",
      "Iteration 3639 loss 0.2218324840068817\n",
      "Iteration 3640 loss 0.22334720194339752\n",
      "Iteration 3641 loss 0.2387075424194336\n",
      "Iteration 3642 loss 0.23375748097896576\n",
      "Iteration 3643 loss 0.23305922746658325\n",
      "Iteration 3644 loss 0.2530938982963562\n",
      "Iteration 3645 loss 0.2402096837759018\n",
      "Iteration 3646 loss 0.26896798610687256\n",
      "Iteration 3647 loss 0.24516771733760834\n",
      "Iteration 3648 loss 0.2741532325744629\n",
      "Iteration 3649 loss 0.23856626451015472\n",
      "Iteration 3650 loss 0.2545347213745117\n",
      "Iteration 3651 loss 0.2427404522895813\n",
      "Iteration 3652 loss 0.24109089374542236\n",
      "Iteration 3653 loss 0.26645851135253906\n",
      "Iteration 3654 loss 0.2796858251094818\n",
      "Iteration 3655 loss 0.25362688302993774\n",
      "Iteration 3656 loss 0.2288249284029007\n",
      "Iteration 3657 loss 0.24307242035865784\n",
      "Iteration 3658 loss 0.25751256942749023\n",
      "Iteration 3659 loss 0.22210092842578888\n",
      "Iteration 3660 loss 0.27458086609840393\n",
      "Iteration 3661 loss 0.2340700477361679\n",
      "Iteration 3662 loss 0.2301461547613144\n",
      "Iteration 3663 loss 0.25645601749420166\n",
      "Iteration 3664 loss 0.2453477382659912\n",
      "Iteration 3665 loss 0.24606838822364807\n",
      "Iteration 3666 loss 0.24289774894714355\n",
      "Iteration 3667 loss 0.2323322296142578\n",
      "Iteration 3668 loss 0.24013301730155945\n",
      "Iteration 3669 loss 0.25306570529937744\n",
      "Iteration 3670 loss 0.29485997557640076\n",
      "Iteration 3671 loss 0.21913142502307892\n",
      "Iteration 3672 loss 0.29541870951652527\n",
      "Iteration 3673 loss 0.23921968042850494\n",
      "Iteration 3674 loss 0.2405192255973816\n",
      "Iteration 3675 loss 0.24184197187423706\n",
      "Iteration 3676 loss 0.286493182182312\n",
      "Iteration 3677 loss 0.24246367812156677\n",
      "Iteration 3678 loss 0.2775736153125763\n",
      "Iteration 3679 loss 0.2761668562889099\n",
      "Iteration 3680 loss 0.28628119826316833\n",
      "Iteration 3681 loss 0.20745885372161865\n",
      "Iteration 3682 loss 0.2588871121406555\n",
      "Iteration 3683 loss 0.2549923062324524\n",
      "Iteration 3684 loss 0.2580336034297943\n",
      "Iteration 3685 loss 0.24025268852710724\n",
      "Iteration 3686 loss 0.23477208614349365\n",
      "Iteration 3687 loss 0.2556650936603546\n",
      "Iteration 3688 loss 0.24823351204395294\n",
      "Iteration 3689 loss 0.259170264005661\n",
      "Iteration 3690 loss 0.26127660274505615\n",
      "Iteration 3691 loss 0.25271326303482056\n",
      "Iteration 3692 loss 0.27393773198127747\n",
      "Iteration 3693 loss 0.25018224120140076\n",
      "Iteration 3694 loss 0.24415002763271332\n",
      "Iteration 3695 loss 0.24577218294143677\n",
      "Iteration 3696 loss 0.23125970363616943\n",
      "Iteration 3697 loss 0.23784835636615753\n",
      "Iteration 3698 loss 0.25985196232795715\n",
      "Iteration 3699 loss 0.22705280780792236\n",
      "Iteration 3700 loss 0.23147495090961456\n",
      "Iteration 3701 loss 0.22939497232437134\n",
      "Iteration 3702 loss 0.22935481369495392\n",
      "Iteration 3703 loss 0.25606781244277954\n",
      "Iteration 3704 loss 0.2610933482646942\n",
      "Iteration 3705 loss 0.2571708559989929\n",
      "Iteration 3706 loss 0.27794188261032104\n",
      "Iteration 3707 loss 0.22947293519973755\n",
      "Iteration 3708 loss 0.26089558005332947\n",
      "Iteration 3709 loss 0.210447758436203\n",
      "Iteration 3710 loss 0.2619532346725464\n",
      "Iteration 3711 loss 0.2233729213476181\n",
      "Iteration 3712 loss 0.25133973360061646\n",
      "Iteration 3713 loss 0.2230195254087448\n",
      "Iteration 3714 loss 0.23629234731197357\n",
      "Iteration 3715 loss 0.2543555200099945\n",
      "Iteration 3716 loss 0.24963034689426422\n",
      "Iteration 3717 loss 0.24079561233520508\n",
      "Iteration 3718 loss 0.24998030066490173\n",
      "Iteration 3719 loss 0.26607874035835266\n",
      "Iteration 3720 loss 0.24491307139396667\n",
      "Iteration 3721 loss 0.24377907812595367\n",
      "Iteration 3722 loss 0.24943982064723969\n",
      "Iteration 3723 loss 0.23591460287570953\n",
      "Iteration 3724 loss 0.24662469327449799\n",
      "Iteration 3725 loss 0.2543584704399109\n",
      "Iteration 3726 loss 0.2621612250804901\n",
      "Iteration 3727 loss 0.2325570434331894\n",
      "Iteration 3728 loss 0.24552582204341888\n",
      "Iteration 3729 loss 0.24296696484088898\n",
      "Iteration 3730 loss 0.26815396547317505\n",
      "Iteration 3731 loss 0.24009929597377777\n",
      "Iteration 3732 loss 0.25719907879829407\n",
      "Iteration 3733 loss 0.22972017526626587\n",
      "Iteration 3734 loss 0.24426323175430298\n",
      "Iteration 3735 loss 0.2469044178724289\n",
      "Iteration 3736 loss 0.2512611448764801\n",
      "Iteration 3737 loss 0.22515150904655457\n",
      "Iteration 3738 loss 0.2258213311433792\n",
      "Iteration 3739 loss 0.25463438034057617\n",
      "Iteration 3740 loss 0.24940913915634155\n",
      "Iteration 3741 loss 0.2620179355144501\n",
      "Iteration 3742 loss 0.23833473026752472\n",
      "Iteration 3743 loss 0.23897002637386322\n",
      "Iteration 3744 loss 0.24563296139240265\n",
      "Iteration 3745 loss 0.24329733848571777\n",
      "Iteration 3746 loss 0.2691557705402374\n",
      "Iteration 3747 loss 0.23306690156459808\n",
      "Iteration 3748 loss 0.2632741332054138\n",
      "Iteration 3749 loss 0.23785492777824402\n",
      "Iteration 3750 loss 0.2803789973258972\n",
      "Iteration 3751 loss 0.2460811585187912\n",
      "Iteration 3752 loss 0.2829018831253052\n",
      "Iteration 3753 loss 0.25468599796295166\n",
      "Iteration 3754 loss 0.2596006691455841\n",
      "Iteration 3755 loss 0.2796964645385742\n",
      "Iteration 3756 loss 0.25260862708091736\n",
      "Iteration 3757 loss 0.28224441409111023\n",
      "Iteration 3758 loss 0.2656230330467224\n",
      "Iteration 3759 loss 0.2626626193523407\n",
      "Iteration 3760 loss 0.246283620595932\n",
      "Iteration 3761 loss 0.24554800987243652\n",
      "Iteration 3762 loss 0.24784111976623535\n",
      "Iteration 3763 loss 0.2538608908653259\n",
      "Iteration 3764 loss 0.21378150582313538\n",
      "Iteration 3765 loss 0.2312537431716919\n",
      "Iteration 3766 loss 0.25194963812828064\n",
      "Iteration 3767 loss 0.23441696166992188\n",
      "Iteration 3768 loss 0.2331116944551468\n",
      "Iteration 3769 loss 0.23892620205879211\n",
      "Iteration 3770 loss 0.2156369686126709\n",
      "Iteration 3771 loss 0.25492969155311584\n",
      "Iteration 3772 loss 0.22996273636817932\n",
      "Iteration 3773 loss 0.261145681142807\n",
      "Iteration 3774 loss 0.2644812762737274\n",
      "Iteration 3775 loss 0.25729504227638245\n",
      "Iteration 3776 loss 0.2752109169960022\n",
      "Iteration 3777 loss 0.23287178575992584\n",
      "Iteration 3778 loss 0.24227048456668854\n",
      "Iteration 3779 loss 0.23624013364315033\n",
      "Iteration 3780 loss 0.27234014868736267\n",
      "Iteration 3781 loss 0.227576345205307\n",
      "Iteration 3782 loss 0.2753835916519165\n",
      "Iteration 3783 loss 0.23305262625217438\n",
      "Iteration 3784 loss 0.2634270489215851\n",
      "Iteration 3785 loss 0.24401775002479553\n",
      "Iteration 3786 loss 0.2330741286277771\n",
      "Iteration 3787 loss 0.21848508715629578\n",
      "Iteration 3788 loss 0.24296504259109497\n",
      "Iteration 3789 loss 0.24606330692768097\n",
      "Iteration 3790 loss 0.25948619842529297\n",
      "Iteration 3791 loss 0.23350605368614197\n",
      "Iteration 3792 loss 0.23481711745262146\n",
      "Iteration 3793 loss 0.2452334463596344\n",
      "Iteration 3794 loss 0.23812440037727356\n",
      "Iteration 3795 loss 0.2368815839290619\n",
      "Iteration 3796 loss 0.2292906790971756\n",
      "Iteration 3797 loss 0.24755269289016724\n",
      "Iteration 3798 loss 0.2109677791595459\n",
      "Iteration 3799 loss 0.2249770164489746\n",
      "Iteration 3800 loss 0.2292378544807434\n",
      "Iteration 3801 loss 0.2139088213443756\n",
      "Iteration 3802 loss 0.25215160846710205\n",
      "Iteration 3803 loss 0.24881479144096375\n",
      "Iteration 3804 loss 0.22754842042922974\n",
      "Iteration 3805 loss 0.2595457434654236\n",
      "Iteration 3806 loss 0.25268426537513733\n",
      "Iteration 3807 loss 0.23711377382278442\n",
      "Iteration 3808 loss 0.2517821490764618\n",
      "Iteration 3809 loss 0.23686939477920532\n",
      "Iteration 3810 loss 0.22989509999752045\n",
      "Iteration 3811 loss 0.21937179565429688\n",
      "Iteration 3812 loss 0.24883431196212769\n",
      "Iteration 3813 loss 0.22161398828029633\n",
      "Iteration 3814 loss 0.20942619442939758\n",
      "Iteration 3815 loss 0.2290581315755844\n",
      "Iteration 3816 loss 0.1776057332754135\n",
      "Iteration 3817 loss 0.2170519232749939\n",
      "Iteration 3818 loss 0.22331464290618896\n",
      "Iteration 3819 loss 0.24804456532001495\n",
      "Iteration 3820 loss 0.2036803960800171\n",
      "Iteration 3821 loss 0.24902799725532532\n",
      "Iteration 3822 loss 0.2678641974925995\n",
      "Iteration 3823 loss 0.22557827830314636\n",
      "Iteration 3824 loss 0.27391955256462097\n",
      "Iteration 3825 loss 0.21699835360050201\n",
      "Iteration 3826 loss 0.25998467206954956\n",
      "Iteration 3827 loss 0.2598479390144348\n",
      "Iteration 3828 loss 0.2608303129673004\n",
      "Iteration 3829 loss 0.24616485834121704\n",
      "Iteration 3830 loss 0.24527475237846375\n",
      "Iteration 3831 loss 0.22923336923122406\n",
      "Iteration 3832 loss 0.229071244597435\n",
      "Iteration 3833 loss 0.22175747156143188\n",
      "Iteration 3834 loss 0.22676217555999756\n",
      "Iteration 3835 loss 0.21385109424591064\n",
      "Iteration 3836 loss 0.2388916164636612\n",
      "Iteration 3837 loss 0.2369234561920166\n",
      "Iteration 3838 loss 0.25879573822021484\n",
      "Iteration 3839 loss 0.22437304258346558\n",
      "Iteration 3840 loss 0.22331570088863373\n",
      "Iteration 3841 loss 0.23652976751327515\n",
      "Iteration 3842 loss 0.22128334641456604\n",
      "Iteration 3843 loss 0.22569584846496582\n",
      "Iteration 3844 loss 0.22625230252742767\n",
      "Iteration 3845 loss 0.24376530945301056\n",
      "Iteration 3846 loss 0.22481726109981537\n",
      "Iteration 3847 loss 0.21820734441280365\n",
      "Iteration 3848 loss 0.263407438993454\n",
      "Iteration 3849 loss 0.22897668182849884\n",
      "Iteration 3850 loss 0.2267412394285202\n",
      "Iteration 3851 loss 0.22629596292972565\n",
      "Iteration 3852 loss 0.2785490155220032\n",
      "Iteration 3853 loss 0.25024572014808655\n",
      "Iteration 3854 loss 0.2758079767227173\n",
      "Iteration 3855 loss 0.24949780106544495\n",
      "Iteration 3856 loss 0.28133055567741394\n",
      "Iteration 3857 loss 0.24621345102787018\n",
      "Iteration 3858 loss 0.2551805377006531\n",
      "Iteration 3859 loss 0.27035024762153625\n",
      "Iteration 3860 loss 0.22759579122066498\n",
      "Iteration 3861 loss 0.25766995549201965\n",
      "Iteration 3862 loss 0.26251357793807983\n",
      "Iteration 3863 loss 0.22471143305301666\n",
      "Iteration 3864 loss 0.26699772477149963\n",
      "Iteration 3865 loss 0.21401189267635345\n",
      "Iteration 3866 loss 0.25146690011024475\n",
      "Iteration 3867 loss 0.23477567732334137\n",
      "Iteration 3868 loss 0.2372264564037323\n",
      "Iteration 3869 loss 0.24739709496498108\n",
      "Iteration 3870 loss 0.24680137634277344\n",
      "Iteration 3871 loss 0.24739742279052734\n",
      "Iteration 3872 loss 0.25391748547554016\n",
      "Iteration 3873 loss 0.23206806182861328\n",
      "Iteration 3874 loss 0.25007253885269165\n",
      "Iteration 3875 loss 0.24789218604564667\n",
      "Iteration 3876 loss 0.25871196389198303\n",
      "Iteration 3877 loss 0.25856393575668335\n",
      "Iteration 3878 loss 0.24323536455631256\n",
      "Iteration 3879 loss 0.2629016637802124\n",
      "Iteration 3880 loss 0.2414504885673523\n",
      "Iteration 3881 loss 0.24698689579963684\n",
      "Iteration 3882 loss 0.2793838083744049\n",
      "Iteration 3883 loss 0.23548653721809387\n",
      "Iteration 3884 loss 0.22844348847866058\n",
      "Iteration 3885 loss 0.2344793975353241\n",
      "Iteration 3886 loss 0.22441086173057556\n",
      "Iteration 3887 loss 0.24556957185268402\n",
      "Iteration 3888 loss 0.2567758858203888\n",
      "Iteration 3889 loss 0.22706621885299683\n",
      "Iteration 3890 loss 0.24178479611873627\n",
      "Iteration 3891 loss 0.2501590847969055\n",
      "Iteration 3892 loss 0.2261202037334442\n",
      "Iteration 3893 loss 0.24484696984291077\n",
      "Iteration 3894 loss 0.24108487367630005\n",
      "Iteration 3895 loss 0.23145483434200287\n",
      "Iteration 3896 loss 0.23521114885807037\n",
      "Iteration 3897 loss 0.25770366191864014\n",
      "Iteration 3898 loss 0.24918591976165771\n",
      "Iteration 3899 loss 0.25839096307754517\n",
      "Iteration 3900 loss 0.2646692097187042\n",
      "Iteration 3901 loss 0.2397586852312088\n",
      "Iteration 3902 loss 0.23200076818466187\n",
      "Iteration 3903 loss 0.247194305062294\n",
      "Iteration 3904 loss 0.2282402515411377\n",
      "Iteration 3905 loss 0.2428184300661087\n",
      "Iteration 3906 loss 0.25260457396507263\n",
      "Iteration 3907 loss 0.25682544708251953\n",
      "Iteration 3908 loss 0.2282123863697052\n",
      "Iteration 3909 loss 0.20357733964920044\n",
      "Iteration 3910 loss 0.23270878195762634\n",
      "Iteration 3911 loss 0.22429296374320984\n",
      "Iteration 3912 loss 0.24405887722969055\n",
      "Iteration 3913 loss 0.21316781640052795\n",
      "Iteration 3914 loss 0.24665041267871857\n",
      "Iteration 3915 loss 0.2281944751739502\n",
      "Iteration 3916 loss 0.2187475711107254\n",
      "Iteration 3917 loss 0.23803862929344177\n",
      "Iteration 3918 loss 0.2339404672384262\n",
      "Iteration 3919 loss 0.23976588249206543\n",
      "Iteration 3920 loss 0.2244330495595932\n",
      "Iteration 3921 loss 0.2349388152360916\n",
      "Iteration 3922 loss 0.2287067174911499\n",
      "Iteration 3923 loss 0.22899669408798218\n",
      "Iteration 3924 loss 0.23726950585842133\n",
      "Iteration 3925 loss 0.24519935250282288\n",
      "Iteration 3926 loss 0.24635805189609528\n",
      "Iteration 3927 loss 0.2544024884700775\n",
      "Iteration 3928 loss 0.24809634685516357\n",
      "Iteration 3929 loss 0.23017378151416779\n",
      "Iteration 3930 loss 0.27030742168426514\n",
      "Iteration 3931 loss 0.21844497323036194\n",
      "Iteration 3932 loss 0.2663499414920807\n",
      "Iteration 3933 loss 0.22035473585128784\n",
      "Iteration 3934 loss 0.23871800303459167\n",
      "Iteration 3935 loss 0.23847702145576477\n",
      "Iteration 3936 loss 0.24100326001644135\n",
      "Iteration 3937 loss 0.23517823219299316\n",
      "Iteration 3938 loss 0.25540274381637573\n",
      "Iteration 3939 loss 0.2159617394208908\n",
      "Iteration 3940 loss 0.23366770148277283\n",
      "Iteration 3941 loss 0.25043797492980957\n",
      "Iteration 3942 loss 0.2108115553855896\n",
      "Iteration 3943 loss 0.21829062700271606\n",
      "Iteration 3944 loss 0.23595508933067322\n",
      "Iteration 3945 loss 0.24141037464141846\n",
      "Iteration 3946 loss 0.24786828458309174\n",
      "Iteration 3947 loss 0.2537859082221985\n",
      "Iteration 3948 loss 0.2650650143623352\n",
      "Iteration 3949 loss 0.23633889853954315\n",
      "Iteration 3950 loss 0.23805859684944153\n",
      "Iteration 3951 loss 0.2072778046131134\n",
      "Iteration 3952 loss 0.22625192999839783\n",
      "Iteration 3953 loss 0.2498563975095749\n",
      "Iteration 3954 loss 0.23080776631832123\n",
      "Iteration 3955 loss 0.2403213232755661\n",
      "Iteration 3956 loss 0.22331349551677704\n",
      "Iteration 3957 loss 0.2246917486190796\n",
      "Iteration 3958 loss 0.22752730548381805\n",
      "Iteration 3959 loss 0.22120678424835205\n",
      "Iteration 3960 loss 0.22774988412857056\n",
      "Iteration 3961 loss 0.22958406805992126\n",
      "Iteration 3962 loss 0.23499652743339539\n",
      "Iteration 3963 loss 0.2240525484085083\n",
      "Iteration 3964 loss 0.24251008033752441\n",
      "Iteration 3965 loss 0.2090284377336502\n",
      "Iteration 3966 loss 0.203757643699646\n",
      "Iteration 3967 loss 0.22775281965732574\n",
      "Iteration 3968 loss 0.22728243470191956\n",
      "Iteration 3969 loss 0.219761922955513\n",
      "Iteration 3970 loss 0.23254764080047607\n",
      "Iteration 3971 loss 0.22059546411037445\n",
      "Iteration 3972 loss 0.21727071702480316\n",
      "Iteration 3973 loss 0.2148398756980896\n",
      "Iteration 3974 loss 0.21430638432502747\n",
      "Iteration 3975 loss 0.19320517778396606\n",
      "Iteration 3976 loss 0.2441168576478958\n",
      "Iteration 3977 loss 0.23940831422805786\n",
      "Iteration 3978 loss 0.26098331809043884\n",
      "Iteration 3979 loss 0.22056415677070618\n",
      "Iteration 3980 loss 0.21265903115272522\n",
      "Iteration 3981 loss 0.2236805111169815\n",
      "Iteration 3982 loss 0.2294558435678482\n",
      "Iteration 3983 loss 0.2306806743144989\n",
      "Iteration 3984 loss 0.2555549144744873\n",
      "Iteration 3985 loss 0.21489156782627106\n",
      "Iteration 3986 loss 0.2328198403120041\n",
      "Iteration 3987 loss 0.20552538335323334\n",
      "Iteration 3988 loss 0.23170506954193115\n",
      "Iteration 3989 loss 0.19529134035110474\n",
      "Iteration 3990 loss 0.25988805294036865\n",
      "Iteration 3991 loss 0.24942219257354736\n",
      "Iteration 3992 loss 0.23524793982505798\n",
      "Iteration 3993 loss 0.23568546772003174\n",
      "Iteration 3994 loss 0.25629693269729614\n",
      "Iteration 3995 loss 0.2279994785785675\n",
      "Iteration 3996 loss 0.2585825026035309\n",
      "Iteration 3997 loss 0.22107422351837158\n",
      "Iteration 3998 loss 0.20711366832256317\n",
      "Iteration 3999 loss 0.21897612512111664\n",
      "Iteration 4000 loss 0.2138451635837555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x715e0cdb0e00>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "m = EncoderDecoder(24, 128, 512)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "losses = []\n",
    "\n",
    "batch_size = 256\n",
    "iterations = 4000\n",
    "\n",
    "for i in range(iterations):\n",
    "    ind = torch.randint(0, x.shape[0], (batch_size,))\n",
    "    u = m.forward(x[ind].float())\n",
    "    loss = torch.abs(((u - y[ind]) ** 2).sum(axis=1) - distances[ind] ** 2).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Iteration {i+1} loss {loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(range(len(losses)), losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2363, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = m.forward(x.float())\n",
    "\n",
    "torch.abs(((u - y) ** 2).sum(axis=1) - distances ** 2).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6972, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 1000\n",
    "\n",
    "# The base data\n",
    "x = torch.randint(high=2, size=(N,24))\n",
    "\n",
    "# Target data\n",
    "# For ith data, look for a 1 at position i % 24 and a 0 in position i % 25; if found, transpose, otherwise, skip\n",
    "do_transpose = (x[torch.arange(N), torch.arange(N) % 24] == 1) & (x[torch.arange(N), (torch.arange(N) + 1) % 24] == 0)\n",
    "# If there is a 1 at the first position and a 1 in the next position, drop it\n",
    "do_drop = (x[torch.arange(N), torch.arange(N) % 24] == 1) & (x[torch.arange(N), (torch.arange(N) + 1) % 24] == 1)\n",
    "\n",
    "y = x.detach().clone()\n",
    "\n",
    "y[do_transpose, (torch.arange(N) % 24)[do_transpose]] = 0\n",
    "y[do_transpose, ((torch.arange(N) + 1) % 24)[do_transpose]] = 1\n",
    "\n",
    "y[do_drop, (torch.arange(N) % 24)[do_drop]] = 0\n",
    "\n",
    "\n",
    "distances = torch.zeros(N)\n",
    "distances[do_transpose] = 0.5\n",
    "distances[do_drop] = 1\n",
    "\n",
    "\n",
    "u = m.forward(x.float())\n",
    "torch.abs(((u - y) ** 2).sum(axis=1) - distances ** 2).mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
